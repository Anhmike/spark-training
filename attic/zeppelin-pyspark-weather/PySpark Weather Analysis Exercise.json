{"paragraphs":[{"text":"%md\n# Weather Data Analytics\nThis notebook performs some basic weather data analytics using the PySpark RDD interface.","dateUpdated":"Jun 5, 2016 9:10:54 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054302_-1750400540","id":"20160605-091054_942241717","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Weather Data Analytics</h1>\n<p>This notebook performs some basic weather data analytics using the PySpark RDD interface.</p>\n"},"dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2195"},{"text":"%md\n## Helper Methods\nFirst we need some helper methods for converting the raw data into something that we can work with. We decide to use Python dictionaries instead of classes, since custom classes cannot be used within Zeppelin due to serialization issues","dateUpdated":"Jun 5, 2016 9:10:54 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054302_-1750400540","id":"20160605-091054_712124641","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Helper Methods</h2>\n<p>First we need some helper methods for converting the raw data into something that we can work with. We decide to use Python dictionaries instead of classes, since custom classes cannot be used within Zeppelin due to serialization issues</p>\n"},"dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2196"},{"text":"%pyspark\n\ndef _get_float(str):\n    \"\"\"\n    Helper method for converting a string to a float. If this is not possible, None will be returned instead\n    \"\"\"\n    if len(str) == 0:\n        return None\n    try:\n        return float(str)\n    except ValueError:\n        return None\n\n\ndef extract_station(line):\n    \"\"\"\n    Extract weather station data from a raw CSV line\n    \"\"\"\n    raw_columns = line.split(',')\n    columns = [c.replace('\"','') for c in raw_columns]\n\n    usaf = columns[0]\n    wban = columns[1]\n    name = columns[2]\n    country = columns[3]\n    state = columns[4]\n    icao = columns[5]\n    latitude = _get_float(columns[6])\n    longitude = _get_float(columns[7])\n    elevation = _get_float(columns[8])\n    date_begin = columns[9]\n    date_end = columns[10]\n    return {'usaf':usaf, 'wban':wban, 'name':name, 'country':country, 'state':state, 'icao':icao, 'latitude':latitude, 'longitude':longitude, 'elevation':elevation, 'date_begin':date_begin, 'date_end':date_end }\n\n\ndef extract_weather(line):\n    \"\"\"\n    Extract weather data from a raw data line.\n    \"\"\"\n    date = line[15:23]\n    time = line[23:27]\n    usaf = line[4:10]\n    wban = line[10:15]\n    airTemperatureQuality = line[92] == '1'\n    airTemperature = float(line[87:92]) / 10\n    windSpeedQuality = line[69] == '1'\n    windSpeed = float(line[65:69]) / 10\n    return {'date':date, 'time':time, 'usaf':usaf, 'wban':wban, 'airTemperatureQuality':airTemperatureQuality, 'airTemperature':airTemperature, 'windSpeedQuality':windSpeedQuality, 'windSpeed':windSpeed }\n","dateUpdated":"Jun 5, 2016 9:10:54 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054302_-1750400540","id":"20160605-091054_752476556","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2197"},{"text":"%md\n## Test extraction methods\n\nLoad data from HDFS at the locations where it resides both for the weather data and for the station data. For example station data could be located in '/user/cloudera/data/weather/isd-history.csv'. And for weather data you can use a whole directory, for example '/user/cloudera/data/weather/2014'. Transform the raw data into Python dictionaries using the extraction functions above.\n\nJust to be sure that everything works out fine, print a couple of elements (say 5) from both rdds.","dateUpdated":"Jun 5, 2016 9:15:06 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054303_-1750785289","id":"20160605-091054_342279509","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Test extraction methods</h2>\n<p>Load data from HDFS at the locations where it resides both for the weather data and for the station data. For example station data could be located in '/user/cloudera/data/weather/isd-history.csv'. And for weather data you can use a whole directory, for example '/user/cloudera/data/weather/2014'. Transform the raw data into Python dictionaries using the extraction functions above.</p>\n<p>Just to be sure that everything works out fine, print a couple of elements (say 5) from both rdds.</p>\n"},"dateCreated":"Jun 5, 2016 9:10:54 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2198","dateFinished":"Jun 5, 2016 9:15:04 AM","dateStarted":"Jun 5, 2016 9:15:04 AM","focus":true},{"text":"%pyspark\n\n# Load stations, say from '/user/cloudera/data/weather/isd-history.csv'. Transform the data into Python dictionary using extract_station\nstations = ...\n\n# Print a couple of elements from the transformed RDD\nprint ...\n","dateUpdated":"Jun 5, 2016 9:14:51 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054303_-1750785289","id":"20160605-091054_1730501097","dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2199","focus":true},{"text":"%pyspark\n# Load weather, say from '/user/cloudera/data/weather/2014'. Transform the data into Python dictionary using extract_weather\nweather = ...\n\n# Print a couple of elements from the transformed RDD\nprint ...","dateUpdated":"Jun 5, 2016 9:15:27 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054303_-1750785289","id":"20160605-091054_1160905958","dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2200","focus":true},{"text":"%md\n## Join Data Sets\nIn order to analyse the data, we need to join the weather data with the station data, so we can get more detailed information where the weather actually was recorded.","dateUpdated":"Jun 5, 2016 9:10:54 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054303_-1750785289","id":"20160605-091054_1457503111","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Join Data Sets</h2>\n<p>In order to analyse the data, we need to join the weather data with the station data, so we can get more detailed information where the weather actually was recorded.</p>\n"},"dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2201"},{"text":"%pyspark\n# Create a key for every weather station using the values for 'usaf' and 'wban' from every record. This can be done using the keyBy method.\nstation_index = ...\n\n# Create a key for every weather measurement element using the values for 'usaf' and 'wban' from every record. This can be done using the keyBy method.\nweather_index = ...\n\n# Now join weather and stations together using the keyed data. This can be done using the join method\njoined_weather = ...\n\n# Print some elements from joined_weather.\nprint ...","dateUpdated":"Jun 5, 2016 9:19:47 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054303_-1750785289","id":"20160605-091054_2056308895","dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2202","focus":true},{"text":"%md\n## Create appropriate Keys\nWe want to analyze the data grouped by country and year. So we need to create appropriate keys.\n\nThis will be done using a helper methid extract_country_year_weather, which should return a tuple\n    ((country, year), weather)\nfor every record in joined_weather.\n\nPay attention to the layout of the elements in joined_weather, as can been see from the output above","dateUpdated":"Jun 5, 2016 9:20:55 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054304_-1765020998","id":"20160605-091054_929267094","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Create appropriate Keys</h2>\n<p>We want to analyze the data grouped by country and year. So we need to create appropriate keys.</p>\n<p>This will be done using a helper methid extract_country_year_weather, which should return a tuple</p>\n<pre><code>((country, year), weather)\n</code></pre>\n<p>for every record in joined_weather.</p>\n<p>Pay attention to the layout of the elements in joined_weather, as can been see from the output above</p>\n"},"dateCreated":"Jun 5, 2016 9:10:54 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2203","dateFinished":"Jun 5, 2016 9:20:50 AM","dateStarted":"Jun 5, 2016 9:20:50 AM","focus":true},{"text":"%pyspark\ndef extract_country_year_weather(data):\n    # data is a nested tuple, so we first need to extract the weather and the station data\n    station = ...\n    weather = ...\n    # Now extract country from station\n    country = ...\n    # and the year from the weather measurement data\n    year =  ...\n    return ((country, year), weather)\n\n# Perform extraction\nweather_per_country_and_year = joined_weather.map(extract_country_year_weather)","dateUpdated":"Jun 5, 2016 9:21:18 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054304_-1765020998","id":"20160605-091054_1690967874","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2204"},{"text":"%md\n## Perform Aggregation\nWe want to extract minimum and maximum of wind speed and of temperature. We also want to consider cases where data is not valid (i.e. windSpeedQuality is False or airTemperature is False).\n\nWe will implement custom aggregation functions that work on dictionaries","dateUpdated":"Jun 5, 2016 9:10:54 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054304_-1765020998","id":"20160605-091054_298473920","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Perform Aggregation</h2>\n<p>We want to extract minimum and maximum of wind speed and of temperature. We also want to consider cases where data is not valid (i.e. windSpeedQuality is False or airTemperature is False).</p>\n<p>We will implement custom aggregation functions that work on dictionaries</p>\n"},"dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2205"},{"text":"%pyspark\n\ndef nullsafe_min(a, b):\n    \"\"\"\n    Helper method for taking the min of two values. Also gracefully handles None values\n    \"\"\"\n    from __builtin__ import min\n    if a is None:\n        return b\n    if b is None:\n        return a\n    return min(a,b)\n\n\ndef nullsafe_max(a, b):\n    \"\"\"\n    Helper method for taking the max of two values. Also gracefully handles None values\n    \"\"\"\n    from __builtin__ import max\n    if a is None:\n        return b\n    if b is None:\n        return a\n    return max(a, b)\n\n\n# Neutral value used in aggregation\nzero_wmm = { 'minTemperature':None, 'maxTemperature':None, 'minWindSpeed':None, 'maxWindSpeed':None }\n\n\ndef reduce_wmm(wmm, data):\n    \"\"\"\n    Used for merging in a new weather data set into an existing WeatherMinMax object. The incoming\n    objects will not be modified, instead a new object will be returned.\n    :param wmm: A Python dictionary representing min/max information\n    :param data: A Python dictionary representring weather measurement information\n    :returns: A new Python dictionary representing min/max information\n    \"\"\"\n    minTemperature = ...\n    maxTemperature = ...\n    minWindSpeed = ...\n    maxWindSpeed = ...\n\n    return { 'minTemperature':minTemperature, 'maxTemperature':maxTemperature, 'minWindSpeed':minWindSpeed, 'maxWindSpeed':maxWindSpeed }\n\n\ndef combine_wmm(left, right):\n    \"\"\"\n    Used for combining two WeatherMinMax objects into a new WeatherMinMax object\n    :param self: First Python dictionary representing min/max information\n    :param other: Second Python dictionary representing min/max information\n    :returns: A new Python dictionary representing combined min/max information\n    \"\"\"\n    minTemperature = ...\n    maxTemperature = ...\n    minWindSpeed = ...\n    maxWindSpeed = ...\n\n    return { 'minTemperature':minTemperature, 'maxTemperature':maxTemperature, 'minWindSpeed':minWindSpeed, 'maxWindSpeed':maxWindSpeed }\n\n\n","dateUpdated":"Jun 5, 2016 9:23:31 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054304_-1765020998","id":"20160605-091054_2076857695","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2206"},{"text":"%pyspark\n\n# Aggregate min/max information per year and country\nweather_minmax = weather_per_country_and_year.aggregateByKey(zero_wmm,reduce_wmm, combine_wmm)\n\nprint weather_minmax.take(5)\n","dateUpdated":"Jun 5, 2016 9:27:16 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12,"lineNumbers":false,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054304_-1765020998","id":"20160605-091054_548081527","dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2207","focus":true},{"text":"%md\n## Format Output\nWe want to create CSV data, so we need to reformat the Python dicts to nicely looking strings","dateUpdated":"Jun 5, 2016 9:10:54 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054304_-1765405747","id":"20160605-091054_1048862306","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Format Output</h2>\n<p>We want to create CSV data, so we need to reformat the Python dicts to nicely looking strings</p>\n"},"dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2208"},{"text":"%pyspark\ndef format_result(row):\n    # Every row contains the key and the data.\n    #   key is (country, year)\n    #   value is Python dictionary containing min/max information\n    (k,v) = row\n    # Create a CSV line containing 'country,year,minTemperature,maxTemperature,minWindSpeed,maxWindSpeed'\n    line = ...\n    # Encode as UTF-8, or we might experience some problems\n    return line.encode('utf-8')\n\n# Transform the weather_minmax data using the function format_result\nresult = ...\n\n# print all entries\n...","dateUpdated":"Jun 5, 2016 9:27:18 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054305_-1765405747","id":"20160605-091054_1683002366","dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2209","focus":true},{"dateUpdated":"Jun 5, 2016 9:10:54 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1465143054305_-1765405747","id":"20160605-091054_1713650725","dateCreated":"Jun 5, 2016 9:10:54 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2210"}],"name":"PySpark Weather Analysis Exercise","id":"2BKN42ZMM","angularObjects":{"2BKGW8Q6E":[],"2BP2JWUVF":[],"2BM4Z6DBF":[],"2BJUEKW1N":[],"2BNG99EDF":[],"2BM6V6VQG":[],"2BKHCUTH5":[],"2BJGUFRFB":[],"2BM4XH3S9":[],"2BKRDB8F5":[],"2BJDKPSX9":[],"2BN3H3JME":[],"2BMJC3KG9":[],"2BM79W2EN":[]},"config":{"looknfeel":"default"},"info":{}}