{"paragraphs":[{"text":"%md\n# Weather Data Analytics\nThis notebook performs some basic weather data analytics using the Spark DataFrame interface.","dateUpdated":"Jun 18, 2016 8:17:00 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820067_-1757987338","id":"20160618-081700_156003449","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Weather Data Analytics</h1>\n<p>This notebook performs some basic weather data analytics using the Spark DataFrame interface.</p>\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3368"},{"text":"%md\n# Pretty Printing","dateUpdated":"Jun 18, 2016 8:17:00 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820068_-1759911083","id":"20160618-081700_2019918003","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Pretty Printing</h1>\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3369"},{"text":"import org.apache.spark.sql.Dataset\n\nimplicit class DatasetZeppelinOutput[T](ds:Dataset[T]) {\n    def toZeppelin(limit:Int = -1)  = {\n        val df = ds.toDF\n        println(\"%table\")\n        println(df.schema.map(_.name).mkString(\"\\t\"))\n        val data = if (limit <= 0) df else df.limit(limit)\n        data.collect.foreach(row => println(row.mkString(\"\\t\")))\n    }\n}","dateUpdated":"Jun 18, 2016 8:17:00 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820068_-1759911083","id":"20160618-081700_1340384645","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.sql.Dataset\ndefined class DatasetZeppelinOutput\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3370"},{"text":"%md\n# Weather Classes\nWe want to reuse the WeatherStation and WeatherData classes from our original RDD example, but this time they will be put into a Dataset instead of a RDD","dateUpdated":"Jun 18, 2016 8:17:00 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820068_-1759911083","id":"20160618-081700_741271485","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Weather Classes</h1>\n<p>We want to reuse the WeatherStation and WeatherData classes from our original RDD example, but this time they will be put into a Dataset instead of a RDD</p>\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3371"},{"text":"// This class contains the actual measurement of a weather station at a specific point in time\ncase class WeatherData(\n    date:String,\n    time:String,\n    usaf:String,\n    wban:String,\n    validTemperature:Boolean,\n    temperature:Float,\n    validWindSpeed:Boolean,\n    windSpeed:Float\n)\n\n// This class contains the weather station meta data\ncase class StationData(\n    usaf:String,\n    wban:String,\n    name:String,\n    country:String,\n    state:String,\n    icao:String,\n    latitude:Float,\n    longitude:Float,\n    elevation:Float,\n    date_begin:String,\n    date_end:String\n)\n","dateUpdated":"Jun 18, 2016 8:17:00 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820068_-1759911083","id":"20160618-081700_997212148","result":{"code":"SUCCESS","type":"TEXT","msg":"defined class WeatherData\ndefined class StationData\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3372"},{"text":"%md\n# Load Data\nFirst we need to load the data from Hive tables. This will give us DataFrames, which then need to be converted into Datasets using the WeatherData and StationData classes","dateUpdated":"Jun 18, 2016 8:17:34 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820068_-1759911083","id":"20160618-081700_1182190248","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Load Data</h1>\n<p>First we need to load the data from Hive tables. This will give us DataFrames, which then need to be converted into Datasets using the WeatherData and StationData classes</p>\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3373"},{"text":"// Load table \"training.weather\" as a DataFrame from Hive\n// YOUR CODE HERE\nval weatherDf = ...\n\n// Inspect the schema\nweatherDf.printSchema()\n\n// Load table \"training.stations\" as a DataFrame from Hive\n// YOUR CODE HERE\nval stationsDf = ...\n\n// Inspect the schema\nstationsDf.printSchema()","dateUpdated":"Jun 18, 2016 8:17:43 AM","config":{"enabled":true,"graph":{"mode":"table","height":528,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820068_-1759911083","id":"20160618-081700_552796092","dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3374","focus":true},{"text":"%md\n## Create Dataset from DataFrame\n\nNow we want to create a Dataset from the DataFrame. This is not directly possible, since the columns in Hive have different names than the fields in our case classes. But this can be easily fixed by renaming the columns in the DataFrame. This way we can convert any table with appropriate information into our classes (which might be part of a bigger library and therefore not easily modifyable).\n\nRenaming a column in a DataFrame can be performed very conveniently using\n\n    val new_df = df.withColumnRenamed(\"old_column_name\", \"newColumnName\")\n    \nor you can also add a new column via\n\n    val new_df = df.withColumn(\"newColumnName\", $\"old_column_name\")\n    \nThe second approach accepts a generic Column expression, so you can also perform conversions.\n","dateUpdated":"Jun 18, 2016 8:17:00 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820069_-1760295832","id":"20160618-081700_829661322","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Create Dataset from DataFrame</h2>\n<p>Now we want to create a Dataset from the DataFrame. This is not directly possible, since the columns in Hive have different names than the fields in our case classes. But this can be easily fixed by renaming the columns in the DataFrame. This way we can convert any table with appropriate information into our classes (which might be part of a bigger library and therefore not easily modifyable).</p>\n<p>Renaming a column in a DataFrame can be performed very conveniently using</p>\n<pre><code>val new_df = df.withColumnRenamed(\"old_column_name\", \"newColumnName\")\n</code></pre>\n<p>or you can also add a new column via</p>\n<pre><code>val new_df = df.withColumn(\"newColumnName\", $\"old_column_name\")\n</code></pre>\n<p>The second approach accepts a generic Column expression, so you can also perform conversions.</p>\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3375"},{"text":"import org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.types._\n\n// Create a Dataset[WeatherData] from the DataFrame weatherDf. You need to rename and convert some columns:\n//    wind_speed(Double) to windSPeed(Float)\n//    air_temperature(Double) to temperature(Float)\n//    wind_speed_qual(String) to validWindSpeed(Boolean)\n//    air_temperature_qual(String) to validTemperature(Boolean)\n// Note that the two quality columns contain 0 or 1, meaning True or False\n\n// YOUR CODE HERE\nval weather:Dataset[WeatherData] = ...\n","dateUpdated":"Jun 18, 2016 8:19:49 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820069_-1760295832","id":"20160618-081700_506518883","dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3376","focus":true},{"text":"// Create a Dataset[StationData] stations from the DataFrame stationsDf. You need to rename and convert some columns:\n//    longitude(String) to longitude(Float)\n//    latitude(String) to latitude(Float)\n//    elevation(String) to elevation(Float)\n\n// YOUR CODE HERE\nval stations:Dataset[StationData] = ...\n","dateUpdated":"Jun 18, 2016 8:19:49 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820069_-1760295832","id":"20160618-081700_437689873","dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3377","focus":true},{"text":"%md\n# Analytics\n\nNow that we have everything in Hive, we can do the analysis easily.\n\n1. Define some helper classes for aggregation\n2. Join both Datasets on station code (wban and usaf)\n3. Group by country and year\n4. Aggregate minimum/maximum values for wind and temperature, pay attention to quality!\n5. Print the results, such that Zeppelin can make nice graphics :)","dateUpdated":"Jun 18, 2016 8:17:00 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820069_-1760295832","id":"20160618-081700_1122204536","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Analytics</h1>\n<p>Now that we have everything in Hive, we can do the analysis easily.</p>\n<ol>\n<li>Define some helper classes for aggregation</li>\n<li>Join both Datasets on station code (wban and usaf)</li>\n<li>Group by country and year</li>\n<li>Aggregate minimum/maximum values for wind and temperature, pay attention to quality!</li>\n<li>Print the results, such that Zeppelin can make nice graphics :)</li>\n</ol>\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3378"},{"text":"%md\n## 1. Define Helper Classes\n\nAgain we will use a WeatherMinMax class used in the aggregation process. Moreover we will define an additional class WeatherResult which will additionally store the country and year, such that we will have a `Dataset[WeatherResult]` in the end.","dateUpdated":"Jun 18, 2016 8:17:00 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820069_-1760295832","id":"20160618-081700_1690248598","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>1. Define Helper Classes</h2>\n<p>Again we will use a WeatherMinMax class used in the aggregation process. Moreover we will define an additional class WeatherResult which will additionally store the country and year, such that we will have a <code>Dataset[WeatherResult]</code> in the end.</p>\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3379"},{"text":"// Minimum / Maximum aggregator, used later\ncase class WeatherMinMax(\n    minTemperature:Float = 99999,\n    maxTemperature:Float = -99999,\n    minWindSpeed:Float = 99999,\n    maxWindSpeed:Float = -99999\n) {\n  // Reduce method for merging in another WeatherData entry\n  def reduce(other:WeatherData) = {\n    val minT = if(other.validTemperature) minTemperature.min(other.temperature) else minTemperature\n    val maxT = if(other.validTemperature) maxTemperature.max(other.temperature) else maxTemperature\n    val minW = if(other.validWindSpeed) minWindSpeed.min(other.windSpeed) else minWindSpeed\n    val maxW = if(other.validWindSpeed) maxWindSpeed.max(other.windSpeed) else maxWindSpeed\n    WeatherMinMax(minT,maxT,minW,maxW)\n  }\n}\n\n// Class for holding the result\ncase class WeatherResult(\n    year:String,\n    country:String,\n    minTemperature:Float,\n    maxTemperature:Float,\n    minWindSpeed:Float,\n    maxWindSpeed:Float\n) { }","dateUpdated":"Jun 18, 2016 8:17:00 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820070_-1759141585","id":"20160618-081700_1094504070","result":{"code":"SUCCESS","type":"TEXT","msg":"defined class WeatherMinMax\ndefined class WeatherResult\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3380"},{"text":"%md\n## 2. Join Weather Measurements with Station Data\n\nNow we again need to lookup station metadata in order to lookup the country of the weather station. This can be done via\n\n    df1.joinWith(df2, join_condition)\n    \nNote that when you try to join on a column, which is present in both datasets, you can use the `as(alias)` method of a dataset, to specify an alias which can be used in column names. For example\n\n    df.as(\"measurement\").select($\"measurement.temperature\".as[Double])\n    \nBut you probably do not need select, but the same pattern can be used for joining.\n\n### Hint\n\nThe result should be a `Dataset[(WeatherData,StationData)]`","dateUpdated":"Jun 18, 2016 8:27:39 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820070_-1759141585","id":"20160618-081700_2132467935","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>2. Join Weather Measurements with Station Data</h2>\n<p>Now we again need to lookup station metadata in order to lookup the country of the weather station. This can be done via</p>\n<pre><code>df1.joinWith(df2, join_condition)\n</code></pre>\n<p>Note that when you try to join on a column, which is present in both datasets, you can use the <code>as(alias)</code> method of a dataset, to specify an alias which can be used in column names. For example</p>\n<pre><code>df.as(\"measurement\").select($\"measurement.temperature\".as[Double])\n</code></pre>\n<p>But you probably do not need select, but the same pattern can be used for joining.</p>\n<h3>Hint</h3>\n<p>The result should be a <code>Dataset[(WeatherData,StationData)]</code></p>\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3381","dateFinished":"Jun 18, 2016 8:27:37 AM","dateStarted":"Jun 18, 2016 8:27:37 AM","focus":true},{"text":"// YOUR CODE HERE\nval joined_weather = ...","dateUpdated":"Jun 18, 2016 8:21:11 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820070_-1759141585","id":"20160618-081700_1033998762","result":{"code":"SUCCESS","type":"TEXT","msg":"joined_weather: org.apache.spark.sql.Dataset[(WeatherData, StationData)] = [_1: struct<date:string,time:string,usaf:string,wban:string,validTemperature:boolean,temperature:float,validWindSpeed:boolean,windSpeed:float>, _2: struct<usaf:string,wban:string,name:string,country:string,state:string,icao:string,latitude:float,longitude:float,elevation:float,date_begin:string,date_end:string>]\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3382","focus":true},{"text":"%md\n## 3. Group Data on Year and Country\n\nNow we want to create groups of the weather data for every country and year. This can be done using the `groupBy` method of a Dataset. In cases where you have Scala problems complaining about unknown types of function arguments, you can use the following construction in your groupBy function:\n\n    df.groupBy( _ match { case (w:TypeName,s:OtherTypeName) => perform_calculation(w,s) } )\n    \nPossibly there are multiple ways to define the grouping key, but I would propose to use a pair `(year:String,country:String)` as the grouping key. This way we have both parts seperated and can access both year and country easily.\n\n### Hint\n\nThe result should be a `GroupedDataset[(String,String),(WeatherData,StationData)]`.    ","dateUpdated":"Jun 18, 2016 8:27:01 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820070_-1759141585","id":"20160618-081700_979521410","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>3. Group Data on Year and Country</h2>\n<p>Now we want to create groups of the weather data for every country and year. This can be done using the <code>groupBy</code> method of a Dataset. In cases where you have Scala problems complaining about unknown types of function arguments, you can use the following construction in your groupBy function:</p>\n<pre><code>df.groupBy( _ match { case (w:TypeName,s:OtherTypeName) =&gt; perform_calculation(w,s) } )\n</code></pre>\n<p>Possibly there are multiple ways to define the grouping key, but I would propose to use a pair <code>(year:String,country:String)</code> as the grouping key. This way we have both parts seperated and can access both year and country easily.</p>\n<h3>Hint</h3>\n<p>The result should be a <code>GroupedDataset[(String,String),(WeatherData,StationData)]</code>.</p>\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3383","dateFinished":"Jun 18, 2016 8:26:57 AM","dateStarted":"Jun 18, 2016 8:26:57 AM","focus":true},{"text":"// YOUR CODE HERE\nval grouped_weather = ...","dateUpdated":"Jun 18, 2016 8:21:48 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820070_-1759141585","id":"20160618-081700_206699968","result":{"code":"SUCCESS","type":"TEXT","msg":"grouped_weather: org.apache.spark.sql.GroupedDataset[(String, String),(WeatherData, StationData)] = org.apache.spark.sql.GroupedDataset@aacb1eb\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3384","focus":true},{"text":"%md\n## 4. Aggregate Data\n\nNow that we have data groups with the all relevant weather information of a country and year contained within a single group, we can perform the aggregation. In order to do that, we will use the `mapGroups` method of `GroupedDataset`. This will require a function `(key:KeyType,values:Iterator[ValueType]) => ...` as the argument. Think about:\n1. What is your KeyType here?\n2. What is your ValueType here?\n \nOnce you figured this out, you can use the Scala `foldLeft` method of a Scala `Iterator` class in order to use the WeatherMinMax class for doing the aggregation.\n\nThe result should be a Dataset with type `(year:String,country:String,wmm:WeatherMinMax)`.\n\n### Hint for Scala\n\nAgain for working with `mapGroups` you might require a Scala partial function with a pattern like\n\n    { case ((a:SomeType, b:OtherType), c:ThirdType) => ... }\n    \n### Hint\n\nThe result should be similar to `Dataset[(String,String,WeatherMinMax)]`    ","dateUpdated":"Jun 18, 2016 8:29:29 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820070_-1759141585","id":"20160618-081700_1525304977","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>4. Aggregate Data</h2>\n<p>Now that we have data groups with the all relevant weather information of a country and year contained within a single group, we can perform the aggregation. In order to do that, we will use the <code>mapGroups</code> method of <code>GroupedDataset</code>. This will require a function <code>(key:KeyType,values:Iterator[ValueType]) =&gt; ...</code> as the argument. Think about:</p>\n<ol>\n<li>What is your KeyType here?</li>\n<li>What is your ValueType here?</li>\n</ol>\n<p>Once you figured this out, you can use the Scala <code>foldLeft</code> method of a Scala <code>Iterator</code> class in order to use the WeatherMinMax class for doing the aggregation.</p>\n<p>The result should be a Dataset with type <code>(year:String,country:String,wmm:WeatherMinMax)</code>.</p>\n<h3>Hint for Scala</h3>\n<p>Again for working with <code>mapGroups</code> you might require a Scala partial function with a pattern like</p>\n<pre><code>{ case ((a:SomeType, b:OtherType), c:ThirdType) =&gt; ... }\n</code></pre>\n<h3>Hint</h3>\n<p>The result should be similar to <code>Dataset[(String,String,WeatherMinMax)]</code></p>\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3385","dateFinished":"Jun 18, 2016 8:29:26 AM","dateStarted":"Jun 18, 2016 8:29:25 AM","focus":true},{"text":"// Use mapGroups in order to perform the aggregation. The aggregation itself can be done using Scala foldLeft together with the WeatherMinMax helper class\n// YOUR CODE HERE\nval analysed_weather = ...","dateUpdated":"Jun 18, 2016 8:22:24 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820071_-1759526334","id":"20160618-081700_1288454200","dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3386","focus":true},{"text":"// Have a look at the schema. In my solution it would be nested.\nanalysed_weather.printSchema()","dateUpdated":"Jun 18, 2016 8:23:02 AM","config":{"enabled":true,"graph":{"mode":"table","height":150,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820071_-1759526334","id":"20160618-081700_1972126699","dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3387","focus":true},{"text":"%md\n## 5. Print Result\n\nOf course we want again a nice visualisation. But this time, this requires an additional step, because we have a nested schema (see above). So we fix that by mapping all result data and put it into the `WeatherResult' class supplied above. Again you might need to use a Scala partial function with a pattern matching expression like\n\n    { case (a:SomeType, b:SomeOtherType) => someCalculation(a,b) }\n    \nThis should then give you a new Dataset with values of type `WeatherResult`. This can then be easily visualized using the `toZeppelin` method.","dateUpdated":"Jun 18, 2016 8:17:00 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820071_-1759526334","id":"20160618-081700_1270822295","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>5. Print Result</h2>\n<p>Of course we want again a nice visualisation. But this time, this requires an additional step, because we have a nested schema (see above). So we fix that by mapping all result data and put it into the `WeatherResult' class supplied above. Again you might need to use a Scala partial function with a pattern matching expression like</p>\n<pre><code>{ case (a:SomeType, b:SomeOtherType) =&gt; someCalculation(a,b) }\n</code></pre>\n<p>This should then give you a new Dataset with values of type <code>WeatherResult</code>. This can then be easily visualized using the <code>toZeppelin</code> method.</p>\n"},"dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3388"},{"text":"// Transform the records inside the Dataset into WeatherResult instances. \n// YOUR CODE HERE\nval result:Dataset[WeatherResult] = ...\n\n// Print result as a Zeppelin table\nresult.toZeppelin()","dateUpdated":"Jun 18, 2016 8:23:31 AM","config":{"enabled":true,"graph":{"mode":"multiBarChart","height":94,"optionOpen":false,"keys":[{"name":"year","index":0,"aggr":"sum"}],"values":[{"name":"minWindSpeed","index":4,"aggr":"min"},{"name":"maxWindSpeed","index":5,"aggr":"max"},{"name":"minTemperature","index":2,"aggr":"min"},{"name":"maxTemperature","index":3,"aggr":"max"}],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820071_-1759526334","id":"20160618-081700_1985442650","dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3389","focus":true},{"dateUpdated":"Jun 18, 2016 8:17:00 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466237820071_-1759526334","id":"20160618-081700_1733711464","dateCreated":"Jun 18, 2016 8:17:00 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3390"}],"name":"Weather Dataset Analysis Exercise","id":"2BMZPCYK9","angularObjects":{"2B44YVSN1":[],"2AJXGMUUJ":[],"2AK8P7CPX":[],"2AM1YV5CU":[],"2AKK3QQXU":[],"2ANGGHHMQ":[]},"config":{"looknfeel":"default"},"info":{}}