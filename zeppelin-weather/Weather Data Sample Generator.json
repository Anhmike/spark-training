{"paragraphs":[{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466248604330_1634478563","id":"20160618-111644_1635493301","dateCreated":"Jun 18, 2016 11:16:44 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4967","text":"%md\n# Generating a Sample of Weather Data\nThis notebook will create a sample of weather data to be used for stream processing. We require the sample to contain all years and stations, and the samples should be ordered by timestamp.\n\nBecause our streaming application expects the raw data, we simply access the original raw files.","dateUpdated":"Jun 18, 2016 11:18:19 AM","dateFinished":"Jun 18, 2016 11:18:15 AM","dateStarted":"Jun 18, 2016 11:18:15 AM","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Generating a Sample of Weather Data</h1>\n<p>This notebook will create a sample of weather data to be used for stream processing. We require the sample to contain all years and stations, and the samples should be ordered by timestamp.</p>\n<p>Because our streaming application expects the raw data, we simply access the original raw files.</p>\n"},"focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466248668614_182344269","id":"20160618-111748_1432174106","dateCreated":"Jun 18, 2016 11:17:48 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4984","dateUpdated":"Jun 18, 2016 11:18:46 AM","dateFinished":"Jun 18, 2016 11:18:46 AM","dateStarted":"Jun 18, 2016 11:18:46 AM","result":{"code":"SUCCESS","type":"TEXT","msg":"storageLocation: String = s3://is24-data-dev-spark-training/data/weather\n"},"text":"val storageLocation = \"s3://is24-data-dev-spark-training/data/weather\""},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466248726065_-1697310638","id":"20160618-111846_1820606220","dateCreated":"Jun 18, 2016 11:18:46 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5010","dateUpdated":"Jun 18, 2016 11:19:38 AM","dateFinished":"Jun 18, 2016 11:19:39 AM","dateStarted":"Jun 18, 2016 11:19:38 AM","result":{"code":"SUCCESS","type":"TEXT","msg":"raw_weather_years: scala.collection.immutable.IndexedSeq[org.apache.spark.rdd.RDD[String]] = Vector(s3://is24-data-dev-spark-training/data/weather/2003 MapPartitionsRDD[298] at textFile at <console>:53, s3://is24-data-dev-spark-training/data/weather/2004 MapPartitionsRDD[300] at textFile at <console>:53, s3://is24-data-dev-spark-training/data/weather/2005 MapPartitionsRDD[302] at textFile at <console>:53, s3://is24-data-dev-spark-training/data/weather/2006 MapPartitionsRDD[304] at textFile at <console>:53, s3://is24-data-dev-spark-training/data/weather/2007 MapPartitionsRDD[306] at textFile at <console>:53, s3://is24-data-dev-spark-training/data/weather/2008 MapPartitionsRDD[308] at textFile at <console>:53, s3://is24-data-dev-spark-training/data/weather/2009 MapPartitionsRDD[310] at te...raw_weather: org.apache.spark.rdd.RDD[String] = UnionRDD[321] at union at <console>:56\n"},"text":"// Get RDDs for all years\nval raw_weather_years = (2003 to 2014) map {i => sc.textFile(storageLocation + \"/\" + i.toString)}\n\n// Put all RDDs into a single one using the \"union\" method of the SparkContext\nval raw_weather = sc.union(raw_weather_years)"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466248778332_-1475249226","id":"20160618-111938_294721450","dateCreated":"Jun 18, 2016 11:19:38 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5034","dateUpdated":"Jun 18, 2016 11:19:51 AM","dateFinished":"Jun 18, 2016 11:20:00 AM","dateStarted":"Jun 18, 2016 11:19:51 AM","result":{"code":"SUCCESS","type":"TEXT","msg":"0168010015999992003010111504+61450+005870FM-15+0150ENBL V0201801N001510030019N0030001N1-00201-00301999999ADDGA1091+003009999GF109991999999000251999999MA1099901999999MW1861REMMET058ENBL 011150Z 18003KT 3000 SHSN SCT003 VV010 M02/M03 Q0999;EQDQ01+000003PRSWM1Q02    003PRSWM1\n0098010015999992003010112204+61450+005870FM-16+0150ENBL V0201601N001012200019N0020001N1+99999+99999999999ADDMW1001REMMET061ENBL 011220Z 16002KT 2000E -SHSN SCT003 BKN010 M02/M03 Q0999;EQDQ01+000000PRSWM2\n0098010015999992003010112504+61450+005870FM-15+0150ENBL V0202301N001512200019N0025001N1+99999+99999999999ADDMW1001REMMET061ENBL 011250Z 23003KT 2500E -SHSN SCT002 BKN010 M02/M03 Q0999;EQDQ01+000000PRSWM2\n0161010015999992003010113204+61450+005870FM-15+0150ENBL V0209999C000010015019N0012001N1-00101-00201999999ADDGA1091+001509999GF109991999999000251999999MA1099801999999MW1861REMMET051ENBL 011320Z 00000KT 1200 SHSN VV005 M01/M02 Q0998;EQDQ01+000003PRSWM1Q02    003PRSWM1\n0161010015999992003010113504+61450+005870FM-15+0150ENBL V0202101N001010012019N0010001N1-00101-00201999999ADDGA1091+001209999GF109991999999000251999999MA1099801999999MW1861REMMET051ENBL 011350Z 21002KT 1000 SHSN VV004 M01/M02 Q0998;EQDQ01+000003PRSWM1Q02    003PRSWM1\n"},"text":"raw_weather.take(5).foreach(println)"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466248791611_2040448305","id":"20160618-111951_2046894375","dateCreated":"Jun 18, 2016 11:19:51 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5054","dateUpdated":"Jun 18, 2016 11:29:26 AM","dateFinished":"Jun 18, 2016 11:30:45 AM","dateStarted":"Jun 18, 2016 11:29:26 AM","result":{"code":"SUCCESS","type":"TEXT","msg":"sample: org.apache.spark.rdd.RDD[String] = PartitionwiseSampledRDD[334] at sample at <console>:56\nsorted: org.apache.spark.rdd.RDD[String] = CoalescedRDD[343] at coalesce at <console>:58\n"},"text":"val sample = raw_weather.sample(false, 0.01)\nval sorted = sample.sortBy(_.substring(15,27)).coalesce(10)\nsorted.saveAsTextFile(\"/user/hadoop/weather_sample\")"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466249235692_-1297600336","id":"20160618-112715_1007313092","dateCreated":"Jun 18, 2016 11:27:15 AM","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5136"}],"name":"Weather Data Sample Generator","id":"2BNW7RPVS","angularObjects":{"2B44YVSN1":[],"2AJXGMUUJ":[],"2AK8P7CPX":[],"2AM1YV5CU":[],"2AKK3QQXU":[],"2ANGGHHMQ":[]},"config":{"looknfeel":"default"},"info":{}}