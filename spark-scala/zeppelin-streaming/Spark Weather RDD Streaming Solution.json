{"paragraphs":[{"text":"%md\n# Pretty Printing\n\nAs usual we first define a helper method for pretty print Spark DataFrames","dateUpdated":"2017-01-09T00:31:22-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483950657012_1116516505","id":"20170109-003057_1724816119","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Pretty Printing</h1>\n<p>As usual we first define a helper method for pretty print Spark DataFrames</p>\n"},"dateCreated":"2017-01-09T00:30:57-0800","dateStarted":"2017-01-09T00:31:17-0800","dateFinished":"2017-01-09T00:31:17-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9964"},{"text":"implicit class ZeppelinOutput[T](ds:org.apache.spark.sql.Dataset[T]) {\n    def toZeppelin(limit:Int = -1)  = {\n        val df = ds.toDF\n        println(\"%table\")\n        println(df.schema.map(_.name).mkString(\"\\t\"))\n        val data = if (limit <= 0) df else df.limit(limit)\n        data.collect.foreach(row => println(row.mkString(\"\\t\")))\n    }\n}","dateUpdated":"2017-01-09T01:01:01-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483950653856_742540574","id":"20170109-003053_1651278420","result":{"code":"SUCCESS","type":"TEXT","msg":"\ndefined class ZeppelinOutput\n"},"dateCreated":"2017-01-09T00:30:53-0800","dateStarted":"2017-01-09T01:01:01-0800","dateFinished":"2017-01-09T01:01:19-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9965"},{"text":"%md\n# Load Station Data\n\nNow we load the station meta data using traditional SparkSQL DataFrame methods. Since the meta data is stored as a simple CSV, this should be simple.","dateUpdated":"2017-01-09T00:28:24-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483950443958_1485969039","id":"20170109-002723_896381522","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Load Station Data</h1>\n<p>Now we load the station meta data using traditional SparkSQL DataFrame methods. Since the meta data is stored as a simple CSV, this should be simple.</p>\n"},"dateCreated":"2017-01-09T00:27:23-0800","dateStarted":"2017-01-09T00:28:21-0800","dateFinished":"2017-01-09T00:28:21-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9966"},{"text":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.FloatType\nimport org.apache.spark.sql.types.DateType\n\ndef extractFloat = udf((v:String) => if (v != null) v.toFloat else None, FloatType)\n\nval isdSchema = StructType(\n        StructField(\"usaf\", StringType) ::\n        StructField(\"wban\", StringType) ::\n        StructField(\"name\", StringType) ::\n        StructField(\"country\", StringType) ::\n        StructField(\"state\", StringType) ::\n        StructField(\"icao\", StringType) ::\n        StructField(\"latitude\", StringType) ::\n        StructField(\"longitude\", StringType) ::\n        StructField(\"elevation\", StringType) ::\n        StructField(\"date_begin\", DateType) ::\n        StructField(\"date_end\", DateType) ::\n        Nil\n    )\nval isd = sqlContext.read\n    .option(\"header\",\"true\")\n    .option(\"dateFormat\",\"yyyyMMdd\")\n    .schema(isdSchema)\n    .csv(\"/user/training/data/weather/isd-history\")\n    .withColumn(\"latitude\", extractFloat($\"latitude\"))\n    .withColumn(\"longitude\", extractFloat($\"longitude\"))\n    .withColumn(\"elevation\", extractFloat($\"elevation\"))\n    \n\nisd.toZeppelin(10)","dateUpdated":"2017-01-09T01:15:19-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"usaf","index":0,"aggr":"sum"}],"values":[{"name":"wban","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"usaf","index":0,"aggr":"sum"},"yAxis":{"name":"wban","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483950501913_831788640","id":"20170109-002821_386723165","result":{"code":"ERROR","type":"TEXT","msg":"\nimport org.apache.spark.sql.types.StructType\n\nimport org.apache.spark.sql.types.StructField\n\nimport org.apache.spark.sql.types.StringType\n\nimport org.apache.spark.sql.types.FloatType\n\nimport org.apache.spark.sql.types.DateType\n\nextractFloat: org.apache.spark.sql.expressions.UserDefinedFunction\n\nisdSchema: org.apache.spark.sql.types.StructType = StructType(StructField(usaf,StringType,true), StructField(wban,StringType,true), StructField(name,StringType,true), StructField(country,StringType,true), StructField(state,StringType,true), StructField(icao,StringType,true), StructField(latitude,StringType,true), StructField(longitude,StringType,true), StructField(elevation,StringType,true), StructField(date_begin,DateType,true), StructField(date_end,DateType,true))\n\nisd: org.apache.spark.sql.DataFrame = [usaf: string, wban: string ... 9 more fields]\n\n\n\n<console>:39: error: value toZeppelin is not a member of org.apache.spark.sql.DataFrame\n       isd.toZeppelin(10)\n           ^\n"},"dateCreated":"2017-01-09T00:28:21-0800","dateStarted":"2017-01-09T01:15:20-0800","dateFinished":"2017-01-09T01:15:42-0800","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:9967","focus":true},{"text":"isd.printSchema()","dateUpdated":"2017-01-09T01:15:25-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483950602155_-872158614","id":"20170109-003002_1177603813","result":{"code":"SUCCESS","type":"TEXT","msg":"root\n |-- usaf: string (nullable = true)\n |-- wban: string (nullable = true)\n |-- name: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n |-- icao: string (nullable = true)\n |-- latitude: float (nullable = true)\n |-- longitude: float (nullable = true)\n |-- elevation: float (nullable = true)\n |-- date_begin: date (nullable = true)\n |-- date_end: date (nullable = true)\n\n"},"dateCreated":"2017-01-09T00:30:02-0800","dateStarted":"2017-01-09T01:15:25-0800","dateFinished":"2017-01-09T01:15:42-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9968","focus":true},{"text":"%md\n## 1. Create a Streaming Context\n\nStreaming is performed using a special StreamingContext, which can be constructed from an existing SparkContext. Additionally a window size for micro batches is required.\n","dateUpdated":"2017-01-09T02:03:06-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483950359165_-1012913771","id":"20170109-002559_1248270322","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>1. Create a Streaming Context</h2>\n<p>Streaming is performed using a special StreamingContext, which can be constructed from an existing SparkContext. Additionally a window size for micro batches is required.</p>\n"},"dateCreated":"2017-01-09T00:25:59-0800","dateStarted":"2017-01-09T02:03:00-0800","dateFinished":"2017-01-09T02:03:00-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9971","focus":true},{"text":"import org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.Seconds\n\nval ssc = new StreamingContext(sc, Seconds(1))","dateUpdated":"2017-01-09T02:10:00-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483950432381_-227717886","id":"20170109-002712_346811562","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.streaming.StreamingContext\n\nimport org.apache.spark.streaming.Seconds\n\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@7833bd07\n"},"dateCreated":"2017-01-09T00:27:12-0800","dateStarted":"2017-01-09T02:10:00-0800","dateFinished":"2017-01-09T02:10:01-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9972","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483956104994_-1762035277","id":"20170109-020144_1560104238","dateCreated":"2017-01-09T02:01:44-0800","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11763","text":"%md\n## 2. Connect to Data Source\n\nAgain we connect to a simple socket as the datasource. The socket will stream weather data samples in raw format, i.e. one record per line.","dateUpdated":"2017-01-09T02:03:18-0800","dateFinished":"2017-01-09T02:03:15-0800","dateStarted":"2017-01-09T02:03:15-0800","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>2. Connect to Data Source</h2>\n<p>Again we connect to a simple socket as the datasource. The socket will stream weather data samples in raw format, i.e. one record per line.</p>\n"}},{"text":"import org.apache.spark.storage.StorageLevel\n\nval stream = ssc.socketTextStream(\"localhost\", 9977, StorageLevel.MEMORY_ONLY)","dateUpdated":"2017-01-09T02:10:02-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483951801636_885710372","id":"20170109-005001_1008352335","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.storage.StorageLevel\n\nstream: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@38d40fe1\n"},"dateCreated":"2017-01-09T00:50:01-0800","dateStarted":"2017-01-09T02:10:03-0800","dateFinished":"2017-01-09T02:10:03-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9973","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483954032366_1011862563","id":"20170109-012712_136743186","dateCreated":"2017-01-09T01:27:12-0800","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11381","text":"%md\n## 3. Extract Weather Data\n\nWe create a case class for representing the relevant weather information. We will fill the information using the `map` method of the Spark DStram.\n","dateUpdated":"2017-01-09T02:04:25-0800","dateFinished":"2017-01-09T02:04:21-0800","dateStarted":"2017-01-09T02:04:21-0800","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>3. Extract Weather Data</h2>\n<p>We create a case class for representing the relevant weather information. We will fill the information using the <code>map</code> method of the Spark DStram.</p>\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483953986374_1592555922","id":"20170109-012626_729864789","dateCreated":"2017-01-09T01:26:26-0800","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11311","text":"case class WeatherData(\n    date:String,\n    time:String,\n    usaf:String,\n    wban:String,\n    airTemperatureQuality:Int,\n    airTemperature:Float,\n    windSpeedQuality:Int,\n    windSpeed:Float\n)\n\nval weatherData = stream.map { row => \n    val date = row.substring(15,23)\n    val time = row.substring(23,27)\n    val usaf = row.substring(4,10)\n    val wban = row.substring(10,15)\n    val airTemperatureQuality = row.charAt(92).toInt - '0'.toInt\n    val airTemperature = row.substring(87,92).toFloat/10\n    val windSpeedQuality = row.charAt(69) - '0'.toInt\n    val windSpeed = row.substring(65,69).toFloat/10\n\n    WeatherData(date,time,usaf,wban,airTemperatureQuality,airTemperature,windSpeedQuality,windSpeed)\n}\n    ","dateUpdated":"2017-01-09T02:10:04-0800","dateFinished":"2017-01-09T02:10:05-0800","dateStarted":"2017-01-09T02:10:05-0800","result":{"code":"SUCCESS","type":"TEXT","msg":"\ndefined class WeatherData\n\nweatherData: org.apache.spark.streaming.dstream.DStream[WeatherData] = org.apache.spark.streaming.dstream.MappedDStream@5ada62bd\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483954121144_312327766","id":"20170109-012841_979692617","dateCreated":"2017-01-09T01:28:41-0800","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11511","text":"%md\n## 4. Create a Sliding Window\n\nAgain we want to create a sliding window which contains the last 10 seconds worth of data and moves forward every second.","dateUpdated":"2017-01-09T02:05:00-0800","dateFinished":"2017-01-09T02:04:57-0800","dateStarted":"2017-01-09T02:04:57-0800","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>4. Create a Sliding Window</h2>\n<p>Again we want to create a sliding window which contains the last 10 seconds worth of data and moves forward every second.</p>\n"}},{"text":"val windowedData = weatherData.window(Seconds(10), Seconds(1))","dateUpdated":"2017-01-09T02:10:08-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483952085300_-434380073","id":"20170109-005445_504096722","result":{"code":"SUCCESS","type":"TEXT","msg":"\nwindowedData: org.apache.spark.streaming.dstream.DStream[WeatherData] = org.apache.spark.streaming.dstream.WindowedDStream@fb7cc14\n"},"dateCreated":"2017-01-09T00:54:45-0800","dateStarted":"2017-01-09T02:10:08-0800","dateFinished":"2017-01-09T02:10:08-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9974","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483954134565_1177844225","id":"20170109-012854_2052710484","dateCreated":"2017-01-09T01:28:54-0800","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11590","text":"%md\n## 5. Performed grouping and Aggregation\n\nWe now perform the grouping and aggregation, such that we calculate for every 10 second window the following metrics per year and country:\n* Minimum air temperature\n* Maximum air temperature\n* Minimum wind speed\n* Maximum wind speed\n\nAgain we need to evaulate the \"quality\" fields of the incoming data to decide if the correspong wind speed or air temeprature is valid.\n\nIn order to use a higher level API than the RDD API, we simply convcert the Weather RDD to a Dataset using the `toDS` method. Then we can use any DataFrame and/or Dataset method for joining, grouping and aggregation.\n\nWe will both print the final results using the `print` DStream action and also register them as a temporary table using the `createOrReplaceTempView` method of the Dataset.","dateUpdated":"2017-01-09T02:09:38-0800","dateFinished":"2017-01-09T02:09:34-0800","dateStarted":"2017-01-09T02:09:34-0800","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>5. Performed grouping and Aggregation</h2>\n<p>We now perform the grouping and aggregation, such that we calculate for every 10 second window the following metrics per year and country:</p>\n<ul>\n<li>Minimum air temperature</li>\n<li>Maximum air temperature</li>\n<li>Minimum wind speed</li>\n<li>Maximum wind speed</li>\n</ul>\n<p>Again we need to evaulate the &ldquo;quality&rdquo; fields of the incoming data to decide if the correspong wind speed or air temeprature is valid.</p>\n<p>In order to use a higher level API than the RDD API, we simply convcert the Weather RDD to a Dataset using the <code>toDS</code> method. Then we can use any DataFrame and/or Dataset method for joining, grouping and aggregation.</p>\n<p>We will both print the final results using the <code>print</code> DStream action and also register them as a temporary table using the <code>createOrReplaceTempView</code> method of the Dataset.</p>\n"}},{"text":"windowedData\n    .transform(rdd => {\n        // 1. Convert RDD to a Dataset[WeatherData]\n        val weather = rdd.toDS\n        // Perform calculation\n        val result = weather\n            // 2. Join with isd data, using 'usaf' and 'wban' columns\n            .join(isd, weather(\"usaf\") === isd(\"usaf\") && weather(\"wban\") === isd(\"wban\"))\n            // 3. Extract year from date column (first four letters), store it in 'year'\n            .withColumn(\"year\", weather(\"date\").substr(0,4))\n            // 4. Group by country (from isd) and year (from above)\n            .groupBy(isd(\"country\"), $\"year\")\n            // 5. Perform aggregations of min/max of temperature and wind speed\n            .agg(\n                min(when(col(\"airTemperatureQuality\") === lit(1), col(\"airTemperature\")).otherwise(9999)).as(\"temp_min\"),\n                max(when(col(\"airTemperatureQuality\") === lit(1), col(\"airTemperature\")).otherwise(-9999)).as(\"temp_max\"),\n                min(when(col(\"windSpeedQuality\") === lit(1), col(\"windSpeed\")).otherwise(9999)).as(\"wind_min\"),\n                max(when(col(\"windSpeedQuality\") === lit(1), col(\"windSpeed\")).otherwise(-9999)).as(\"wind_max\")\n            )\n        // 6. Store resulting Dataset as a temporary view using 'createOrReplaceTempView' method\n        result.createOrReplaceTempView(\"weather_agg\")\n        // 7. Return RDD of Dataset in order to conform with the DStream API\n        result.rdd\n    })\n    // Print first 10 results\n    .print(10)","dateUpdated":"2017-01-09T02:19:02-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483952154359_987738350","id":"20170109-005554_729378682","dateCreated":"2017-01-09T00:55:54-0800","dateStarted":"2017-01-09T02:19:02-0800","dateFinished":"2017-01-09T02:19:03-0800","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:9975","errorMessage":"","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483954157172_-493859098","id":"20170109-012917_291516717","dateCreated":"2017-01-09T01:29:17-0800","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11672","text":"%md\n# Start!","dateUpdated":"2017-01-09T02:02:22-0800","dateFinished":"2017-01-09T02:02:22-0800","dateStarted":"2017-01-09T02:02:22-0800","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Start!</h1>\n"}},{"text":"ssc.start()","dateUpdated":"2017-01-09T02:10:13-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483952215998_986092085","id":"20170109-005655_1237628677","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-01-09T00:56:55-0800","dateStarted":"2017-01-09T02:10:13-0800","dateFinished":"2017-01-09T02:10:14-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9976","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483956654453_-1702605513","id":"20170109-021054_1143897633","dateCreated":"2017-01-09T02:10:54-0800","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:12115","text":"%md\n## Peek inside using SQL\n\nSince we registered the result as a SparkSQL temporary table, we can peek inside.","dateUpdated":"2017-01-09T02:11:22-0800","dateFinished":"2017-01-09T02:11:19-0800","dateStarted":"2017-01-09T02:11:19-0800","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Peek inside using SQL</h2>\n<p>Since we registered the result as a SparkSQL temporary table, we can peek inside.</p>\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"country","index":0,"aggr":"sum"}],"values":[{"name":"year","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"country","index":0,"aggr":"sum"},"yAxis":{"name":"year","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483956619339_1433836872","id":"20170109-021019_1195681982","dateCreated":"2017-01-09T02:10:19-0800","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:12001","text":"%sql\nselect * from weather_agg","dateUpdated":"2017-01-09T02:10:37-0800","dateFinished":"2017-01-09T02:10:38-0800","dateStarted":"2017-01-09T02:10:37-0800","result":{"code":"SUCCESS","type":"TABLE","msg":"country\tyear\ttemp_min\ttemp_max\twind_min\twind_max\nSW\t2005\t-2.5\t-2.5\t6.0\t6.0\nRS\t2005\t-13.0\t-13.0\t4.0\t4.0\nCA\t2005\t-30.0\t-7.8\t1.2\t9.3\nNL\t2005\t5.5\t7.5\t3.0\t13.0\nFR\t2005\t3.0\t3.0\t4.1\t4.1\nPL\t2005\t-5.7\t-5.7\t4.0\t4.0\nUK\t2005\t6.4\t7.9\t3.6\t12.9\nLU\t2005\t-3.8\t-3.8\t2.0\t2.0\nUS\t2005\t-22.0\t8.0\t0.0\t14.9\nAU\t2005\t-6.0\t1.4\t0.0\t1.0\nNO\t2005\t-0.5\t4.0\t7.0\t11.3\nGP\t2005\t23.9\t23.9\t5.1\t5.1\nBE\t2005\t0.0\t6.0\t1.0\t4.1\nEZ\t2005\t-6.1\t0.8\t6.0\t8.0\n","comment":"","msgTable":[[{"key":"year","value":"SW"},{"key":"year","value":"2005"},{"key":"year","value":"-2.5"},{"key":"year","value":"-2.5"},{"key":"year","value":"6.0"},{"key":"year","value":"6.0"}],[{"key":"temp_min","value":"RS"},{"key":"temp_min","value":"2005"},{"key":"temp_min","value":"-13.0"},{"key":"temp_min","value":"-13.0"},{"key":"temp_min","value":"4.0"},{"key":"temp_min","value":"4.0"}],[{"key":"temp_max","value":"CA"},{"key":"temp_max","value":"2005"},{"key":"temp_max","value":"-30.0"},{"key":"temp_max","value":"-7.8"},{"key":"temp_max","value":"1.2"},{"key":"temp_max","value":"9.3"}],[{"key":"wind_min","value":"NL"},{"key":"wind_min","value":"2005"},{"key":"wind_min","value":"5.5"},{"key":"wind_min","value":"7.5"},{"key":"wind_min","value":"3.0"},{"key":"wind_min","value":"13.0"}],[{"key":"wind_max","value":"FR"},{"key":"wind_max","value":"2005"},{"key":"wind_max","value":"3.0"},{"key":"wind_max","value":"3.0"},{"key":"wind_max","value":"4.1"},{"key":"wind_max","value":"4.1"}],[{"value":"PL"},{"value":"2005"},{"value":"-5.7"},{"value":"-5.7"},{"value":"4.0"},{"value":"4.0"}],[{"value":"UK"},{"value":"2005"},{"value":"6.4"},{"value":"7.9"},{"value":"3.6"},{"value":"12.9"}],[{"value":"LU"},{"value":"2005"},{"value":"-3.8"},{"value":"-3.8"},{"value":"2.0"},{"value":"2.0"}],[{"value":"US"},{"value":"2005"},{"value":"-22.0"},{"value":"8.0"},{"value":"0.0"},{"value":"14.9"}],[{"value":"AU"},{"value":"2005"},{"value":"-6.0"},{"value":"1.4"},{"value":"0.0"},{"value":"1.0"}],[{"value":"NO"},{"value":"2005"},{"value":"-0.5"},{"value":"4.0"},{"value":"7.0"},{"value":"11.3"}],[{"value":"GP"},{"value":"2005"},{"value":"23.9"},{"value":"23.9"},{"value":"5.1"},{"value":"5.1"}],[{"value":"BE"},{"value":"2005"},{"value":"0.0"},{"value":"6.0"},{"value":"1.0"},{"value":"4.1"}],[{"value":"EZ"},{"value":"2005"},{"value":"-6.1"},{"value":"0.8"},{"value":"6.0"},{"value":"8.0"}]],"columnNames":[{"name":"country","index":0,"aggr":"sum"},{"name":"year","index":1,"aggr":"sum"},{"name":"temp_min","index":2,"aggr":"sum"},{"name":"temp_max","index":3,"aggr":"sum"},{"name":"wind_min","index":4,"aggr":"sum"},{"name":"wind_max","index":5,"aggr":"sum"}],"rows":[["SW","2005","-2.5","-2.5","6.0","6.0"],["RS","2005","-13.0","-13.0","4.0","4.0"],["CA","2005","-30.0","-7.8","1.2","9.3"],["NL","2005","5.5","7.5","3.0","13.0"],["FR","2005","3.0","3.0","4.1","4.1"],["PL","2005","-5.7","-5.7","4.0","4.0"],["UK","2005","6.4","7.9","3.6","12.9"],["LU","2005","-3.8","-3.8","2.0","2.0"],["US","2005","-22.0","8.0","0.0","14.9"],["AU","2005","-6.0","1.4","0.0","1.0"],["NO","2005","-0.5","4.0","7.0","11.3"],["GP","2005","23.9","23.9","5.1","5.1"],["BE","2005","0.0","6.0","1.0","4.1"],["EZ","2005","-6.1","0.8","6.0","8.0"]]}},{"text":"ssc.stop(false)","dateUpdated":"2017-01-09T02:10:44-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483952418244_1594626","id":"20170109-010018_778177279","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-01-09T01:00:18-0800","dateStarted":"2017-01-09T02:10:44-0800","dateFinished":"2017-01-09T02:10:45-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9977","focus":true},{"text":"","dateUpdated":"2017-01-09T01:02:14-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1483952515244_-1080933892","id":"20170109-010155_1017767255","dateCreated":"2017-01-09T01:01:55-0800","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:9978"}],"name":"Spark Weather RDD Streaming Solution","id":"2C6CRZ8B5","angularObjects":{"2C7PVCCMW:shared_process":[],"2C7N8FNUK:shared_process":[],"2C4ZABBYG:shared_process":[],"2C7NHPHKH:shared_process":[],"2C4ACPMVH:shared_process":[],"2C5JAXWD8:shared_process":[],"2C68PFPBP:shared_process":[],"2C6BVC9ES:shared_process":[],"2C66FR28V:shared_process":[],"2C4Y5MV3V:shared_process":[],"2C6KQ2GNT:shared_process":[],"2C78K3BRW:shared_process":[],"2C81FGN7P:shared_process":[],"2C5BQGTGT:shared_process":[],"2C7VFSCFR:shared_process":[],"2C5MXJ5D6:shared_process":[],"2C44EB781:shared_process":[],"2C4JS2HDQ:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}