{"paragraphs":[{"text":"%md\n# Creating a DataFrame\n\nA DataFrame can be manually constructed from Scala objects in multiple ways. For example we can simply create a DataFrame from Scala tuples as follows.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Creating a DataFrame</h1>\n<p>A DataFrame can be manually constructed from Scala objects in multiple ways. For example we can simply create a DataFrame from Scala tuples as follows.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599385_-1431527095","id":"20160612-125948_2027140278","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:12523"},{"text":"val df = spark.createDataFrame(Array((\"Alice\", 13), (\"Bob\", 12)))\ndf.collect().foreach(println)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndf: org.apache.spark.sql.DataFrame = [_1: string, _2: int]\n[Alice,13]\n[Bob,12]\n"}]},"apps":[],"jobName":"paragraph_1529303599396_-1448071298","id":"20160612-130006_416625952","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12524"},{"text":"%md\nEach DataFrame has a *schema* - in this case the columns are again called `_1` and `_2`, implictly refering to the fields of a Scala tuple.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Each DataFrame has a <em>schema</em> - in this case the columns are again called <code>_1</code> and <code>_2</code>, implictly refering to the fields of a Scala tuple.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599399_-1447686549","id":"20180324-180515_686882172","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12525"},{"text":"df.printSchema()","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _1: string (nullable = true)\n |-- _2: integer (nullable = false)\n\n"}]},"apps":[],"jobName":"paragraph_1529303599401_-1449995043","id":"20160612-150733_816863585","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12526"},{"text":"%md\nAs an alternative you can also construct a DataFrame from case classes. Scala will automatically infer the schema including the column names via reflection.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>As an alternative you can also construct a DataFrame from case classes. Scala will automatically infer the schema including the column names via reflection.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599403_-1449225545","id":"20180324-180554_1243161938","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12527"},{"text":"case class Person(name:String, age:Int)\n\nval df = spark.createDataFrame(Array(Person(\"Alice\", 13), Person(\"Bob\", 12)))\n\ndf.collect().foreach(println)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndefined class Person\n\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\n[Alice,13]\n[Bob,12]\n"}]},"apps":[],"jobName":"paragraph_1529303599406_-1450379791","id":"20160612-150509_1437708850","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12528"},{"text":"df.printSchema()","dateUpdated":"2018-06-18T06:33:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- name: string (nullable = true)\n |-- age: integer (nullable = false)\n\n"}]},"apps":[],"jobName":"paragraph_1529303599409_-1440761069","id":"20160612-151740_420873295","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12529"},{"text":"%md\nThere is even a simpler alternative for creating a DataFrame from a sequence of Scala case classes using the `toDF` method.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>There is even a simpler alternative for creating a DataFrame from a sequence of Scala case classes using the <code>toDF</code> method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599410_-1439606822","id":"20180324-180652_998280390","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12530"},{"text":"val df = Seq(Person(\"Alice\", 13), Person(\"Bob\", 12)).toDF","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\n"}]},"apps":[],"jobName":"paragraph_1529303599412_-1441915316","id":"20160617-201720_142048293","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12531"},{"text":"%md\n## Create DataFrame from RDD\n\nIn some cases it is required to create a DataFrame from a Spark RDD. This is also possible with exactly the same semantics as creating a DataFrame from a Scala collection.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Create DataFrame from RDD</h2>\n<p>In some cases it is required to create a DataFrame from a Spark RDD. This is also possible with exactly the same semantics as creating a DataFrame from a Scala collection.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599413_-1442300065","id":"20160612-151435_1007185367","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12532"},{"text":"val rdd = sc.parallelize(Array(Person(\"Alice\", 13), Person(\"Bob\", 12)))\nval df = spark.createDataFrame(rdd)\n\ndf.collect().foreach(println)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[Person] = ParallelCollectionRDD[63] at parallelize at <console>:34\n\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\n[Alice,13]\n[Bob,12]\n"}]},"apps":[],"jobName":"paragraph_1529303599415_-1441530567","id":"20160612-150638_1523664879","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12533"},{"text":"val df = rdd.toDF()\n\ndf.collect().foreach(println)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\n[Alice,13]\n[Bob,12]\n"}]},"apps":[],"jobName":"paragraph_1529303599417_-1443839060","id":"20160612-152808_868719593","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12534"},{"text":"%md\n## Creating a DataFrame from Rows\nInternally a DataFrame is a Spark RDD with `Row` instances as elements. This fact can be used to manually construct a DataFrame from rows","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Creating a DataFrame from Rows</h2>\n<p>Internally a DataFrame is a Spark RDD with <code>Row</code> instances as elements. This fact can be used to manually construct a DataFrame from rows</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599418_-1442684813","id":"20180324-180145_163737251","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12535"},{"text":"import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\n\nval rdd = sc.parallelize(\n    Array(\n        Row(\"Alice\", 13), \n        Row(\"Bob\", 12)\n    ))\nval schema = StructType(\n    StructField(\"name\", StringType, true) ::\n    StructField(\"age\", IntegerType, true) ::\n    Nil\n)\nval df = spark.createDataFrame(rdd, schema)\n\ndf.collect().foreach(println)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[0] at parallelize at <console>:80\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,IntegerType,true))\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\n[Alice,13]\n[Bob,12]\n"}]},"apps":[],"jobName":"paragraph_1529303599420_-1444993307","id":"20160612-150748_776002189","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12536"},{"text":"df.printSchema()","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1529303599422_-1444223809","id":"20160612-151811_1726581061","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12537"},{"text":"%md\n## Accessing the RDD in the DataFrame\nInternally each DataFrame holds an RDD. This can also be accessed via the `rdd` field of a DataFrame.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Accessing the RDD in the DataFrame</h2>\n<p>Internally each DataFrame holds an RDD. This can also be accessed via the <code>rdd</code> field of a DataFrame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599425_-1459229016","id":"20160612-151322_1187613408","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12538"},{"text":"val rdd = df.rdd\n\nrdd.collect().foreach(println)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[76] at rdd at <console>:44\n[Alice,13]\n[Bob,12]\n"}]},"apps":[],"jobName":"paragraph_1529303599428_-1460383263","id":"20160612-151333_943343452","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12539"},{"text":"%md\n# Pretty Printing\n\nIn contrast to the RDD interface, DataFrames contain rich metadata and can therefore be interpreted nicely as tables. The `show` method of a DataFrame will print the table. Additionally Zeppelin also provides a `z.show(...)` function for even nicer representation.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Pretty Printing</h1>\n<p>In contrast to the RDD interface, DataFrames contain rich metadata and can therefore be interpreted nicely as tables. The <code>show</code> method of a DataFrame will print the table. Additionally Zeppelin also provides a <code>z.show(...)</code> function for even nicer representation.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599431_-1459998514","id":"20160611-085900_116012142","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12540"},{"text":"df.show()\n","dateUpdated":"2018-06-18T06:33:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+---+\n| name|age|\n+-----+---+\n|Alice| 13|\n|  Bob| 12|\n+-----+---+\n\n"}]},"apps":[],"jobName":"paragraph_1529303599434_-1461152761","id":"20180324-181037_2085149055","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12541"},{"text":"z.show(df)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":134,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"age","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"age","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\nAlice\t13\nBob\t12\n"}]},"apps":[],"jobName":"paragraph_1529303599438_-1462691756","id":"20160611-085918_1891664968","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12542"},{"text":"%md\n# Reading a DataFrame\nOf course normally you won't manually construct a DataFrame but you want to read from files in HDFS/S3/whatever instead. You can retrieve a `DataFrameReader` instance from the Spark session via `spark.read`. This reader supports multiple file formats (most notably JSON, CSV, Parquet and ORC). Additional libraries add support for more file formats like Avro.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Reading a DataFrame</h1>\n<p>Of course normally you won&rsquo;t manually construct a DataFrame but you want to read from files in HDFS/S3/whatever instead. You can retrieve a <code>DataFrameReader</code> instance from the Spark session via <code>spark.read</code>. This reader supports multiple file formats (most notably JSON, CSV, Parquet and ORC). Additional libraries add support for more file formats like Avro.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599440_-1452688285","id":"20170105-012624_2037335489","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12543"},{"text":"val persons = spark.read.json(\"s3://dimajix-training/data/persons.json\")\nz.show(persons)","dateUpdated":"2018-06-18T07:30:44+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":129,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\theight\tname\tsex\n23\t156\tAlice\tfemale\n21\t181\tBob\tmale\n27\t176\tCharlie\tmale\n24\t167\tEve\tfemale\n19\t172\tFrances\tfemale\n31\t191\tGeorge\tfemale\n"}]},"apps":[],"jobName":"paragraph_1529303599443_-1452303536","id":"20170105-012654_1765180674","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12544"},{"text":"%md\nThe JSON file looks as follows:\n\n    { \"name\":\"Alice\", \"age\":14, \"sex\":\"female\", \"height\":176 }\n    { \"name\":\"Bob\", \"age\":21, \"sex\":\"male\", \"height\":181 }\n    { \"name\":\"Charlie\", \"age\":27, \"sex\":\"male\", \"height\":176 }\n    { \"name\":\"Eve\", \"age\":24, \"sex\":\"female\", \"height\":167 }\n    { \"name\":\"Frances\", \"age\":19, \"sex\":\"female\", \"height\":172 }\n    { \"name\":\"George\", \"age\":31, \"sex\":\"male\", \"height\":191 }\n","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The JSON file looks as follows:</p>\n<pre><code>{ &quot;name&quot;:&quot;Alice&quot;, &quot;age&quot;:14, &quot;sex&quot;:&quot;female&quot;, &quot;height&quot;:176 }\n{ &quot;name&quot;:&quot;Bob&quot;, &quot;age&quot;:21, &quot;sex&quot;:&quot;male&quot;, &quot;height&quot;:181 }\n{ &quot;name&quot;:&quot;Charlie&quot;, &quot;age&quot;:27, &quot;sex&quot;:&quot;male&quot;, &quot;height&quot;:176 }\n{ &quot;name&quot;:&quot;Eve&quot;, &quot;age&quot;:24, &quot;sex&quot;:&quot;female&quot;, &quot;height&quot;:167 }\n{ &quot;name&quot;:&quot;Frances&quot;, &quot;age&quot;:19, &quot;sex&quot;:&quot;female&quot;, &quot;height&quot;:172 }\n{ &quot;name&quot;:&quot;George&quot;, &quot;age&quot;:31, &quot;sex&quot;:&quot;male&quot;, &quot;height&quot;:191 }\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599446_-1453457783","id":"20180324-181125_742118475","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12545"},{"text":"%md\n### Reading CSV files\nSpark also supports to read CSV files. It even supports headers in CSV files and can optionally try to guess the types of columns.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Reading CSV files</h3>\n<p>Spark also supports to read CSV files. It even supports headers in CSV files and can optionally try to guess the types of columns.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599449_-1456151025","id":"20180324-181343_701519241","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12546"},{"text":"val csv1 = spark.read\n    .option(\"header\",\"true\")\n    .option(\"inferSchema\",\"true\")\n    .csv(\"s3://dimajix-training/data/persons_header.csv\")\ncsv1.toZeppelin()","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\theight\tname\tsex\n23\t156\tAlice\tfemale\n21\t181\tBob\tmale\n27\t176\tCharlie\tmale\n24\t167\tEve\tfemale\n19\t172\tFrances\tfemale\n31\t191\tGeorge\tfemale\n"}]},"apps":[],"jobName":"paragraph_1529303599453_-1457690021","id":"20170105-012711_1659277547","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12547"},{"text":"csv1.printSchema()","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- age: integer (nullable = true)\n |-- height: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- sex: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1529303599457_-1471540981","id":"20170105-014033_512220053","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12548"},{"text":"%md\n## Specifying a Schema\nIn many cases you don't want to rely on automatic schema detection from Spark, because this might not be stable (on different files) or you might miss important changes in the schema in new files. You can (and should) explicitly specify a schema when working with CSV files for reproducible results.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Specifying a Schema</h2>\n<p>In many cases you don&rsquo;t want to rely on automatic schema detection from Spark, because this might not be stable (on different files) or you might miss important changes in the schema in new files. You can (and should) explicitly specify a schema when working with CSV files for reproducible results.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599459_-1470771483","id":"20180324-181440_1160826041","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12549"},{"text":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.IntegerType\n\nval schema = StructType(\n    StructField(\"age\", IntegerType) ::\n    StructField(\"height\", IntegerType) ::\n    StructField(\"name\", StringType) ::\n    StructField(\"sex\", StringType) ::\n    Nil\n    )\n    \nval csv3 = spark.read\n    .schema(schema)\n    .csv(\"s3://dimajix-training/data/persons_headerless.csv\")\n\nz.show(csv3)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"age","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"age","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\n23\t156\n21\t181\n27\t176\n24\t167\n19\t172\n31\t191\n"}]},"apps":[],"jobName":"paragraph_1529303599461_-1473079977","id":"20170105-013943_566941124","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12550"},{"text":"%md\n# Selecting Columns\n\nNow that we know how to create DataFrames via Scala sequences and RDDs or read data from files in HDFS/S3, we want to actually do something useful with them. The simplest thing is *selecting columns* (i.e. projections). Here we'll immediately see an important difference to RDDs: DataFrames are completely dynamically typed objects. The schema of a DataFrame is not known at compile time (even if it was manually created in code, it is still represented dynamically). This allows a more generic approach and does not requiore to write custom classes for different objects. Everything is a data frame with a dynamically injected schema. Columns are addressed via their names in strings - not as members of some Scala classes.\n\nNevertheless this is already a topic with multiple solutions. Let's first look at the simplest approach, which selects one column by its name:","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Selecting Columns</h1>\n<p>Now that we know how to create DataFrames via Scala sequences and RDDs or read data from files in HDFS/S3, we want to actually do something useful with them. The simplest thing is <em>selecting columns</em> (i.e. projections). Here we&rsquo;ll immediately see an important difference to RDDs: DataFrames are completely dynamically typed objects. The schema of a DataFrame is not known at compile time (even if it was manually created in code, it is still represented dynamically). This allows a more generic approach and does not requiore to write custom classes for different objects. Everything is a data frame with a dynamically injected schema. Columns are addressed via their names in strings - not as members of some Scala classes.</p>\n<p>Nevertheless this is already a topic with multiple solutions. Let&rsquo;s first look at the simplest approach, which selects one column by its name:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599462_-1471925730","id":"20160612-151621_251190447","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12551"},{"text":"val result = persons.select(\"name\")\n\nz.show(result)\n","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\nAlice\nBob\nCharlie\nEve\nFrances\nGeorge\n"}]},"apps":[],"jobName":"paragraph_1529303599463_-1472310479","id":"20160612-151633_1423873011","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12552"},{"text":"%md\nIn addition to the simple approach above, Spark also offers additional ways to address columns\n* Via `$\"column_name\"` - this works like an unqualified column in SQL\n* Via `'column_name` - this works like an unqualified column in SQL\n* Via the function `col(\"column_name\")` - this works like an unqualified column in SQL\n* Via using the dataframe itself as reference in `dataframe(\"column_name\")` - this works like a column qualified with its (temporary) table name in SQL\n \nThe last syntax helps to resolve ambigious cases, for example if some data frame is the result of a **JOIN** operation with overlapping column names.","user":"anonymous","dateUpdated":"2018-06-18T07:50:50+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In addition to the simple approach above, Spark also offers additional ways to address columns<br/>* Via <code>$&quot;column_name&quot;</code> - this works like an unqualified column in SQL<br/>* Via <code>&#39;column_name</code> - this works like an unqualified column in SQL<br/>* Via the function <code>col(&quot;column_name&quot;)</code> - this works like an unqualified column in SQL<br/>* Via using the dataframe itself as reference in <code>dataframe(&quot;column_name&quot;)</code> - this works like a column qualified with its (temporary) table name in SQL</p>\n<p>The last syntax helps to resolve ambigious cases, for example if some data frame is the result of a <strong>JOIN</strong> operation with overlapping column names.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599465_-1474618972","id":"20180326-155235_1512036684","dateCreated":"2018-06-18T06:33:19+0000","dateStarted":"2018-06-18T07:50:50+0000","dateFinished":"2018-06-18T07:50:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12553"},{"text":"val result = persons.select($\"age\", col(\"name\"), 'height, persons(\"sex\"))\n\nz.show(result)\n","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"age","index":0,"aggr":"sum"}],"values":[{"name":"name","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"age","index":0,"aggr":"sum"},"yAxis":{"name":"name","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\tname\theight\tsex\n23\tAlice\t156\tfemale\n21\tBob\t181\tmale\n27\tCharlie\t176\tmale\n24\tEve\t167\tfemale\n19\tFrances\t172\tfemale\n31\tGeorge\t191\tfemale\n"}]},"apps":[],"jobName":"paragraph_1529303599466_-1473464725","id":"20160612-152001_527472554","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12554"},{"text":"%md\n## Functions and Literals\nSpark also offers functions like we know from SQL. For example we have `length` for retrieving the length of a string column, `concat` for concatenating multiple columns to a single string and many more. Spark has all common string function, mathematical functions, aggregation functions (more on that later), and also functions to work with lists, sets and maps.\n\nYou can find almost all functions in the object `org.apache.spark.sql.functions`. Some additional functions are implemented as methods of a DataFrame `Column` in `org.apache.spark.sql.Column`-","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Functions and Literals</h2>\n<p>Spark also offers functions like we know from SQL. For example we have <code>length</code> for retrieving the length of a string column, <code>concat</code> for concatenating multiple columns to a single string and many more. Spark has all common string function, mathematical functions, aggregation functions (more on that later), and also functions to work with lists, sets and maps.</p>\n<p>You can find almost all functions in the object <code>org.apache.spark.sql.functions</code>. Some additional functions are implemented as methods of a DataFrame <code>Column</code> in <code>org.apache.spark.sql.Column</code>-</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599467_-1473849474","id":"20160612-152140_1844336454","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12555"},{"text":"val result = persons.select(concat(lit(\"Name:\"), col(\"name\"), lit(\" Age:\"), $\"age\") as \"description\")\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"description\nName:Alice Age:23\nName:Bob Age:21\nName:Charlie Age:27\nName:Eve Age:24\nName:Frances Age:19\nName:George Age:31\n"}]},"apps":[],"jobName":"paragraph_1529303599468_-1475773219","id":"20160612-152200_1663756389","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12556"},{"text":"val result = persons.select('name, upper('name).alias(\"upper_name\"))\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tupper_name\nAlice\tALICE\nBob\tBOB\nCharlie\tCHARLIE\nEve\tEVE\nFrances\tFRANCES\nGeorge\tGEORGE\n"}]},"apps":[],"jobName":"paragraph_1529303599469_-1476157968","id":"20160612-152358_839240513","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12557"},{"text":"val result = persons.select('name, ($\"age\" * 2).alias(\"double_age\"))\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tdouble_age\nAlice\t46\nBob\t42\nCharlie\t54\nEve\t48\nFrances\t38\nGeorge\t62\n"}]},"apps":[],"jobName":"paragraph_1529303599471_-1475388470","id":"20160612-152435_1333366716","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12558"},{"text":"%md\n### Case/When Functions\nSpark also provides the `CASE ... WHEN ... END` construct known from SQL, where you can implemenet if/then conditions for individual rows.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Case/When Functions</h3>\n<p>Spark also provides the <code>CASE ... WHEN ... END</code> construct known from SQL, where you can implemenet if/then conditions for individual rows.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599472_-1465000250","id":"20180326-160033_789881465","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12559"},{"text":"val result = persons.select(\n        $\"name\",\n        $\"age\",\n        when($\"age\" % 2 === 0, \"even_age\").otherwise(\"odd_age\").alias(\"text\")\n    )\n    \nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\ttext\nAlice\t23\todd_age\nBob\t21\todd_age\nCharlie\t27\todd_age\nEve\t24\teven_age\nFrances\t19\todd_age\nGeorge\t31\todd_age\n"}]},"apps":[],"jobName":"paragraph_1529303599473_-1465384999","id":"20160612-152524_1040140298","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12560"},{"text":"%md\n## Adding Columns\n\nSpark not only allows you to transform DataFrames via `select`, it also provides a very convenient method for adding columns to a DataFrames. This frees you from copying all columns in a `select` statement when you only want to create new columns.\n\nIf you specify the name of an existing column as the target column, the existing column will be replaced with the new expression.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Adding Columns</h2>\n<p>Spark not only allows you to transform DataFrames via <code>select</code>, it also provides a very convenient method for adding columns to a DataFrames. This frees you from copying all columns in a <code>select</code> statement when you only want to create new columns.</p>\n<p>If you specify the name of an existing column as the target column, the existing column will be replaced with the new expression.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599475_-1464615501","id":"20160617-162557_838805065","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12561"},{"text":"val result = persons.withColumn(\"text\", when($\"age\" % 2 === 0, \"even_age\").otherwise(\"odd_age\"))\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\theight\tname\tsex\ttext\n23\t156\tAlice\tfemale\todd_age\n21\t181\tBob\tmale\todd_age\n27\t176\tCharlie\tmale\todd_age\n24\t167\tEve\tfemale\teven_age\n19\t172\tFrances\tfemale\todd_age\n31\t191\tGeorge\tfemale\todd_age\n"}]},"apps":[],"jobName":"paragraph_1529303599476_-1466539245","id":"20160617-162610_764186088","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12562"},{"text":"%md\nTechnically you could implement a generic `withColumn` by using some Scala code for collecting all incoming columns.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Technically you could implement a generic <code>withColumn</code> by using some Scala code for collecting all incoming columns.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599476_-1466539245","id":"20180326-160327_1340912163","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12563"},{"text":"val cols = persons.schema.fields.map(field => col(field.name)) :+ when(col(\"sex\") === lit(\"male\"), lit(\"Mr.\")).otherwise(\"Mrs.\").alias(\"title\")\nval df = persons.select(cols:_*)\nz.show(df)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1529303599479_-1466154496","id":"20180326-160242_1868749324","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12564"},{"text":"%md \nA more convenient way would be to use the special expression `col(\"*\")`","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>A more convenient way would be to use the special expression <code>col(&quot;*&quot;)</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599482_-1467308743","id":"20180326-160407_273315784","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12565"},{"text":"val result = persons.select(col(\"*\"), when($\"age\" % 2 === 0, \"even_age\").otherwise(\"odd_age\").as(\"text\"))\n\nz.show(result)\n","dateUpdated":"2018-06-18T06:33:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1529303599483_-1467693492","id":"20180326-160438_1633012540","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12566"},{"text":"%md\n## Dropping Columns\nSometimes you just want to perform the opposite operation: Get rid of some columns. This can be useful for cleaning up temporary results before writing a result to disk.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Dropping Columns</h2>\n<p>Sometimes you just want to perform the opposite operation: Get rid of some columns. This can be useful for cleaning up temporary results before writing a result to disk.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599485_-1470001985","id":"20160617-162733_380188317","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12567"},{"text":"val result2 = result.drop(\"age\")\nz.show(result2)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"height\tname\tsex\ttext\n156\tAlice\tfemale\todd_age\n181\tBob\tmale\todd_age\n176\tCharlie\tmale\todd_age\n167\tEve\tfemale\teven_age\n172\tFrances\tfemale\todd_age\n191\tGeorge\tfemale\todd_age\n"}]},"apps":[],"jobName":"paragraph_1529303599486_-1468847739","id":"20160617-162750_466208521","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12568"},{"text":"%md\n## Filtering\n\nNow we have seen how we can perform 1:1 transformations, i.e. every row of some incoming DataFrame is transformed and produces exactly one outgoing row. Often we also require to remove some rows, i.e. we would like to perform a SQL `WHERE` clause. This is also supported with some easy syntax in Spark. The simplest way is to construct a boolean column expression and then pass it as a predicate into the `filter` or `where` method of a DataFrame. Both methods are completely equivalent, the `filter` method is for Scala developers and the `where` method for all of you with an SQL background.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Filtering</h2>\n<p>Now we have seen how we can perform 1:1 transformations, i.e. every row of some incoming DataFrame is transformed and produces exactly one outgoing row. Often we also require to remove some rows, i.e. we would like to perform a SQL <code>WHERE</code> clause. This is also supported with some easy syntax in Spark. The simplest way is to construct a boolean column expression and then pass it as a predicate into the <code>filter</code> or <code>where</code> method of a DataFrame. Both methods are completely equivalent, the <code>filter</code> method is for Scala developers and the <code>where</code> method for all of you with an SQL background.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599487_-1469232488","id":"20160612-153031_659656525","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12569"},{"text":"val result = persons.filter($\"age\" > 22)\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"age","index":0,"aggr":"sum"}],"values":[{"name":"height","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"age","index":0,"aggr":"sum"},"yAxis":{"name":"height","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\theight\tname\tsex\n23\t156\tAlice\tfemale\n27\t176\tCharlie\tmale\n24\t167\tEve\tfemale\n31\t191\tGeorge\tfemale\n"}]},"apps":[],"jobName":"paragraph_1529303599488_-1384972478","id":"20160612-153548_1115248994","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12570"},{"text":"%md\nAlternatively you can also specify a condition completely as an SQL expression, which then is parsed by Spark and translated into a column expression like above.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Alternatively you can also specify a condition completely as an SQL expression, which then is parsed by Spark and translated into a column expression like above.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599489_-1385357227","id":"20180326-160943_1911127373","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12571"},{"text":"val result = persons.where(\"age > 22\")\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\theight\tname\tsex\n23\t156\tAlice\tfemale\n27\t176\tCharlie\tmale\n24\t167\tEve\tfemale\n31\t191\tGeorge\tfemale\n"}]},"apps":[],"jobName":"paragraph_1529303599491_-1384587730","id":"20160612-153209_1553421093","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12572"},{"text":"%md\n# Aggregations\n\nSpark also supports many aggregations like `min`, `max`, `sum`, `avg` and so on. If used directly inside a `select`, then the aggregation will run on the whole DataFrame and produce a single row as the result. You can find all aggregation functions again in the object `org.apache.spark.sql.functions`.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Aggregations</h1>\n<p>Spark also supports many aggregations like <code>min</code>, <code>max</code>, <code>sum</code>, <code>avg</code> and so on. If used directly inside a <code>select</code>, then the aggregation will run on the whole DataFrame and produce a single row as the result. You can find all aggregation functions again in the object <code>org.apache.spark.sql.functions</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599492_-1386511474","id":"20160612-153253_1859986543","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12573"},{"text":"val df = sc.parallelize(1 to 100).toDF\nval result = df.select(\n    sum('value).alias(\"sum\"), \n    avg('value).alias(\"avg\"), \n    min('value).alias(\"min\"), \n    max('value).alias(\"max\"), \n    count('value).alias(\"count\")\n)\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":89,"optionOpen":false,"keys":[{"name":"sum","index":0,"aggr":"sum"}],"values":[{"name":"avg","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"sum","index":0,"aggr":"sum"},"yAxis":{"name":"avg","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"sum\tavg\tmin\tmax\tcount\n5050\t50.5\t1\t100\t100\n"}]},"apps":[],"jobName":"paragraph_1529303599494_-1385741976","id":"20160612-153747_1650442270","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12574"},{"text":"%md\nSome very simple aggregation functions are also provided directly without the need for a `select`, most notably `count`.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Some very simple aggregation functions are also provided directly without the need for a <code>select</code>, most notably <code>count</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599495_-1386126725","id":"20180326-161239_1481493783","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12575"},{"text":"df.count()","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres164: Long = 100\n"}]},"apps":[],"jobName":"paragraph_1529303599496_-1388050470","id":"20160612-154004_869347895","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12576"},{"text":"%md\n## Making Data Distinct\n\nSpark also offers a special function for removing duplicate rows. This would be equivalent to an SQL `SELECT DISTINCT *`","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Making Data Distinct</h2>\n<p>Spark also offers a special function for removing duplicate rows. This would be equivalent to an SQL <code>SELECT DISTINCT *</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599498_-1387280972","id":"20160612-154030_610244901","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12577"},{"text":"val df = sc.parallelize(Array(\"Alice\",\"Bob\",\"Alice\")).toDF\nval result = df.distinct()\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"value\nBob\nAlice\n"}]},"apps":[],"jobName":"paragraph_1529303599499_-1387665721","id":"20160612-154035_439876147","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12578"},{"text":"%md\n## Grouping and Aggregating\nOf course in most cases it is required to use grouped aggregations. Spark offers a `groupBy` method for creating groups. In contrast to most other methods, this does not return a new DataFrame. Instead a special *grouped data object* is returned which requires an aggregation to be performed as the next step. Some simple numeric aggregations like `min`, `max`, `sum` and `avg` are available directly as part of the grouped object like in the following example.\n\nNote that the `groupBy` column is automatically added by Spark to the resulting DataFrame.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Grouping and Aggregating</h2>\n<p>Of course in most cases it is required to use grouped aggregations. Spark offers a <code>groupBy</code> method for creating groups. In contrast to most other methods, this does not return a new DataFrame. Instead a special <em>grouped data object</em> is returned which requires an aggregation to be performed as the next step. Some simple numeric aggregations like <code>min</code>, <code>max</code>, <code>sum</code> and <code>avg</code> are available directly as part of the grouped object like in the following example.</p>\n<p>Note that the <code>groupBy</code> column is automatically added by Spark to the resulting DataFrame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599500_-1389589465","id":"20160612-154202_1946092920","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12579"},{"text":"val result = persons.select('sex,'age).groupBy('sex).avg()\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[{"name":"avg(age)","index":1,"aggr":"sum"}],"groups":[],"scatter":{"yAxis":{"name":"avg(age)","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"sex\tavg(age)\nfemale\t24.25\nmale\t24.0\n"}]},"apps":[],"jobName":"paragraph_1529303599501_-1389974214","id":"20160612-154557_888346984","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12580"},{"text":"%md\nIn addition to use simple aggregations, you can also use the generic `agg` method which essentially works like a `select` with the only difference that it only supports aggregations. Again the `groupBy` columns are added to the result implicitly by Spark.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In addition to use simple aggregations, you can also use the generic <code>agg</code> method which essentially works like a <code>select</code> with the only difference that it only supports aggregations. Again the <code>groupBy</code> columns are added to the result implicitly by Spark.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599503_-1389204716","id":"20180326-161627_1523001075","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12581"},{"text":"val result = persons.groupBy('sex)\n    .agg(\n        avg(col(\"age\")) as \"avg_age\",\n        min(col(\"height\")) as \"min_height\",\n        max(col(\"height\")) as \"max_height\"\n    )\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"sex\tavg_age\tmin_height\tmax_height\nfemale\t24.25\t156\t191\nmale\t24.0\t176\t181\n"}]},"apps":[],"jobName":"paragraph_1529303599505_-1379201245","id":"20160612-154724_579125114","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12582"},{"text":"%md\n## Sorting Data\n\nFinally results often need to be sorted (by importance, frequence or whatever). This can be done by the `orderBy` method of a DataFrame, which takes one or multiple columns for defining the order.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Sorting Data</h2>\n<p>Finally results often need to be sorted (by importance, frequence or whatever). This can be done by the <code>orderBy</code> method of a DataFrame, which takes one or multiple columns for defining the order.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599506_-1378046998","id":"20160612-154903_648261643","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12583"},{"text":"val result = persons.orderBy(col(\"name\"))\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\theight\tname\tsex\n23\t156\tAlice\tfemale\n21\t181\tBob\tmale\n27\t176\tCharlie\tmale\n24\t167\tEve\tfemale\n19\t172\tFrances\tfemale\n31\t191\tGeorge\tfemale\n"}]},"apps":[],"jobName":"paragraph_1529303599507_-1378431747","id":"20160612-154939_820007544","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12584"},{"text":"%md\nYou can sort by multiple columns and you can also specify to sort in ascending or descending order:","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>You can sort by multiple columns and you can also specify to sort in ascending or descending order:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599508_-1380355492","id":"20180326-161941_169508617","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12585"},{"text":"val result = persons.orderBy($\"sex\".desc, $\"age\".asc)\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\theight\tname\tsex\n21\t181\tBob\tmale\n27\t176\tCharlie\tmale\n19\t172\tFrances\tfemale\n23\t156\tAlice\tfemale\n24\t167\tEve\tfemale\n31\t191\tGeorge\tfemale\n"}]},"apps":[],"jobName":"paragraph_1529303599509_-1380740241","id":"20160612-155236_301979744","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12586"},{"text":"%md\n## Joining Data\n\nA very important topic is joining data, i.e. merging two DataFrames on some common key. In order to demonstrate Sparks joining capabilities, we load some additional address data, which partially matches the persons data.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Joining Data</h2>\n<p>A very important topic is joining data, i.e. merging two DataFrames on some common key. In order to demonstrate Sparks joining capabilities, we load some additional address data, which partially matches the persons data.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599510_-1379585994","id":"20160612-155307_1442775029","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12587"},{"text":"val addresses = spark.read.json(\"s3://dimajix-training/data/addresses.json\")\nz.show(addresses)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"city\tname\nHamburg\tAlice\nFrankfurt\tBob\nBerlin\tHenry\n"}]},"apps":[],"jobName":"paragraph_1529303599511_-1379970743","id":"20170105-020841_2109038029","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12588"},{"text":"%md\nNow we can merge both DataFrames on the column *name*. The join condition is specified explicitly. Note that the result now has the join columns twice - one time from the `persons` DataFrame, the other time from the `addresses` DataFrame.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now we can merge both DataFrames on the column <em>name</em>. The join condition is specified explicitly. Note that the result now has the join columns twice - one time from the <code>persons</code> DataFrame, the other time from the <code>addresses</code> DataFrame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599513_-1382279236","id":"20180326-162127_1642011647","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12589"},{"text":"val result = persons.join(addresses, persons(\"name\") === addresses(\"name\"))\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\theight\tname\tsex\tcity\tname\n23\t156\tAlice\tfemale\tHamburg\tAlice\n21\t181\tBob\tmale\tFrankfurt\tBob\n"}]},"apps":[],"jobName":"paragraph_1529303599514_-1381124989","id":"20160612-155332_1943504111","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12590"},{"text":"val df = persons.join(addresses, persons(\"name\") === addresses(\"name\"), \"outer\")\n    .withColumn(\"mergedName\", coalesce(persons(\"name\"), addresses(\"name\")))\nz.show(df)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1529303599515_-1381509738","id":"20180326-164834_776683218","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12591"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530691985617_-421596241","id":"20180704-081305_488498162","dateCreated":"2018-07-04T08:13:05+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:20212","text":"val result = persons.join(addresses, Seq(\"name\"), \"fullOuter\")\n\nz.show(result)","dateUpdated":"2018-07-04T08:13:15+0000"},{"text":"%md\n# Interacting with SQL\n\nSpark DataFrame supports all operations equivalent to SQL. You can even explicitly write some SQL code that accesses one or multiple DataFrames. But first you have to register the DataFrame as a temporary named view to make it accessible from SQL. After that you can use the `spark.sql` method to perform SQL transformations.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Interacting with SQL</h1>\n<p>Spark DataFrame supports all operations equivalent to SQL. You can even explicitly write some SQL code that accesses one or multiple DataFrames. But first you have to register the DataFrame as a temporary named view to make it accessible from SQL. After that you can use the <code>spark.sql</code> method to perform SQL transformations.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599516_-1383433483","id":"20160617-161812_1127220850","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12592"},{"text":"persons.createOrReplaceTempView(\"persons\")\n\nval result = spark.sql(\"SELECT * FROM persons\")\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"age\theight\tname\tsex\n23\t156\tAlice\tfemale\n21\t181\tBob\tmale\n27\t176\tCharlie\tmale\n24\t167\tEve\tfemale\n19\t172\tFrances\tfemale\n31\t191\tGeorge\tfemale\n"}]},"apps":[],"jobName":"paragraph_1529303599517_-1383818232","id":"20160617-161843_1855518036","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12593"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":false},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1530692393060_552525556","id":"20180704-081953_1986852769","dateCreated":"2018-07-04T08:19:53+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:20271","text":"%sql\nSELECt \n    sex,\n    MIN(age) AS min_age,\n    MAX(age) AS max_age\nFROM persons\nWHERE height > 160\nGROUP BY sex\nHAVING min_age > 20"},{"text":"%md\n# User Defined Functions\n\nOne very powerful capability of Spark are user defined functions (UDFs). This allows you to create new SQL functions in Scala and use them either with programmatic SQL or directly within SQL. Let us create an example using a special function that converts a string to a lower/upper case mixture. This is not available in SQL or Spark and would be very hard to implement with plain SQL functions. We register this function and make it available in SQL vi athe name \"funny\" and we also store the UDF in a Scala value to use it in programmatic SQL.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>User Defined Functions</h1>\n<p>One very powerful capability of Spark are user defined functions (UDFs). This allows you to create new SQL functions in Scala and use them either with programmatic SQL or directly within SQL. Let us create an example using a special function that converts a string to a lower/upper case mixture. This is not available in SQL or Spark and would be very hard to implement with plain SQL functions. We register this function and make it available in SQL vi athe name &ldquo;funny&rdquo; and we also store the UDF in a Scala value to use it in programmatic SQL.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599517_-1383818232","id":"20160612-155902_920718644","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12594"},{"text":"val funny = spark.udf.register(\"funny\", {s:String => s.zipWithIndex.map(kv => if (kv._2 % 2 == 1) kv._1.toLower else kv._1.toUpper).mkString(\"\")})\n\nval result = persons.select(funny('name) as \"funny_name\")\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"funny_name\nAlIcE\nBoB\nChArLiE\nEvE\nFrAnCeS\nGeOrGe\n"}]},"apps":[],"jobName":"paragraph_1529303599518_-1382663985","id":"20160612-155931_1503875483","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12595"},{"text":"val result = spark.sql(\"select funny(name) as funny_name from persons\")\n\nz.show(result)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"funny_name\nAlIcE\nBoB\nChArLiE\nEvE\nFrAnCeS\nGeOrGe\n"}]},"apps":[],"jobName":"paragraph_1529303599519_-1383048734","id":"20160617-161940_1666377443","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12596"},{"text":"%md\n# Caching Data\n\nSpark DataFrames are evaluated lazily, and this is possible because every DataFrame contains the logical execution plan to create the result. This allows a magnitude of optimizations, but on the other hand this also means that even in complex and long chains of transformations with multiple joins and aggregations, the whole execution plan has to be executed over and over again whenever some downstream transformation needs to be calculated.\n\nIn order to avoid recalculation of some intermediate result over and over again, Spark provides some mechanism to cache data. The easiest way is to call the `cache` method of a DataFrame which tries to store the DataFrames rows in memory. This doesn't happen immediately, but as soon as the DataFrame is calculated. This can be enfored with a `count` for exmaple.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Caching Data</h1>\n<p>Spark DataFrames are evaluated lazily, and this is possible because every DataFrame contains the logical execution plan to create the result. This allows a magnitude of optimizations, but on the other hand this also means that even in complex and long chains of transformations with multiple joins and aggregations, the whole execution plan has to be executed over and over again whenever some downstream transformation needs to be calculated.</p>\n<p>In order to avoid recalculation of some intermediate result over and over again, Spark provides some mechanism to cache data. The easiest way is to call the <code>cache</code> method of a DataFrame which tries to store the DataFrames rows in memory. This doesn&rsquo;t happen immediately, but as soon as the DataFrame is calculated. This can be enfored with a <code>count</code> for exmaple.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599520_-1397284443","id":"20160617-162320_1495447774","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12597"},{"text":"persons.cache()\npersons.count()","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres189: persons.type = [age: bigint, height: bigint ... 2 more fields]\n\nres190: Long = 6\n"}]},"apps":[],"jobName":"paragraph_1529303599521_-1397669192","id":"20160617-162336_1875131146","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12598"},{"text":"%md\nIn order to free up some memory again, the DataFrame can also be uncached. This is done with the `unpersist` method.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In order to free up some memory again, the DataFrame can also be uncached. This is done with the <code>unpersist</code> method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599522_-1396514945","id":"20180326-164307_1664183516","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12599"},{"text":"persons.unpersist()","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres192: persons.type = [age: bigint, height: bigint ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1529303599523_-1396899694","id":"20160617-162409_876463061","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12600"},{"text":"%md\nIn addition to the simple `cache` method, Spark also provides a `persist` method, which lets you define precisely how the data should be cached: In memory, on disk or with a mixture of both storages.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In addition to the simple <code>cache</code> method, Spark also provides a <code>persist</code> method, which lets you define precisely how the data should be cached: In memory, on disk or with a mixture of both storages.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599524_-1398823439","id":"20180326-164336_1429926308","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12601"},{"text":"import org.apache.spark.storage.StorageLevel\n\npersons.persist(StorageLevel.MEMORY_AND_DISK)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1529303599524_-1398823439","id":"20180326-164426_380163537","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12602"},{"text":"%md\n# Writing and Reading Data\n\nFinally let us have a look at input/output operations again. We already saw how to read in data from S3/HDFS using DataFrameReaders returned by `spark.read`:","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Writing and Reading Data</h1>\n<p>Finally let us have a look at input/output operations again. We already saw how to read in data from S3/HDFS using DataFrameReaders returned by <code>spark.read</code>:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599525_-1399208188","id":"20160612-160043_427568179","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12603"},{"text":"val df = spark.read.text(\"s3://dimajix-training/data/alice\")\n\nz.show(df)","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":194,"optionOpen":false,"keys":[{"name":"value","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"value","index":0,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"value\n﻿Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\n\nThis eBook is for the use of anyone anywhere at no cost and with\n"}]},"apps":[],"jobName":"paragraph_1529303599526_-1398053941","id":"20160612-160702_744208944","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12604"},{"text":"%md\nWith a similar syntax, we can also write a DataFrame to HDFS (and also S3 given appropriate permissions) again.","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>With a similar syntax, we can also write a DataFrame to HDFS (and also S3 given appropriate permissions) again.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599527_-1398438690","id":"20180326-164547_119048270","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12605"},{"text":"val result = persons.select(concat('name, lit(\",\"), 'age))\n\nresult.write.mode(\"overwrite\").text(\"/tmp/persons\")","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nresult: org.apache.spark.sql.DataFrame = [concat(name, ,, age): string]\n"}]},"apps":[],"jobName":"paragraph_1529303599528_-1400362434","id":"20160612-160808_277383298","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12606"},{"text":"%sh\nhdfs dfs -getmerge /tmp/persons /tmp/persons\ncat /tmp/persons","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/sh","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Alice,23\nBob,21\nCharlie,27\nEve,24\nFrances,19\nGeorge,31\n"}]},"apps":[],"jobName":"paragraph_1529303599530_-1399592937","id":"20160612-160938_1466157130","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12607"},{"text":"%md\nYou can also chose to write using a different format, for example **csv**, **parquet** or **orc**:","dateUpdated":"2018-06-18T06:33:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>You can also chose to write using a different format, for example <strong>csv</strong>, <strong>parquet</strong> or <strong>orc</strong>:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1529303599531_-1399977686","id":"20180326-164630_18953108","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12608"},{"text":"result.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/persons_csv\")\n","dateUpdated":"2018-06-18T06:33:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1529303599532_-1401901430","id":"20180326-164712_1812551011","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12609"},{"text":"result.write.mode(\"overwrite\").csv(\"/tmp/persons_csv\")","dateUpdated":"2018-06-18T06:33:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1529303599533_-1402286179","id":"20180326-164739_1475137011","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12610"},{"text":"%md\n# WordCount Revisited","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>WordCount Revisited</h1>\n"}]},"apps":[],"jobName":"paragraph_1529303599534_-1401131932","id":"20160612-161131_370663008","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12611"},{"text":"val text = spark.read.text(\"s3://dimajix-training/data/alice\")\nval words = text.select(explode(split('value,\" \")).alias(\"word\")).filter('word !== lit(\"\"))\nval counts = words.groupBy($\"word\").count().sort($\"count\".desc)\n\nz.show(counts.limit(20))","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"word\tcount\nthe\t1664\nand\t780\nto\t773\na\t662\nof\t596\nshe\t484\nsaid\t416\nin\t401\nit\t356\nwas\t329\nyou\t301\nI\t260\nas\t246\nthat\t226\nAlice\t221\nwith\t214\nat\t211\nher\t203\nhad\t175\nall\t168\n"}]},"apps":[],"jobName":"paragraph_1529303599534_-1401131932","id":"20160612-161147_839240215","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12612"},{"text":"","dateUpdated":"2018-06-18T06:33:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1529303599535_-1401516681","id":"20160617-162253_1296143013","dateCreated":"2018-06-18T06:33:19+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:12613"}],"name":"Spark DataFrame Snippets","id":"2DGKRTFTX","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}