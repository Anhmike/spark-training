{"paragraphs":[{"text":"%md\n# 1 Creating a DataFrame\n\nA DataFrame can be manually constructed from Scala objects in multiple ways. For example we can simply create a DataFrame from Scala tuples as follows. While this is probably not the most common thing to do, it is easy and helpful in some situations where you already have some data in a collection.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>1 Creating a DataFrame</h1>\n<p>A DataFrame can be manually constructed from Scala objects in multiple ways. For example we can simply create a DataFrame from Scala tuples as follows. While this is probably not the most common thing to do, it is easy and helpful in some situations where you already have some data in a collection.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508768_-661877275","id":"20160612-125948_2027140278","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:16352"},{"text":"val df = spark.createDataFrame(Array((\"Alice\", 13), (\"Bob\", 12)))\nval rows = df.collect()\nrows.foreach(println)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndf: org.apache.spark.sql.DataFrame = [_1: string, _2: int]\n[Alice,13]\n[Bob,12]\n\nrows: Unit = ()\n"}]},"apps":[],"jobName":"paragraph_1542726508773_-663801019","id":"20160612-130006_416625952","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16353"},{"text":"%md\nEach DataFrame has a *schema* - in this case the columns are again called `_1` and `_2`, implictly refering to the fields of a Scala tuple.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Each DataFrame has a <em>schema</em> - in this case the columns are again called <code>_1</code> and <code>_2</code>, implictly refering to the fields of a Scala tuple.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508779_-664570517","id":"20180324-180515_686882172","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16354"},{"text":"%md\n## 1.1 Creating a DataFrame from case classes\nAs an alternative you can also construct a DataFrame from case classes. Scala will automatically infer the schema including the column names via reflection.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.1 Creating a DataFrame from case classes</h2>\n<p>As an alternative you can also construct a DataFrame from case classes. Scala will automatically infer the schema including the column names via reflection.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508783_-666109512","id":"20180324-180554_1243161938","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16355"},{"text":"case class Person(name:String, age:Int)\n\nval df = spark.createDataFrame(Array(Person(\"Alice\", 13), Person(\"Bob\", 12)))\n\ndf.collect().foreach(println)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndefined class Person\n\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\n[Alice,13]\n[Bob,12]\n"}]},"apps":[],"jobName":"paragraph_1542726508786_-654951794","id":"20160612-150509_1437708850","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16356"},{"text":"%md\nNote that the resulting DataFrame now has two columns, each named by the original field names","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Note that the resulting DataFrame now has two columns, each named by the original field names</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508790_-656490790","id":"20181110-121628_1415806641","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16357"},{"text":"%md\n## 1.2 Displaying Records\n\nIn addition to the `collect()` method, Spark also supports a `show()` method, which uses some ASCII table for dispalying the\nresult","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.2 Displaying Records</h2>\n<p>In addition to the <code>collect()</code> method, Spark also supports a <code>show()</code> method, which uses some ASCII table for dispalying the<br/>result</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508794_-658029786","id":"20181110-121905_398652057","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16358"},{"text":"df.show()","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+---+\n| name|age|\n+-----+---+\n|Alice| 13|\n|  Bob| 12|\n+-----+---+\n\n"}]},"apps":[],"jobName":"paragraph_1542726508799_-659953530","id":"20181110-122010_965977602","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16359"},{"text":"%md\nIn contrast to the RDD interface, DataFrames contain rich metadata and can therefore be interpreted nicely as tables. The show method of a DataFrame will print the table. Additionally Zeppelin also provides a z.show(...) function for even nicer representation.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In contrast to the RDD interface, DataFrames contain rich metadata and can therefore be interpreted nicely as tables. The show method of a DataFrame will print the table. Additionally Zeppelin also provides a z.show(&hellip;) function for even nicer representation.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508804_-577232517","id":"20181110-122550_1846914297","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16360"},{"text":"z.show(df)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\nAlice\t13\nBob\t12\n"}]},"apps":[],"jobName":"paragraph_1542726508809_-579156261","id":"20181110-122607_810994216","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16361"},{"text":"%md\n## 1.3 Create DataFrame from RDD\n\nIn some cases it is required to create a DataFrame from a Spark RDD. This is also possible with exactly the same semantics as creating a DataFrame from a Scala collection.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.3 Create DataFrame from RDD</h2>\n<p>In some cases it is required to create a DataFrame from a Spark RDD. This is also possible with exactly the same semantics as creating a DataFrame from a Scala collection.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508813_-580695257","id":"20160612-151435_1007185367","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16362"},{"text":"val rdd = sc.parallelize(Array(Person(\"Alice\", 13), Person(\"Bob\", 12)))\nval df = spark.createDataFrame(rdd)\n\ndf.show()","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[Person] = ParallelCollectionRDD[9] at parallelize at <console>:29\n\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\n+-----+---+\n| name|age|\n+-----+---+\n|Alice| 13|\n|  Bob| 12|\n+-----+---+\n\n"}]},"apps":[],"jobName":"paragraph_1542726508816_-569537539","id":"20160612-150638_1523664879","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16363"},{"text":"%md\nYou can also convert an existing RDD to a DataFrame via the `toDF` method.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>You can also convert an existing RDD to a DataFrame via the <code>toDF</code> method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508821_-571461283","id":"20181110-121541_1877555195","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16364"},{"text":"val df = rdd.toDF()\n\ndf.show()","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\n+-----+---+\n| name|age|\n+-----+---+\n|Alice| 13|\n|  Bob| 12|\n+-----+---+\n\n"}]},"apps":[],"jobName":"paragraph_1542726508825_-573000279","id":"20160612-152808_868719593","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16365"},{"text":"%md\nThere is even a simpler alternative for creating a DataFrame from a sequence of Scala case classes using the `toDF` method.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>There is even a simpler alternative for creating a DataFrame from a sequence of Scala case classes using the <code>toDF</code> method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508829_-574539274","id":"20180324-180652_998280390","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16366"},{"text":"val df = Seq(Person(\"Alice\", 13), Person(\"Bob\", 12)).toDF\n\ndf.show()","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\n+-----+---+\n| name|age|\n+-----+---+\n|Alice| 13|\n|  Bob| 12|\n+-----+---+\n\n"}]},"apps":[],"jobName":"paragraph_1542726508833_-588390235","id":"20160617-201720_142048293","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16367"},{"text":"%md\n## 1.4 Inspect Schema\n\nThe spark object has different methods for creating a so called Spark DataFrame object. This object is similar to a table, it contains rows of records, which all conform to a common schema with named columns and specific types. On the surface it heavily borrows concepts from Pandas DataFrames or R DataFrames, although the syntax and many operations are syntactically very different.\n\nAs the first step, we want to see the contents of the DataFrame. This can be easily done by using the show method.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.4 Inspect Schema</h2>\n<p>The spark object has different methods for creating a so called Spark DataFrame object. This object is similar to a table, it contains rows of records, which all conform to a common schema with named columns and specific types. On the surface it heavily borrows concepts from Pandas DataFrames or R DataFrames, although the syntax and many operations are syntactically very different.</p>\n<p>As the first step, we want to see the contents of the DataFrame. This can be easily done by using the show method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508837_-589929230","id":"20181110-121709_2079739831","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16368"},{"text":"df.printSchema()","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- name: string (nullable = true)\n |-- age: integer (nullable = false)\n\n"}]},"apps":[],"jobName":"paragraph_1542726508845_-593007221","id":"20160612-151811_1726581061","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16369"},{"text":"%md\n## 1.5 Creating a DataFrame from Rows\nInternally a DataFrame is a Spark RDD with `Row` instances as elements. This fact can be used to manually construct a DataFrame from rows and an explicitly specified schema.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.5 Creating a DataFrame from Rows</h2>\n<p>Internally a DataFrame is a Spark RDD with <code>Row</code> instances as elements. This fact can be used to manually construct a DataFrame from rows and an explicitly specified schema.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508848_-581849503","id":"20180324-180145_163737251","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16370"},{"text":"import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\n\nval rdd = sc.parallelize(\n    Array(\n        Row(\"Alice\", 13), \n        Row(\"Bob\", 12)\n    ))\nval schema = StructType(\n    StructField(\"name\", StringType, true) ::\n    StructField(\"age\", IntegerType, true) ::\n    Nil\n)\nval df = spark.createDataFrame(rdd, schema)\n\ndf.show()","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.sql.Row\n\nimport org.apache.spark.sql.types._\n\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[16] at parallelize at <console>:31\n\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,IntegerType,true))\n\ndf: org.apache.spark.sql.DataFrame = [name: string, age: int]\n+-----+---+\n| name|age|\n+-----+---+\n|Alice| 13|\n|  Bob| 12|\n+-----+---+\n\n"}]},"apps":[],"jobName":"paragraph_1542726508851_-581464754","id":"20160612-150748_776002189","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16371"},{"text":"%md\n## 1.6 Accessing the RDD in the DataFrame\nEach DataFrame can be converted to an RDD. This can also be accessed via the `rdd` field of a DataFrame. Note that the RDD is created lazily, since this presents an optimization barrier","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.6 Accessing the RDD in the DataFrame</h2>\n<p>Each DataFrame can be converted to an RDD. This can also be accessed via the <code>rdd</code> field of a DataFrame. Note that the RDD is created lazily, since this presents an optimization barrier</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508854_-582619001","id":"20160612-151322_1187613408","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16372"},{"text":"val rdd = df.rdd\n\nrdd.collect().foreach(println)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[22] at rdd at <console>:37\n[Alice,13]\n[Bob,12]\n"}]},"apps":[],"jobName":"paragraph_1542726508858_-584157997","id":"20160612-151333_943343452","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16373"},{"text":"%md\n# 2 Reading a DataFrame\nOf course normally you won't manually construct a DataFrame but you want to read from files in HDFS/S3/whatever instead. You can retrieve a `DataFrameReader` instance from the Spark session via `spark.read`. This reader supports multiple file formats (most notably JSON, CSV, Parquet and ORC). Additional libraries add support for more file formats like Avro.\n\nIn the following example we will use a JSON file file containing some informations on a couple of persons, which will serve as the basis for the next examples","dateUpdated":"2018-11-20T15:27:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>2 Reading a DataFrame</h1>\n<p>Of course normally you won&rsquo;t manually construct a DataFrame but you want to read from files in HDFS/S3/whatever instead. You can retrieve a <code>DataFrameReader</code> instance from the Spark session via <code>spark.read</code>. This reader supports multiple file formats (most notably JSON, CSV, Parquet and ORC). Additional libraries add support for more file formats like Avro.</p>\n<p>In the following example we will use a JSON file file containing some informations on a couple of persons, which will serve as the basis for the next examples</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508862_-585696992","id":"20170105-012624_2037335489","dateCreated":"2018-11-20T15:08:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16374","user":"anonymous","dateFinished":"2018-11-20T15:27:33+0000","dateStarted":"2018-11-20T15:27:33+0000"},{"text":"val persons = spark.read.json(\"s3://dimajix-training/data/persons.json\")\nz.show(persons)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njava.lang.IllegalArgumentException: AWS Access Key ID and Secret Access Key must be specified as the username or password (respectively) of a s3 URL, or by setting the fs.s3.awsAccessKeyId or fs.s3.awsSecretAccessKey properties (respectively).\n  at org.apache.hadoop.fs.s3.S3Credentials.initialize(S3Credentials.java:70)\n  at org.apache.hadoop.fs.s3.Jets3tFileSystemStore.initialize(Jets3tFileSystemStore.java:93)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n  at com.sun.proxy.$Proxy19.initialize(Unknown Source)\n  at org.apache.hadoop.fs.s3.S3FileSystem.initialize(S3FileSystem.java:91)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:381)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:379)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:344)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:379)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:294)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:249)\n  ... 48 elided\n"}]},"apps":[],"jobName":"paragraph_1542726508865_-600702199","id":"20170105-012654_1765180674","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16375"},{"text":"%md\nThe JSON file looks as follows:\n\n    { \"name\":\"Alice\", \"age\":14, \"sex\":\"female\", \"height\":176 }\n    { \"name\":\"Bob\", \"age\":21, \"sex\":\"male\", \"height\":181 }\n    { \"name\":\"Charlie\", \"age\":27, \"sex\":\"male\", \"height\":176 }\n    { \"name\":\"Eve\", \"age\":24, \"sex\":\"female\", \"height\":167 }\n    { \"name\":\"Frances\", \"age\":19, \"sex\":\"female\", \"height\":172 }\n    { \"name\":\"George\", \"age\":31, \"sex\":\"male\", \"height\":191 }\n","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The JSON file looks as follows:</p>\n<pre><code>{ &quot;name&quot;:&quot;Alice&quot;, &quot;age&quot;:14, &quot;sex&quot;:&quot;female&quot;, &quot;height&quot;:176 }\n{ &quot;name&quot;:&quot;Bob&quot;, &quot;age&quot;:21, &quot;sex&quot;:&quot;male&quot;, &quot;height&quot;:181 }\n{ &quot;name&quot;:&quot;Charlie&quot;, &quot;age&quot;:27, &quot;sex&quot;:&quot;male&quot;, &quot;height&quot;:176 }\n{ &quot;name&quot;:&quot;Eve&quot;, &quot;age&quot;:24, &quot;sex&quot;:&quot;female&quot;, &quot;height&quot;:167 }\n{ &quot;name&quot;:&quot;Frances&quot;, &quot;age&quot;:19, &quot;sex&quot;:&quot;female&quot;, &quot;height&quot;:172 }\n{ &quot;name&quot;:&quot;George&quot;, &quot;age&quot;:31, &quot;sex&quot;:&quot;male&quot;, &quot;height&quot;:191 }\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508868_-601856446","id":"20180324-181125_742118475","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16376"},{"text":"persons.printSchema()","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508869_-602241195","id":"20181110-122808_2097838626","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16377"},{"text":"%md\n## 2.1 Reading CSV files\nSpark also supports to read CSV files. It even supports headers in CSV files and can optionally try to guess the types of columns.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>2.1 Reading CSV files</h2>\n<p>Spark also supports to read CSV files. It even supports headers in CSV files and can optionally try to guess the types of columns.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508871_-601471697","id":"20180324-181343_701519241","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16378"},{"text":"val persons = spark.read\n    .option(\"header\",\"true\")\n    .option(\"inferSchema\",\"true\")\n    .csv(\"s3://dimajix-training/data/persons_header.csv\")\n\nz.show(persons)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508873_-603780191","id":"20170105-012711_1659277547","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16379"},{"text":"persons.printSchema()","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508876_-604934437","id":"20170105-014033_512220053","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16380"},{"text":"%md\n## 2.2 Specifying a Schema\nIn many cases you don't want to rely on automatic schema detection from Spark, because this might not be stable (on different files) or you might miss important changes in the schema in new files. You can (and should) explicitly specify a schema when working with CSV files for reproducible results.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>2.2 Specifying a Schema</h2>\n<p>In many cases you don&rsquo;t want to rely on automatic schema detection from Spark, because this might not be stable (on different files) or you might miss important changes in the schema in new files. You can (and should) explicitly specify a schema when working with CSV files for reproducible results.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508881_-594546217","id":"20180324-181440_1160826041","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16381"},{"text":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.IntegerType\n\nval schema = StructType(\n    StructField(\"age\", IntegerType) ::\n    StructField(\"height\", IntegerType) ::\n    StructField(\"name\", StringType) ::\n    StructField(\"sex\", StringType) ::\n    Nil\n    )\n    \nval persons = spark.read\n    .schema(schema)\n    .csv(\"s3://dimajix-training/data/persons_headerless.csv\")\n\nz.show(persons)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"age","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"age","index":1,"aggr":"sum"}}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508885_-596085213","id":"20170105-013943_566941124","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16382"},{"text":"persons.printSchema()","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508888_-597239459","id":"20181110-122933_1751043793","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16383"},{"text":"%md\n# 3 Simple Transformations\n\nNow that we know how to create DataFrames via Scala sequences and RDDs or read data from files in HDFS/S3, we want to actually do something useful with them. The simplest thing is *selecting columns* (i.e. projections). Here we'll immediately see an important difference to RDDs: DataFrames are completely dynamically typed objects. The schema of a DataFrame is not known at compile time (even if it was manually created in code, it is still represented dynamically). This allows a more generic approach and does not requiore to write custom classes for different objects. Everything is a data frame with a dynamically injected schema. Columns are addressed via their names in strings - not as members of some Scala classes.\n\nNevertheless this is already a topic with multiple solutions. Let's first look at the simplest approach, which selects one column by its name:","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>3 Simple Transformations</h1>\n<p>Now that we know how to create DataFrames via Scala sequences and RDDs or read data from files in HDFS/S3, we want to actually do something useful with them. The simplest thing is <em>selecting columns</em> (i.e. projections). Here we&rsquo;ll immediately see an important difference to RDDs: DataFrames are completely dynamically typed objects. The schema of a DataFrame is not known at compile time (even if it was manually created in code, it is still represented dynamically). This allows a more generic approach and does not requiore to write custom classes for different objects. Everything is a data frame with a dynamically injected schema. Columns are addressed via their names in strings - not as members of some Scala classes.</p>\n<p>Nevertheless this is already a topic with multiple solutions. Let&rsquo;s first look at the simplest approach, which selects one column by its name:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508891_-596854710","id":"20160612-151621_251190447","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16384"},{"text":"%md\n## 3.1 Projections\n\nThe simplest thing to do is to create a new DataFrame with a subset of the available columns. In SQL this type of operation would be performed by a simple `SELECT` statement. It's not different with Spark DataFrames.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.1 Projections</h2>\n<p>The simplest thing to do is to create a new DataFrame with a subset of the available columns. In SQL this type of operation would be performed by a simple <code>SELECT</code> statement. It&rsquo;s not different with Spark DataFrames.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508893_-599163204","id":"20181110-123030_533120490","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16385"},{"text":"val result = persons.select(\"name\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508894_-598008957","id":"20160612-151633_1423873011","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16386"},{"text":"%md\n## 3.2 Addressing Columns\n\nSpark supports multiple different ways for addressing a columns. We just saw one way, but also the following methods are supported for specifying a column:\n\n* Via `$\"column_name\"` - this works like an unqualified column in SQL\n* Via `'column_name` - this works like an unqualified column in SQL\n* Via the function `col(\"column_name\")` - this works like an unqualified column in SQL\n* Via using the dataframe itself as reference in `dataframe(\"column_name\")` - this works like a column qualified with its (temporary) table name in SQL\n \nThe last syntax helps to resolve ambigious cases, for example if some data frame is the result of a **JOIN** operation with overlapping column names.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.2 Addressing Columns</h2>\n<p>Spark supports multiple different ways for addressing a columns. We just saw one way, but also the following methods are supported for specifying a column:</p>\n<ul>\n  <li>Via <code>$&quot;column_name&quot;</code> - this works like an unqualified column in SQL</li>\n  <li>Via <code>&#39;column_name</code> - this works like an unqualified column in SQL</li>\n  <li>Via the function <code>col(&quot;column_name&quot;)</code> - this works like an unqualified column in SQL</li>\n  <li>Via using the dataframe itself as reference in <code>dataframe(&quot;column_name&quot;)</code> - this works like a column qualified with its (temporary) table name in SQL</li>\n</ul>\n<p>The last syntax helps to resolve ambigious cases, for example if some data frame is the result of a <strong>JOIN</strong> operation with overlapping column names.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508896_-612629415","id":"20180326-155235_1512036684","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16387"},{"text":"val result = persons.select($\"age\", col(\"name\"), 'height, persons(\"sex\"))\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"age","index":0,"aggr":"sum"}],"values":[{"name":"name","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"age","index":0,"aggr":"sum"},"yAxis":{"name":"name","index":1,"aggr":"sum"}}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508897_-613014164","id":"20160612-152001_527472554","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16388"},{"text":"%md\n## 3.3 Transformations\n\nThe `select` method actually accepts any column object. A column object conceptually represents a column in a DataFrame. The column may either refer directly to an existing column of the input DataFrame, or it may represent the result of a calculation or transformation of one or multiple columns of the input DataFrame. For example if we simply want to transform the name into upper case, we can do so by using a function upper provided by PySpark.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.3 Transformations</h2>\n<p>The <code>select</code> method actually accepts any column object. A column object conceptually represents a column in a DataFrame. The column may either refer directly to an existing column of the input DataFrame, or it may represent the result of a calculation or transformation of one or multiple columns of the input DataFrame. For example if we simply want to transform the name into upper case, we can do so by using a function upper provided by PySpark.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508899_-612244666","id":"20160612-152140_1844336454","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16389"},{"text":"val result = persons.select(concat(lit(\"Name:\"), col(\"name\"), lit(\" Age:\"), $\"age\"))\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508901_-614553160","id":"20160612-152200_1663756389","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16390"},{"text":"%md\n### Defining new Column Names\n\nThe resulting DataFrame again has a schema, but the column names to not look very nice. But by using the `alias` method of a `Column` object, you can immediately rename the newly created column like you are already used to in SQL with `SELECT complex_operation(...) AS nice_name FROM ....`\n\nTechnically specifying a new name for the resulting column is not required (as we already saw above), if the name is not specified, PySpark will generate a name from the expression. But since this generated name tends to be rather long and contains the logic instead of the intention, it is highly recommended to always explicitly specify the name of the resulting column using `as`.\n","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Defining new Column Names</h3>\n<p>The resulting DataFrame again has a schema, but the column names to not look very nice. But by using the <code>alias</code> method of a <code>Column</code> object, you can immediately rename the newly created column like you are already used to in SQL with <code>SELECT complex_operation(...) AS nice_name FROM ....</code></p>\n<p>Technically specifying a new name for the resulting column is not required (as we already saw above), if the name is not specified, PySpark will generate a name from the expression. But since this generated name tends to be rather long and contains the logic instead of the intention, it is highly recommended to always explicitly specify the name of the resulting column using <code>as</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508903_-613783662","id":"20181110-123316_550950756","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16391"},{"text":"val result = persons.select('name, upper('name).alias(\"upper_name\"))\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508905_-616092155","id":"20160612-152358_839240513","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16392"},{"text":"%md\nYou can also perform simple mathematical calculations like addition, multiplication etc.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>You can also perform simple mathematical calculations like addition, multiplication etc.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508907_-615322658","id":"20181110-123447_207709925","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16393"},{"text":"val result = persons.select('name, ($\"age\" * 2).alias(\"double_age\"))\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"}}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508908_-617246402","id":"20160612-152435_1333366716","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16394"},{"text":"%md\n### Case/When Functions\nSpark also provides the `CASE ... WHEN ... END` construct known from SQL, where you can implemenet if/then conditions for individual rows.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Case/When Functions</h3>\n<p>Spark also provides the <code>CASE ... WHEN ... END</code> construct known from SQL, where you can implemenet if/then conditions for individual rows.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508911_-616861653","id":"20180326-160033_789881465","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16395"},{"text":"val result = persons.select(\n        $\"name\",\n        $\"age\",\n        when($\"sex\" === \"male\", \"Mr\").otherwise(\"Mrs\").alias(\"salutation\")\n    )\n    \nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508913_-606858182","id":"20160612-152524_1040140298","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16396"},{"text":"%md\n### Common Functions\n\nYou can find the full list of available functions in the Spark documentation in `org.apache.spark.sql.functiuons`. Commonly used functions for example are as follows:\n\n* `concat(cols,...)` - Concatenates multiple input columns together into a single column.\n* `substring(col,start,len)` - Substring starts at pos and is of length len when str is String type or returns the slice of byte array that starts at pos in byte and is of length len when str is Binary type.\n* `instr(col,substr)` - Locate the position of the first occurrence of substr column in the given string. Returns null if either of the arguments are null.\n* `locate(col,substr, pos)` - Locate the position of the first occurrence of substr in a string column, after position pos.\n* `length(col)` - Computes the character length of string data or number of bytes of binary data.\n* `upper(col)` - Converts a string column to upper case.\n* `lower(col)` - Converts a string column to lower case.\n* `coalesce(col,...)` - Returns the first column that is not null.\n* `isnull(col)` - An expression that returns true iff the column is null.\n* `isnan(col)` - An expression that returns true iff the column is NaN.\n* `hash(col,...)` - Calculates the hash code of given columns.\n\nSpark also supports conditional expressions, like the SQL `CASE WHEN` construct\n\n* `when(condition, value)` - Evaluates a list of conditions and returns one of multiple possible result expressions.\n\nThere are also some special functions often required\n\n* `col(str)` - Returns a Column based on the given column name.\n* `lit(val)` - Creates a Column of literal value.\n* `expr(str)` - Parses the expression string into the column that it represents\n\n### User Defined Functions\n\nUnfortunately you cannot directly use normal Python functions for transforming DataFrame columns. Although PySpark already provides many useful functions, this might not always sufficient. But fortunately you can convert a standard Python function into a PySpark function, thereby defining a so called user defined function (UDF). Details will be explained in detail in the training.\n","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Common Functions</h3>\n<p>You can find the full list of available functions in the Spark documentation in <code>org.apache.spark.sql.functiuons</code>. Commonly used functions for example are as follows:</p>\n<ul>\n  <li><code>concat(cols,...)</code> - Concatenates multiple input columns together into a single column.</li>\n  <li><code>substring(col,start,len)</code> - Substring starts at pos and is of length len when str is String type or returns the slice of byte array that starts at pos in byte and is of length len when str is Binary type.</li>\n  <li><code>instr(col,substr)</code> - Locate the position of the first occurrence of substr column in the given string. Returns null if either of the arguments are null.</li>\n  <li><code>locate(col,substr, pos)</code> - Locate the position of the first occurrence of substr in a string column, after position pos.</li>\n  <li><code>length(col)</code> - Computes the character length of string data or number of bytes of binary data.</li>\n  <li><code>upper(col)</code> - Converts a string column to upper case.</li>\n  <li><code>lower(col)</code> - Converts a string column to lower case.</li>\n  <li><code>coalesce(col,...)</code> - Returns the first column that is not null.</li>\n  <li><code>isnull(col)</code> - An expression that returns true iff the column is null.</li>\n  <li><code>isnan(col)</code> - An expression that returns true iff the column is NaN.</li>\n  <li><code>hash(col,...)</code> - Calculates the hash code of given columns.</li>\n</ul>\n<p>Spark also supports conditional expressions, like the SQL <code>CASE WHEN</code> construct</p>\n<ul>\n  <li><code>when(condition, value)</code> - Evaluates a list of conditions and returns one of multiple possible result expressions.</li>\n</ul>\n<p>There are also some special functions often required</p>\n<ul>\n  <li><code>col(str)</code> - Returns a Column based on the given column name.</li>\n  <li><code>lit(val)</code> - Creates a Column of literal value.</li>\n  <li><code>expr(str)</code> - Parses the expression string into the column that it represents</li>\n</ul>\n<h3>User Defined Functions</h3>\n<p>Unfortunately you cannot directly use normal Python functions for transforming DataFrame columns. Although PySpark already provides many useful functions, this might not always sufficient. But fortunately you can convert a standard Python function into a PySpark function, thereby defining a so called user defined function (UDF). Details will be explained in detail in the training.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508915_-606088684","id":"20181110-123621_1964144417","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16397"},{"text":"%md\n## 3.4 Injecting Literal Values\n\nSometimes it is required to inkect literal values (i.e. strings or numbers) into a transformation expression. Since using simply a string could mean wither a column with that name or the literal itself, Spark offers the function `lit` to explicitly mark a string (or any other value) as a literal. `lit` creates a Spark column object from the value. This means that all column methods are avilable for the literal.\n","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.4 Injecting Literal Values</h2>\n<p>Sometimes it is required to inkect literal values (i.e. strings or numbers) into a transformation expression. Since using simply a string could mean wither a column with that name or the literal itself, Spark offers the function <code>lit</code> to explicitly mark a string (or any other value) as a literal. <code>lit</code> creates a Spark column object from the value. This means that all column methods are avilable for the literal.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508916_-608012429","id":"20181110-124018_1551886893","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16398"},{"text":"val result = persons.select(concat(lit(\"Name:\"), persons.name, lit(\" Age:\"), persons.age).alias(\"text\"))\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508918_-607242931","id":"20181110-124110_1563256821","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16399"},{"text":"%md\n## 3.5 Exercises\n\nWrite a small select statement, which creates a new column called \"salutation\" which contains either \"Mr <name>\" or \"Mrs <name>\" dependeing on the sex of each person.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.5 Exercises</h2>\n<p>Write a small select statement, which creates a new column called &ldquo;salutation&rdquo; which contains either &ldquo;Mr <name>&rdquo; or &ldquo;Mrs <name>&rdquo; dependeing on the sex of each person.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508921_-609936173","id":"20181110-124200_2075950165","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16400"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508923_-609166675","id":"20181110-124316_1635308081","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16401"},{"text":"%md\n## 3.6 Adding Columns\n\nSpark not only allows you to transform DataFrames via `select`, it also provides a very convenient method `withColumn` for adding columns to a DataFrames. This frees you from copying all columns in a `select` statement when you only want to create new columns.\n\nAs you can see from the example above, withColumn always takes two arguments: The first one is the name of the new column (and it has to be a string), and the second argument is the expression containing the logic for calculating the actual contents.\n\nIf you specify the name of an existing column as the target column, the existing column will be replaced with the new expression.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.6 Adding Columns</h2>\n<p>Spark not only allows you to transform DataFrames via <code>select</code>, it also provides a very convenient method <code>withColumn</code> for adding columns to a DataFrames. This frees you from copying all columns in a <code>select</code> statement when you only want to create new columns.</p>\n<p>As you can see from the example above, withColumn always takes two arguments: The first one is the name of the new column (and it has to be a string), and the second argument is the expression containing the logic for calculating the actual contents.</p>\n<p>If you specify the name of an existing column as the target column, the existing column will be replaced with the new expression.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508926_-610320922","id":"20160617-162557_838805065","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16402"},{"text":"val result = persons.withColumn(\"salutation\", when(persons(\"sex\") === \"male\", \"Mr\").otherwise(\"Mrs\"))\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508928_-526445662","id":"20160617-162610_764186088","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16403"},{"text":"%md\nTechnically you could implement a generic `withColumn` by using some Scala code for collecting all incoming columns.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Technically you could implement a generic <code>withColumn</code> by using some Scala code for collecting all incoming columns.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508930_-525676164","id":"20180326-160327_1340912163","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16404"},{"text":"val cols = persons.schema.fields.map(field => col(field.name)) :+ \n            when(col(\"sex\") === lit(\"male\"), lit(\"Mr\")).otherwise(\"Mrs\").alias(\"title\")\n            \nval result = persons.select(cols:_*)\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508932_-527984657","id":"20180326-160242_1868749324","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16405"},{"text":"%md \nA more convenient way would be to use the special expression `col(\"*\")`","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>A more convenient way would be to use the special expression <code>col(&quot;*&quot;)</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508936_-529523653","id":"20180326-160407_273315784","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16406"},{"text":"val result = persons.select(col(\"*\"), when($\"age\" % 2 === 0, \"even_age\").otherwise(\"odd_age\").as(\"text\"))\n\nz.show(result)\n","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508938_-528754155","id":"20180326-160438_1633012540","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16407"},{"text":"%md\n## 3.7 Dropping Columns\nSometimes you just want to perform the opposite operation: Get rid of some columns. This can be useful for cleaning up temporary results before writing a result to disk.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.7 Dropping Columns</h2>\n<p>Sometimes you just want to perform the opposite operation: Get rid of some columns. This can be useful for cleaning up temporary results before writing a result to disk.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508940_-531062648","id":"20160617-162733_380188317","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16408"},{"text":"val result = result.drop(\"age\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508943_-530677900","id":"20160617-162750_466208521","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16409"},{"text":"%md\n## 3.8 Exercise\n\nUsing the `persons` DataFrame, perform the following operations:\n* Add a new column `status` which should be `child` if the person is younger than 18 and `adult` otherwise\n* Replace the column `name` by a new column `hashed_name` containing the hash value of the name\n* Drop the column `sex`","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>3.8 Exercise</h2>\n<p>Using the <code>persons</code> DataFrame, perform the following operations:<br/>* Add a new column <code>status</code> which should be <code>child</code> if the person is younger than 18 and <code>adult</code> otherwise<br/>* Replace the column <code>name</code> by a new column <code>hashed_name</code> containing the hash value of the name<br/>* Drop the column <code>sex</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508944_-520289679","id":"20181110-124645_421519774","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16410"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508946_-519520181","id":"20181110-124706_1845369036","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16411"},{"text":"%md\n# 4 Filtering\n\nNow we have seen how we can perform 1:1 transformations, i.e. every row of some incoming DataFrame is transformed and produces exactly one outgoing row. Often we also require to remove some rows, i.e. we would like to perform a SQL `WHERE` clause. This is also supported with some easy syntax in Spark. The simplest way is to construct a boolean column expression and then pass it as a predicate into the `filter` or `where` method of a DataFrame. Both methods are completely equivalent, the `filter` method is for Scala developers and the `where` method for all of you with an SQL background.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>4 Filtering</h1>\n<p>Now we have seen how we can perform 1:1 transformations, i.e. every row of some incoming DataFrame is transformed and produces exactly one outgoing row. Often we also require to remove some rows, i.e. we would like to perform a SQL <code>WHERE</code> clause. This is also supported with some easy syntax in Spark. The simplest way is to construct a boolean column expression and then pass it as a predicate into the <code>filter</code> or <code>where</code> method of a DataFrame. Both methods are completely equivalent, the <code>filter</code> method is for Scala developers and the <code>where</code> method for all of you with an SQL background.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508948_-521828675","id":"20160612-153031_659656525","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16412"},{"text":"val result = persons.filter($\"age\" > 22)\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"age","index":0,"aggr":"sum"}],"values":[{"name":"height","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"age","index":0,"aggr":"sum"},"yAxis":{"name":"height","index":1,"aggr":"sum"}}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508950_-521059177","id":"20160612-153548_1115248994","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16413"},{"text":"%md\nAlternatively you can also specify a condition completely as an SQL expression, which then is parsed by Spark and translated into a column expression like above.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Alternatively you can also specify a condition completely as an SQL expression, which then is parsed by Spark and translated into a column expression like above.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508951_-521443926","id":"20180326-160943_1911127373","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16414"},{"text":"val result = persons.where(\"age > 22\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508952_-523367670","id":"20160612-153209_1553421093","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16415"},{"text":"%md\nWhen using SQL expressions, you can also work with aliases to address specific DataFrames in case of ambiguous column names:","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>When using SQL expressions, you can also work with aliases to address specific DataFrames in case of ambiguous column names:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508954_-522598173","id":"20181110-124756_1692293962","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16416"},{"text":"val result = persons.alias(\"persons\").filter(\"persons.age > 22 AND persons.height < 170\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508955_-522982922","id":"20181110-124805_1743096658","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16417"},{"text":"%md\n## 4.1 Exercise\n\nPerform different filter operations:\n\n* Select all women with a height of at least 160\n* Select all persons which are younger than 20 or older than 30","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>4.1 Exercise</h2>\n<p>Perform different filter operations:</p>\n<ul>\n  <li>Select all women with a height of at least 160</li>\n  <li>Select all persons which are younger than 20 or older than 30</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508957_-525291415","id":"20181110-124844_824235455","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16418"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1542726508958_-524137168","id":"20181110-124907_643940099","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16419"},{"text":"%md\n## 4.2 Limit Operations\n\nWhen working with large datasets, it may be helpful to limit the amount of records (like an SQL `LIMIT` operation).","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>4.2 Limit Operations</h2>\n<p>When working with large datasets, it may be helpful to limit the amount of records (like an SQL <code>LIMIT</code> operation).</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508959_-524521917","id":"20181110-124929_1994756216","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16420"},{"text":"val result = persons.limit(3)\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508961_-539142375","id":"20181110-124953_446516372","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16421"},{"text":"%md\n# 5 Aggregations\n\nSpark also supports many aggregations like `min`, `max`, `sum`, `avg` and so on. If used directly inside a `select`, then the aggregation will run on the whole DataFrame and produce a single row as the result. You can find all aggregation functions again in the object `org.apache.spark.sql.functions`.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>5 Aggregations</h1>\n<p>Spark also supports many aggregations like <code>min</code>, <code>max</code>, <code>sum</code>, <code>avg</code> and so on. If used directly inside a <code>select</code>, then the aggregation will run on the whole DataFrame and produce a single row as the result. You can find all aggregation functions again in the object <code>org.apache.spark.sql.functions</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508963_-538372878","id":"20160612-153253_1859986543","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16422"},{"text":"val df = sc.parallelize(1 to 100).toDF\nval result = df.select(\n    sum('value).alias(\"sum\"), \n    avg('value).alias(\"avg\"), \n    min('value).alias(\"min\"), \n    max('value).alias(\"max\"), \n    count('value).alias(\"count\")\n)\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":89,"optionOpen":false,"keys":[{"name":"sum","index":0,"aggr":"sum"}],"values":[{"name":"avg","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"sum","index":0,"aggr":"sum"},"yAxis":{"name":"avg","index":1,"aggr":"sum"}}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508964_-540296622","id":"20160612-153747_1650442270","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16423"},{"text":"%md\nSome very simple aggregation functions are also provided directly without the need for a `select`, most notably `count`.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Some very simple aggregation functions are also provided directly without the need for a <code>select</code>, most notably <code>count</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508966_-539527124","id":"20180326-161239_1481493783","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16424"},{"text":"df.count()","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508967_-539911873","id":"20160612-154004_869347895","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16425"},{"text":"%md\n# 6 Making Data Distinct\n\nSpark also offers a special function for removing duplicate rows. This would be equivalent to an SQL `SELECT DISTINCT *`","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>6 Making Data Distinct</h1>\n<p>Spark also offers a special function for removing duplicate rows. This would be equivalent to an SQL <code>SELECT DISTINCT *</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508969_-542220367","id":"20160612-154030_610244901","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16426"},{"text":"val df = sc.parallelize(Array(\"Alice\",\"Bob\",\"Alice\")).toDF\nval result = df.distinct()\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508971_-541450869","id":"20160612-154035_439876147","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16427"},{"text":"%md\n# 8 Grouping and Aggregating\nOf course in most cases it is required to use grouped aggregations. Spark offers a `groupBy` method for creating groups. In contrast to most other methods, this does not return a new DataFrame. Instead a special *grouped data object* is returned which requires an aggregation to be performed as the next step. Some simple numeric aggregations like `min`, `max`, `sum` and `avg` are available directly as part of the grouped object like in the following example.\n\nNote that you only need to specify the aggregation expression, the grouping columns are added automatically by Spark to the resulting DataFrame.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>8 Grouping and Aggregating</h1>\n<p>Of course in most cases it is required to use grouped aggregations. Spark offers a <code>groupBy</code> method for creating groups. In contrast to most other methods, this does not return a new DataFrame. Instead a special <em>grouped data object</em> is returned which requires an aggregation to be performed as the next step. Some simple numeric aggregations like <code>min</code>, <code>max</code>, <code>sum</code> and <code>avg</code> are available directly as part of the grouped object like in the following example.</p>\n<p>Note that you only need to specify the aggregation expression, the grouping columns are added automatically by Spark to the resulting DataFrame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508973_-543759362","id":"20160612-154202_1946092920","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16428"},{"text":"val result = persons.select($\"sex\",$\"age\").groupBy($\"sex\").avg()\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[{"name":"avg(age)","index":1,"aggr":"sum"}],"groups":[],"scatter":{"yAxis":{"name":"avg(age)","index":1,"aggr":"sum"}}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508976_-532601644","id":"20160612-154557_888346984","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16429"},{"text":"%md\nIn addition to use simple aggregations, you can also use the generic `agg` method which essentially works like a `select` with the only difference that it only supports aggregations. Again the `groupBy` columns are added to the result implicitly by Spark.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In addition to use simple aggregations, you can also use the generic <code>agg</code> method which essentially works like a <code>select</code> with the only difference that it only supports aggregations. Again the <code>groupBy</code> columns are added to the result implicitly by Spark.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508979_-532216895","id":"20180326-161627_1523001075","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16430"},{"text":"val result = persons.groupBy($\"sex\")\n    .agg(\n        avg(col(\"age\")) as \"avg_age\",\n        min(col(\"height\")) as \"min_height\",\n        max(col(\"height\")) as \"max_height\"\n    )\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508982_-533371142","id":"20160612-154724_579125114","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16431"},{"text":"%md\nSometimes it may be useful to access all elements of a group as a list. But since `groupBy` does not return a normal DataFrame and requires an aggregate function as the next step, this requires a small trick. Using the `collect_list` function, you can put all elemenets of a single column of every group into a new column.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Sometimes it may be useful to access all elements of a group as a list. But since <code>groupBy</code> does not return a normal DataFrame and requires an aggregate function as the next step, this requires a small trick. Using the <code>collect_list</code> function, you can put all elemenets of a single column of every group into a new column.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508984_-535679635","id":"20181110-125329_1785716427","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16432"},{"text":"val result = persons.groupBy($\"sex\").agg(\n        collect_list(struct($\"name\", $\"age\")).alias(\"list\")\n)\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508986_-534910137","id":"20181110-125404_603357600","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16433"},{"text":"result.printSchema()","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508988_-537218631","id":"20181110-125440_2135957395","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16434"},{"text":"%md\n### Aggregation Functions\n\nSpark supports many aggregation functions, they can be found in the documentation at Spark documentation at `org.apache.spark.sql.functions`. Aggregation functions are marked as such in the documentation. Among common aggregation functions, there are for example:\n\n* count\n* sum\n* avg\n* corr\n* first\n* last","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Aggregation Functions</h3>\n<p>Spark supports many aggregation functions, they can be found in the documentation at Spark documentation at <code>org.apache.spark.sql.functions</code>. Aggregation functions are marked as such in the documentation. Among common aggregation functions, there are for example:</p>\n<ul>\n  <li>count</li>\n  <li>sum</li>\n  <li>avg</li>\n  <li>corr</li>\n  <li>first</li>\n  <li>last</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508989_-537603380","id":"20181110-125455_507762122","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16435"},{"text":"%md\n## 8.1 Exercise\n\nUsing the `persons` DataFrame, calculate the average height and the number of records per sex.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>8.1 Exercise</h2>\n<p>Using the <code>persons</code> DataFrame, calculate the average height and the number of records per sex.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508993_-551454340","id":"20181110-125636_127166522","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16436"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-11-20T15:08:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508995_-550684842","id":"20181110-125635_1898890181","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16437"},{"text":"%md\n# 8 Sorting Data\n\nFinally results often need to be sorted (by importance, frequence or whatever). This can be done by the `orderBy` method of a DataFrame, which takes one or multiple columns for defining the order.","dateUpdated":"2018-11-20T15:08:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>8 Sorting Data</h1>\n<p>Finally results often need to be sorted (by importance, frequence or whatever). This can be done by the <code>orderBy</code> method of a DataFrame, which takes one or multiple columns for defining the order.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726508997_-552993336","id":"20160612-154903_648261643","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16438"},{"text":"val result = persons.orderBy(col(\"name\"))\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726508999_-552223838","id":"20160612-154939_820007544","dateCreated":"2018-11-20T15:08:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16439"},{"text":"%md\nIf nothing else is specified, Spark will sort the records in increasing order of the sort columns. If you require descending order, this can be specified by manipulating the sort column with the `desc` method as follows:","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>If nothing else is specified, Spark will sort the records in increasing order of the sort columns. If you require descending order, this can be specified by manipulating the sort column with the <code>desc</code> method as follows:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509000_-554147583","id":"20180326-161941_169508617","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16440"},{"text":"val result = persons.orderBy($\"age\".desc)\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509002_-553378085","id":"20160612-155236_301979744","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16441"},{"text":"%md\n## 8.1 Exercise\n\nAs an exercise we want to sort all persons first by their sex and then by their descening age. Sorting by multiple columns can easily be achieved by specifying multiple columns as arguments in the `orderBy` method.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>8.1 Exercise</h2>\n<p>As an exercise we want to sort all persons first by their sex and then by their descening age. Sorting by multiple columns can easily be achieved by specifying multiple columns as arguments in the <code>orderBy</code> method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509004_-555686578","id":"20181110-125812_375582333","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16442"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509007_-555301829","id":"20181110-125837_1682369152","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16443"},{"text":"%md\n# 9 Joining Data\n\nEvery relation algebra also contains join operations which lets you combine multiple tables by a matching criterion. Spark also supports joins of multiple DataFrames. In order to shed some light on that, we need a second DataFrame in addition to the `persons` DataFrame. Therefore we load some address data as follows:","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>9 Joining Data</h1>\n<p>Every relation algebra also contains join operations which lets you combine multiple tables by a matching criterion. Spark also supports joins of multiple DataFrames. In order to shed some light on that, we need a second DataFrame in addition to the <code>persons</code> DataFrame. Therefore we load some address data as follows:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509009_-545298358","id":"20160612-155307_1442775029","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16444"},{"text":"val addresses = spark.read.json(\"s3://dimajix-training/data/addresses.json\")\nz.show(addresses)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509011_-544528860","id":"20170105-020841_2109038029","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16445"},{"text":"%md\nNow that we have the `addresses` DataFrame, we want to combine it with the `persons` DataFrame such that the city of every person is added as a new column. This is achieved by the join method which essentially takes two parameters: The first parameter specifies the second DataFrame to join with, and the second parameter specifies the join condition. In this case we want to join all records, where the name column matches.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now that we have the <code>addresses</code> DataFrame, we want to combine it with the <code>persons</code> DataFrame such that the city of every person is added as a new column. This is achieved by the join method which essentially takes two parameters: The first parameter specifies the second DataFrame to join with, and the second parameter specifies the join condition. In this case we want to join all records, where the name column matches.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509013_-546837353","id":"20180326-162127_1642011647","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16446"},{"text":"val result = persons.join(addresses, persons(\"name\") === addresses(\"name\"))\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509015_-546067856","id":"20160612-155332_1943504111","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16447"},{"text":"%md\nLet me make some relevant remarks:\n\n* The resulting DataFrame now contains two `name` columns - one comes from the `persons` DataFrame, the other from the `addresses` DataFrame. Since the join condition could have used some more complex expression, this behaviour is only logical since PySpark cannot assume that all joins simply use directly some column value. For example we could also have transformed the column on the fly by converting the name to upper case directly inside the join condition.\n* The result contains only persons where an address was found, although the original `persons` DataFrame contained more persons.\n* There are no records of addresses without any person, although the `addresses` DataFrame contains information about some persons not available in the persons DataFrame.\n\nSo let us first address the first observation. We can easily get rid of the copied name column by either performing an explicit select of the desired columns, or by dropping the duplicate columns. Since PySpark records the lineage of every column, the duplicate name columns can be addressed by their original DataFrame even after the join operation:\n","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Let me make some relevant remarks:</p>\n<ul>\n  <li>The resulting DataFrame now contains two <code>name</code> columns - one comes from the <code>persons</code> DataFrame, the other from the <code>addresses</code> DataFrame. Since the join condition could have used some more complex expression, this behaviour is only logical since PySpark cannot assume that all joins simply use directly some column value. For example we could also have transformed the column on the fly by converting the name to upper case directly inside the join condition.</li>\n  <li>The result contains only persons where an address was found, although the original <code>persons</code> DataFrame contained more persons.</li>\n  <li>There are no records of addresses without any person, although the <code>addresses</code> DataFrame contains information about some persons not available in the persons DataFrame.</li>\n</ul>\n<p>So let us first address the first observation. We can easily get rid of the copied name column by either performing an explicit select of the desired columns, or by dropping the duplicate columns. Since PySpark records the lineage of every column, the duplicate name columns can be addressed by their original DataFrame even after the join operation:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509017_-548376349","id":"20181110-130039_1108957771","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16448"},{"text":"val result = persons.join(addresses, persons(\"name\") === addresses(\"name\")).drop(addresses(\"name\"))\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509019_-547606851","id":"20181110-130153_1498829536","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16449"},{"text":"%md\n## 9.1 Join Types\n\nNow let us explain the last two observations. These are due to the used join type, which was a so called *inner* join. In this case, only records with information from both DataFrames are included in the result.\n\nIn addition to the *inner* join, Spark also supports some additional joins:\n* *outer join* will contain records for all elements from both DataFrames. If either the left or right DataFrames doesn't contain any information, the result will contain `None` values (= `NULL` values) for the corresponding columns.\n* In a *right join*, the second DataFrame (the right DataFrame) as specified as an argument is the leading element. The result will contain records for every record in that DataFrame.\n* In a *left join*, the first DataFrame (the left DataFrame) as specified as the object iteself is the leading element. The result will contain records for every record in that DataFrame.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>9.1 Join Types</h2>\n<p>Now let us explain the last two observations. These are due to the used join type, which was a so called <em>inner</em> join. In this case, only records with information from both DataFrames are included in the result.</p>\n<p>In addition to the <em>inner</em> join, Spark also supports some additional joins:<br/>* <em>outer join</em> will contain records for all elements from both DataFrames. If either the left or right DataFrames doesn&rsquo;t contain any information, the result will contain <code>None</code> values (= <code>NULL</code> values) for the corresponding columns.<br/>* In a <em>right join</em>, the second DataFrame (the right DataFrame) as specified as an argument is the leading element. The result will contain records for every record in that DataFrame.<br/>* In a <em>left join</em>, the first DataFrame (the left DataFrame) as specified as the object iteself is the leading element. The result will contain records for every record in that DataFrame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509021_-549915345","id":"20181110-130217_1135307606","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16450"},{"text":"val result = persons.join(addresses, persons(\"name\") === addresses(\"name\"), \"left\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509023_-549145847","id":"20181110-130506_2117004565","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16451"},{"text":"val result = persons.join(addresses, persons(\"name\") === addresses(\"name\"), \"right\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509025_-563766305","id":"20181110-130520_884492335","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16452"},{"text":"val result = persons.join(addresses, persons(\"name\") === addresses(\"name\"), \"outer\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509028_-564920552","id":"20181110-130519_1919758235","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16453"},{"text":"%md\n## 9.2 Merging Columns\nIn many case you want to merge at least the join columns from both DataFrames (left and right), especially if you perform an outer join. This can be achieved by a `coalesce` operation as follows:","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>9.2 Merging Columns</h2>\n<p>In many case you want to merge at least the join columns from both DataFrames (left and right), especially if you perform an outer join. This can be achieved by a <code>coalesce</code> operation as follows:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509030_-564151054","id":"20181110-130302_1737266170","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16454"},{"text":"val df = persons.join(addresses, persons(\"name\") === addresses(\"name\"), \"outer\")\n    .withColumn(\"mergedName\", coalesce(persons(\"name\"), addresses(\"name\")))\nz.show(df)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509032_-566459547","id":"20180326-164834_776683218","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16455"},{"text":"%md\nSpark also supports to explicitly specify the join columns, in this case the result will conatin the merged column.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Spark also supports to explicitly specify the join columns, in this case the result will conatin the merged column.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509033_-566844296","id":"20181110-130423_452599254","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16456"},{"text":"val result = persons.join(addresses, Seq(\"name\"), \"fullOuter\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509035_-566074798","id":"20180704-081305_488498162","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16457"},{"text":"%md\n## 9.2 Exercise\n\nAs an exercise, we use another DataFrame loaded from a file called lastnames.json, which can be joined to the persons DataFrame again:","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>9.2 Exercise</h2>\n<p>As an exercise, we use another DataFrame loaded from a file called lastnames.json, which can be joined to the persons DataFrame again:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509036_-567998543","id":"20181110-130601_961383739","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16458"},{"text":"val lastnames = spark.read.json(\"s3://dimajix-training/data/lastnames.json\")\nz.show(lastnames)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509037_-568383292","id":"20181110-130600_286064374","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16459"},{"text":"%md\nNow join the lastnames DataFrame to the persons DataFrame whenever the name column of both DataFrames matches. Note what happens due to the fact that we have two last names for \"Bob\".","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now join the lastnames DataFrame to the persons DataFrame whenever the name column of both DataFrames matches. Note what happens due to the fact that we have two last names for &ldquo;Bob&rdquo;.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509038_-567229045","id":"20181110-130558_96939573","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16460"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509038_-567229045","id":"20181110-131635_596129894","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16461"},{"text":"%md\n# 10 Set Operations\n\nSpark also offers some set operations, like `UNION`, `INTERSECT` and `SUBTRACT`.\n\nFirst let us create two simple data frames for experiments.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>10 Set Operations</h1>\n<p>Spark also offers some set operations, like <code>UNION</code>, <code>INTERSECT</code> and <code>SUBTRACT</code>.</p>\n<p>First let us create two simple data frames for experiments.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509039_-567613794","id":"20181110-131643_1427921410","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16462"},{"text":"val schema = StructType(\n    StructField(\"name\", StringType, true) ::\n    StructField(\"age\", IntegerType, true) ::\n    Nil\n)\nval df1 = spark.createDataFrame(\n        sc.parallelize(Array(\n            Row(\"Alice\", 23),\n            Row(\"Bob\", 44),\n            Row(\"Charlie\", 31)\n        )\n    ), schema)\n\nval df2 = spark.createDataFrame(\n    sc.parallelize(Array(\n            Row(\"Alice\", 23),\n            Row(\"Bob\", 44),\n            Row(\"Henry\", 31)\n        )\n    ), schema)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,IntegerType,true))\n\ndf1: org.apache.spark.sql.DataFrame = [name: string, age: int]\n\ndf2: org.apache.spark.sql.DataFrame = [name: string, age: int]\n"}]},"apps":[],"jobName":"paragraph_1542726509040_-557225574","id":"20181110-131724_375306800","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16463"},{"text":"%md\n## 10.1 Unions\n\nThe most well known operation is a `union` which actually corresponds to an SQL `UNION ALL`, i.e. it will keep duplicate records.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>10.1 Unions</h2>\n<p>The most well known operation is a <code>union</code> which actually corresponds to an SQL <code>UNION ALL</code>, i.e. it will keep duplicate records.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509041_-557610323","id":"20181110-132209_786785229","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16464"},{"text":"val result = df1.union(df2)\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\nAlice\t23\nBob\t44\nCharlie\t31\nAlice\t23\nBob\t44\nHenry\t31\n"}]},"apps":[],"jobName":"paragraph_1542726509042_-556456076","id":"20181110-132228_780405285","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16465"},{"text":"%md\nWhen you do not want to keep duplicate records, you can simply run a `distinct()` transformation after the `union()`.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>When you do not want to keep duplicate records, you can simply run a <code>distinct()</code> transformation after the <code>union()</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509043_-556840825","id":"20181110-132253_488040219","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16466"},{"text":"val result = df1.union(df2).distinct()\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\nBob\t44\nCharlie\t31\nAlice\t23\nHenry\t31\n"}]},"apps":[],"jobName":"paragraph_1542726509044_-558764569","id":"20181110-132316_310412083","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16467"},{"text":"%md\n### Union and UnionByName\n\nA simple `union` operation simply takes the schema of the first DataFrame and appends the records of the second data frame. The columns will be matched by their position and types will be changed if required.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Union and UnionByName</h3>\n<p>A simple <code>union</code> operation simply takes the schema of the first DataFrame and appends the records of the second data frame. The columns will be matched by their position and types will be changed if required.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509045_-559149318","id":"20181110-132337_1532440560","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16468"},{"text":"val schema = StructType(\n    StructField(\"age\", IntegerType, true) ::\n    StructField(\"name\", StringType, true) ::\n    Nil\n)\nval df3 = spark.createDataFrame(\n        sc.parallelize(Array(\n            Row(23, \"Alice\"),\n            Row(44, \"Bob\"),\n            Row(31, \"Henry\")\n        )\n    ), schema)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(age,IntegerType,true), StructField(name,StringType,true))\n\ndf3: org.apache.spark.sql.DataFrame = [age: int, name: string]\n"}]},"apps":[],"jobName":"paragraph_1542726509046_-557995072","id":"20181110-132354_1756446274","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16469"},{"text":"val result = df1.union(df3).distinct()\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\n44\tBob\n31\tHenry\n23\tAlice\nAlice\t23\nBob\t44\nCharlie\t31\n"}]},"apps":[],"jobName":"paragraph_1542726509046_-557995072","id":"20181110-132441_664843564","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16470"},{"text":"result.printSchema()","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- name: string (nullable = true)\n |-- age: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1542726509047_-558379820","id":"20181110-132503_1757441759","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16471"},{"text":"val result = df1.unionByName(df3)\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\n\n\n<console>:37: error: value unionByName is not a member of org.apache.spark.sql.DataFrame\n       val result = df1.unionByName(df3)\n                        ^\n"}]},"apps":[],"jobName":"paragraph_1542726509048_-560303565","id":"20181110-132512_1690629187","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16472"},{"text":"%md\n## 10.2 Intersect and Subtract\n\nSpark also supports additional set operations like `INTERSECT` and `SUBTRACT`","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>10.2 Intersect and Subtract</h2>\n<p>Spark also supports additional set operations like <code>INTERSECT</code> and <code>SUBTRACT</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509048_-560303565","id":"20181110-132708_289858927","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16473"},{"text":"val result = df1.intersect(df2)\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"name\tage\nBob\t44\nAlice\t23\n"}]},"apps":[],"jobName":"paragraph_1542726509049_-560688314","id":"20181110-132735_1696815182","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16474"},{"text":"val result = df1.subtract(df2)\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\n\n\n<console>:37: error: value subtract is not a member of org.apache.spark.sql.DataFrame\n       val result = df1.subtract(df2)\n                        ^\n"}]},"apps":[],"jobName":"paragraph_1542726509050_-559534067","id":"20181110-132754_959893754","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16475"},{"text":"%md\n# 11 Caching Data\n\nIn some situations, you may want to persist intermediate results. For example iterative algorithms may benefit from *caching* intermediate results, if the same DataFrame is transformed again and again. Spark provides some capabilities to persist intermediate results using the methods `cache()` or `persist(storageLevel)`.\n\nNote that also caching is lazy, which means that records will not be created at the time when you call `cache()` or `persist()` but at the first time when the DataFrame is evaluated. This could be even a simple `count()` action.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>11 Caching Data</h1>\n<p>In some situations, you may want to persist intermediate results. For example iterative algorithms may benefit from <em>caching</em> intermediate results, if the same DataFrame is transformed again and again. Spark provides some capabilities to persist intermediate results using the methods <code>cache()</code> or <code>persist(storageLevel)</code>.</p>\n<p>Note that also caching is lazy, which means that records will not be created at the time when you call <code>cache()</code> or <code>persist()</code> but at the first time when the DataFrame is evaluated. This could be even a simple <code>count()</code> action.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509051_-559918816","id":"20181110-132816_1433009653","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16476"},{"text":"persons.cache()","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509052_-561842561","id":"20181110-132838_387028558","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16477"},{"text":"persons.storageLevel","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509053_-562227309","id":"20181110-132845_44080556","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16478"},{"text":"%md\nIn order to free up some memory again, the DataFrame can also be uncached. This is done with the `unpersist` method.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In order to free up some memory again, the DataFrame can also be uncached. This is done with the <code>unpersist</code> method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509054_-561073063","id":"20181110-133242_1336355338","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16479"},{"text":"persons.unpersist()","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509054_-561073063","id":"20181110-132853_1479769839","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16480"},{"text":"persons.storageLevel","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509055_-561457812","id":"20181110-132900_1429231266","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16481"},{"text":"%md\nIn addition to the simple `cache` method, Spark also provides a `persist` method, which lets you define precisely how the data should be cached: In memory, on disk or with a mixture of both storages.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In addition to the simple <code>cache</code> method, Spark also provides a <code>persist</code> method, which lets you define precisely how the data should be cached: In memory, on disk or with a mixture of both storages.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509056_-871180676","id":"20181110-133333_402374254","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16482"},{"text":"import org.apache.spark.storage.StorageLevel\n\npersons.persist(StorageLevel.MEMORY_AND_DISK)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509057_-871565425","id":"20181110-132909_1698293666","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16483"},{"text":"%md\n# 12 Interacting with SQL\n\nSpark DataFrame supports all operations equivalent to SQL. You can even explicitly write some SQL code that accesses one or multiple DataFrames. But first you have to register the DataFrame as a temporary named view to make it accessible from SQL. After that you can use the `spark.sql` method to perform SQL transformations.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>12 Interacting with SQL</h1>\n<p>Spark DataFrame supports all operations equivalent to SQL. You can even explicitly write some SQL code that accesses one or multiple DataFrames. But first you have to register the DataFrame as a temporary named view to make it accessible from SQL. After that you can use the <code>spark.sql</code> method to perform SQL transformations.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509058_-870411178","id":"20160617-161812_1127220850","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16484"},{"text":"persons.createOrReplaceTempView(\"persons\")\n\nval result = spark.sql(\"SELECT * FROM persons\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509059_-870795927","id":"20160617-161843_1855518036","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16485"},{"text":"%md\nIn Zeppelin you can also enter SQL directly using the `%sql` call type","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In Zeppelin you can also enter SQL directly using the <code>%sql</code> call type</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509059_-870795927","id":"20181110-133008_739496768","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16486"},{"text":"%sql\nSELECT * FROM persons","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509060_-872719672","id":"20181110-133000_1925287802","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16487"},{"text":"%md\n## 12.1 Exercise\n\nPerform the following tasks, in order to join `persons` with `addresses`in SQL:\n\n* Register `addresses` DataFrame as `addresses`\n* Join `persons` with `addresses`\n* Only select persons which are 20 years or older","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>12.1 Exercise</h2>\n<p>Perform the following tasks, in order to join <code>persons</code> with <code>addresses</code>in SQL:</p>\n<ul>\n  <li>Register <code>addresses</code> DataFrame as <code>addresses</code></li>\n  <li>Join <code>persons</code> with <code>addresses</code></li>\n  <li>Only select persons which are 20 years or older</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509061_-873104421","id":"20181110-132942_1177365713","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16488"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509062_-871950174","id":"20180704-081953_1986852769","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16489"},{"text":"%md\n# 13 User Defined Functions\n\nOne very powerful capability of Spark are user defined functions (UDFs). This allows you to create new SQL functions in Scala and use them either with programmatic SQL or directly within SQL. \n\nLet us create an example using a special function that converts a string to a lower/upper case mixture. This is not available in SQL or Spark and would be very hard to implement with plain SQL functions. We register this function and make it available in SQL vi athe name \"funny\" and we also store the UDF in a Scala value to use it in programmatic SQL.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>13 User Defined Functions</h1>\n<p>One very powerful capability of Spark are user defined functions (UDFs). This allows you to create new SQL functions in Scala and use them either with programmatic SQL or directly within SQL. </p>\n<p>Let us create an example using a special function that converts a string to a lower/upper case mixture. This is not available in SQL or Spark and would be very hard to implement with plain SQL functions. We register this function and make it available in SQL vi athe name &ldquo;funny&rdquo; and we also store the UDF in a Scala value to use it in programmatic SQL.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509063_-872334923","id":"20160612-155902_920718644","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16490"},{"text":"def fn(s:String) = s.zipWithIndex.map(kv => if (kv._2 % 2 == 1) kv._1.toLower else kv._1.toUpper).mkString(\"\")\n\nfn(\"This is a test\")","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nfn: (s: String)String\n\nres20: String = ThIs iS A TeSt\n"}]},"apps":[],"jobName":"paragraph_1542726509064_-874258667","id":"20181110-133116_750983601","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16491"},{"text":"val funny = spark.udf.register(\"funny\", fn _)\n\nval result = persons.select(funny('name) as \"funny_name\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\nfunny: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\n\n\n\n<console>:33: error: not found: value persons\n       val result = persons.select(funny('name) as \"funny_name\")\n                    ^\n"}]},"apps":[],"jobName":"paragraph_1542726509065_-874643416","id":"20160612-155931_1503875483","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16492"},{"text":"val result = spark.sql(\"select funny(name) as funny_name from persons\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509066_-873489170","id":"20160617-161940_1666377443","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16493"},{"text":"%md\n# 14 Writing Data\n\nFinally let us have a look at input/output operations again. We already saw how to read in data from S3/HDFS using DataFrameReaders returned by `spark.read`:","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>14 Writing Data</h1>\n<p>Finally let us have a look at input/output operations again. We already saw how to read in data from S3/HDFS using DataFrameReaders returned by <code>spark.read</code>:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509067_-873873918","id":"20160612-160043_427568179","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16494"},{"text":"val df = spark.read.text(\"s3://dimajix-training/data/alice\")\n\nz.show(df)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":194,"optionOpen":false,"keys":[{"name":"value","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"value","index":0,"aggr":"sum"}}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509068_-875797663","id":"20160612-160702_744208944","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16495"},{"text":"%md\nWith a similar syntax, we can also write a DataFrame to HDFS (and also S3 given appropriate permissions) again.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>With a similar syntax, we can also write a DataFrame to HDFS (and also S3 given appropriate permissions) again.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509069_-876182412","id":"20180326-164547_119048270","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16496"},{"text":"val result = persons.select(concat('name, lit(\",\"), 'age))\n\nresult.write.mode(\"overwrite\").text(\"/tmp/persons\")","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509070_-875028165","id":"20160612-160808_277383298","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16497"},{"text":"%sh\nhdfs dfs -getmerge /tmp/persons /tmp/persons\ncat /tmp/persons","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509071_-875412914","id":"20160612-160938_1466157130","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16498"},{"text":"%md\nYou can also chose to write using a different format, for example **csv**, **parquet** or **orc**:","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>You can also chose to write using a different format, for example <strong>csv</strong>, <strong>parquet</strong> or <strong>orc</strong>:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509072_-865024694","id":"20180326-164630_18953108","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16499"},{"text":"result.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/persons_csv\")\n","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509073_-865409443","id":"20180326-164712_1812551011","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16500"},{"text":"result.write.mode(\"overwrite\").csv(\"/tmp/persons_csv\")","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509073_-865409443","id":"20180326-164739_1475137011","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16501"},{"text":"%md\n# 15 WordCount Revisited\n\nAlthough the famous \"word count\" exampole doesn't fit really well to the DataFrame world, it is still viable.","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>15 WordCount Revisited</h1>\n<p>Although the famous &ldquo;word count&rdquo; exampole doesn&rsquo;t fit really well to the DataFrame world, it is still viable.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509075_-864639945","id":"20160612-161131_370663008","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16502"},{"text":"val text = spark.read.text(\"s3://dimajix-training/data/alice\")\nval words = text.select(explode(split('value,\" \")).alias(\"word\")).filter('word !== lit(\"\"))\nval counts = words.groupBy($\"word\").count().sort($\"count\".desc)\n\nz.show(counts.limit(20))","dateUpdated":"2018-11-20T15:08:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"}}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509076_-866563689","id":"20160612-161147_839240215","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16503"},{"text":"%md\n# 16 Accessing Hive Tables\n\nPySpark supports accessing data in Hive tables. This enables to use Hive as a central database which takes the burden of specyfing the schema for a file over and over again.\n\nFirst let's retreieve the catalog containing all tables of a specific Hive database:","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>16 Accessing Hive Tables</h1>\n<p>PySpark supports accessing data in Hive tables. This enables to use Hive as a central database which takes the burden of specyfing the schema for a file over and over again.</p>\n<p>First let&rsquo;s retreieve the catalog containing all tables of a specific Hive database:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509078_-865794192","id":"20160617-162253_1296143013","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16504"},{"text":"val tables = spark.catalog.listTables(dbName=\"training\")\nz.show(tables)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\n\n\n\norg.apache.spark.sql.AnalysisException: Database 'training' does not exist.;\n  at org.apache.spark.sql.internal.CatalogImpl.requireDatabaseExists(CatalogImpl.scala:43)\n  at org.apache.spark.sql.internal.CatalogImpl.listTables(CatalogImpl.scala:95)\n  ... 48 elided\n"}]},"apps":[],"jobName":"paragraph_1542726509080_-868102685","id":"20181110-133527_113907095","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16505"},{"text":"%md\nNow let's read in one table:","dateUpdated":"2018-11-20T15:08:29+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now let&rsquo;s read in one table:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1542726509081_-868487434","id":"20181110-133545_1074976121","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16506"},{"text":"val result = spark.read.table(\"training.stations\")\n\nz.show(result)","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509082_-867333187","id":"20181110-133618_662069055","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16507"},{"text":"","dateUpdated":"2018-11-20T15:08:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1542726509083_-867717936","id":"20181110-133648_1559837685","dateCreated":"2018-11-20T15:08:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16508"}],"name":"Spark DataFrame - Full","id":"2DWYGTF72","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}