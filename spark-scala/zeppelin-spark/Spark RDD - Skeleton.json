{"paragraphs":[{"text":"%md\n# 1 Creating RDDs\n\nAs the first step we always need some way to generate an RDD. In the following examples, we discuss some methods for creatign RDDs from local Scala collections. This is not the normal use case, but can come in handy not only in trainings.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>1 Creating RDDs</h1>\n<p>As the first step we always need some way to generate an RDD. In the following examples, we discuss some methods for creatign RDDs from local Scala collections. This is not the normal use case, but can come in handy not only in trainings.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703912_-1682588617","id":"20160608-202711_364435625","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1098"},{"text":"%md\n## 1.1 Creating RDDs from Scala Collections\n\nThe simplest thing is to create a Spark RDD from a Scala collection. Spark can store any object in an RDD as long as it can be serialized. The simplest example uses numbers","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.1 Creating RDDs from Scala Collections</h2>\n<p>The simplest thing is to create a Spark RDD from a Scala collection. Spark can store any object in an RDD as long as it can be serialized. The simplest example uses numbers</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703918_-1683358115","id":"20181110-134942_669022169","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1099"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703922_-1672585146","id":"20160608-202711_1542689802","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1100"},{"text":"%md\nOf course we can also use strings as objects","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Of course we can also use strings as objects</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703925_-1675278388","id":"20181110-135040_263615688","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1101"},{"text":"val rdd = // YOUR CODE HERE\nrdd.collect().foreach(println)","dateUpdated":"2018-12-18T13:28:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703930_-1675663137","id":"20181110-135124_2112920349","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1102"},{"text":"%md\n### Creating RDDs from Iterables\nIt is even possible to create an RDD from an Scala iterable object, like a range.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Iterables</h3>\n<p>It is even possible to create an RDD from an Scala iterable object, like a range.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703932_-1677971630","id":"20181110-135105_2137348480","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1103"},{"text":"val rdd = // YOUR CODE HERE\nrdd.collect().foreach(println)","dateUpdated":"2018-12-18T13:29:07+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703935_-1677586881","id":"20160608-202711_1417431983","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1104"},{"text":"%md\n### Creating a range RDD\nSpark also directly supports creating a range RDD as follows","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating a range RDD</h3>\n<p>Spark also directly supports creating a range RDD as follows</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703939_-1592942123","id":"20181110-135152_1785240713","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1105"},{"text":"val rdd = sc.range(1,31,3)\nrdd.collect().foreach(println)","dateUpdated":"2018-12-18T13:29:38+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703943_-1594481119","id":"20160617-161025_1250626577","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1106"},{"text":"%md\n## 1.1 Loading Data from S3/HDFS\n\nOf course normally you'd want to read data from some storage (HDFS or S3 for example). Spark supports various very simple file formats (text files, Hadoop Sequence files), but not structured formats like CSV or parquet. Tabular formats are supported in the Spark DataFrame API, but the RDD API can only handle simple serialization formats.\n\nWe will use a text file as an example.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.1 Loading Data from S3/HDFS</h2>\n<p>Of course normally you&rsquo;d want to read data from some storage (HDFS or S3 for example). Spark supports various very simple file formats (text files, Hadoop Sequence files), but not structured formats like CSV or parquet. Tabular formats are supported in the Spark DataFrame API, but the RDD API can only handle simple serialization formats.</p>\n<p>We will use a text file as an example.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703946_-1595635366","id":"20160608-202711_1905251818","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1107"},{"text":"val alice = // YOUR CODE HERE\n\nalice.take(10).foreach(println)","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703950_-1597174361","id":"20160608-202711_438644709","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1108"},{"text":"%md \n## 1.2 Retrieving Data\n\nNow that we have an RDD (which is distributed in the cluster), we also want to have a method for retrieving all records back to the local machine where the Spark driver runs. This can be done via the `collect()` method.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1.2 Retrieving Data</h2>\n<p>Now that we have an RDD (which is distributed in the cluster), we also want to have a method for retrieving all records back to the local machine where the Spark driver runs. This can be done via the <code>collect()</code> method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703954_-1586401392","id":"20160608-202711_2137747659","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1109"},{"text":"val numbers = sc.parallelize(1 to 100)\nval localData = // YOUR CODE HERE\n","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703957_-1589094634","id":"20160608-202711_1723490503","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1110"},{"text":"%md\n### First record\n\nSince an RDD could contain a huge amount of data, using `collect()` is highly discouraged except in situations where you really know that the number of records is limited. But nevertheless it is quire useful (specifically in interactive environments like Zeppelin) to peek inside an RDD.\n\nUsing the `first()` method allows us to retrieve the very first record.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>First record</h3>\n<p>Since an RDD could contain a huge amount of data, using <code>collect()</code> is highly discouraged except in situations where you really know that the number of records is limited. But nevertheless it is quire useful (specifically in interactive environments like Zeppelin) to peek inside an RDD.</p>\n<p>Using the <code>first()</code> method allows us to retrieve the very first record.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703960_-1590248881","id":"20181110-135548_1848397879","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1111"},{"text":"val localData = // YOUR CODE HERE\n\nprint(localData)","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703963_-1589864132","id":"20160608-202711_1445453185","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1112"},{"text":"%md\n### First n records\n\nUsing the `take(n)` method you can also retrieve the first `n` records from an RDD.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>First n records</h3>\n<p>Using the <code>take(n)</code> method you can also retrieve the first <code>n</code> records from an RDD.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703965_-1592172625","id":"20181110-135730_1377630540","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1113"},{"text":"val localData = // YOUR CODE HERE\n\nlocalData.foreach(println)","dateUpdated":"2018-12-18T13:29:52+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703967_-1591403128","id":"20160608-202711_1396665431","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1114"},{"text":"%md\n### Sampling n records\nIn addition to taking top `n` records, Spark also supports *sampling* records, which will retrieve a (hopefully) representative subset of the data.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Sampling n records</h3>\n<p>In addition to taking top <code>n</code> records, Spark also supports <em>sampling</em> records, which will retrieve a (hopefully) representative subset of the data.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703969_-1606023586","id":"20181110-135923_975091599","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1115"},{"text":"val localData = // YOUR CODE HERE\n\nlocalData.foreach(println)","dateUpdated":"2018-12-18T13:29:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703970_-1604869339","id":"20160612-120518_1545543070","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1116"},{"text":"%md\n# 2 Simple Transformations\n\nSpark RDDs try to closely mimic the Scala collection API (where it makes sense). The simplest type of transformation of a Scala collection is a `map` operation. A Spark RDD also provides exactly that operation.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>2 Simple Transformations</h1>\n<p>Spark RDDs try to closely mimic the Scala collection API (where it makes sense). The simplest type of transformation of a Scala collection is a <code>map</code> operation. A Spark RDD also provides exactly that operation.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703971_-1605254088","id":"20160608-202711_1759637107","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1117"},{"text":"val result = // YOUR CODE HERE\n\nresult.foreach(println)","dateUpdated":"2018-12-18T13:30:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703972_-1607177833","id":"20160608-202711_1223554512","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1118"},{"text":"val result = // YOUR CODE HERE\nval words = // YOUR CODE HERE\n\nwords.foreach(x => println(x.mkString(\",\")))","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703973_-1607562581","id":"20160608-202711_2124763159","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1119"},{"text":"%md\n### `flatMap` transformations\n\nOf course Spark also provides a `flatMap` operation, which will flatten results of the provided function.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3><code>flatMap</code> transformations</h3>\n<p>Of course Spark also provides a <code>flatMap</code> operation, which will flatten results of the provided function.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703974_-1606408335","id":"20181110-140258_1042389775","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1120"},{"text":"val result = // YOUR CODE HERE\nval words = result.take(20)\n\nwords.foreach(println)","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703975_-1606793084","id":"20160608-202711_388847350","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1121"},{"text":"%md\n# 3 Filtering Data\n\nThe next complex operation is filtering. In Spark that works in the same way as in Scala.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>3 Filtering Data</h1>\n<p>The next complex operation is filtering. In Spark that works in the same way as in Scala.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703977_-1609101577","id":"20160608-202711_739446510","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1122"},{"text":"val result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703977_-1609101577","id":"20160608-202711_389628381","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1123"},{"text":"%md\n# 4 Aggregations\n\nSo far we have looked at simple transformations, where each record is transformed independently. The next level of complexity is *global aggregations*. The simplest aggregation is the number of records. But Spark also supports some more common aggregations out of the box.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>4 Aggregations</h1>\n<p>So far we have looked at simple transformations, where each record is transformed independently. The next level of complexity is <em>global aggregations</em>. The simplest aggregation is the number of records. But Spark also supports some more common aggregations out of the box.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703978_-1607947330","id":"20160608-202711_806559764","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1124"},{"text":"println(s\"Count=${numbers.count()}\")\nprintln(s\"Sum=${numbers.sum()}\")\nprintln(s\"Mean=${numbers.mean()}\")\nprintln(s\"Min=${numbers.min()}\")\nprintln(s\"Max=${numbers.max()}\")\nprintln(s\"Variance=${numbers.variance()}\")\nprintln(s\"Stddev=${numbers.stdev()}\")\nprintln(s\"SampleVariance=${numbers.sampleVariance()}\")\nprintln(s\"SampleStddev=${numbers.sampleStdev()}\")\n","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703979_-1608332079","id":"20160608-202711_933173684","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1125"},{"text":"%md\n## 4.1 Simple statistics\nFor numeric RDDs Spark also provides a method `stats()` which provides some basic statistics like minimum, maximum average etc","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>4.1 Simple statistics</h2>\n<p>For numeric RDDs Spark also provides a method <code>stats()</code> which provides some basic statistics like minimum, maximum average etc</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703980_-1610255824","id":"20181110-143418_1234273588","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1126"},{"text":"val result = // YOUR CODE HERE\n\nprintln(result)","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703981_-1610640573","id":"20160608-202711_777032947","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1127"},{"text":"%md\n## 4.2 Aggregation via `reduce`\n\nIn many real world cases the built in aggregations are not powerful enough. But as with Scala, Spark also supports generic aggregations via a `reduce` method.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>4.2 Aggregation via <code>reduce</code></h2>\n<p>In many real world cases the built in aggregations are not powerful enough. But as with Scala, Spark also supports generic aggregations via a <code>reduce</code> method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703982_-1609486326","id":"20181110-140615_1089818781","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1128"},{"text":"val result = // YOUR CODE HERE\n\nprintln(result)","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703983_-1609871075","id":"20160608-202711_744682822","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1129"},{"text":"%md\n## 4.3 Aggregation via `aggregate`\n\nSince `reduce` has many limitations, Spark provides a more generic and powerful aggregation method called `aggregate`. This generic method requires three parameters:\n* starting value\n* partial aggregation function\n* final aggregation function\n\nLet us have a look at an example for calculating the average value. Conceptionally this can be achieved by counting the number of records and summing up all values. We could do that with two `reduce` invocations, but these functions are expensive, so we seek some way to perform both aggregations at the same time. This can be done with the `aggregate` method and appropriate aggregation state functions.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>4.3 Aggregation via <code>aggregate</code></h2>\n<p>Since <code>reduce</code> has many limitations, Spark provides a more generic and powerful aggregation method called <code>aggregate</code>. This generic method requires three parameters:<br/>* starting value<br/>* partial aggregation function<br/>* final aggregation function</p>\n<p>Let us have a look at an example for calculating the average value. Conceptionally this can be achieved by counting the number of records and summing up all values. We could do that with two <code>reduce</code> invocations, but these functions are expensive, so we seek some way to perform both aggregations at the same time. This can be done with the <code>aggregate</code> method and appropriate aggregation state functions.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703984_-1599482855","id":"20181110-140654_191471293","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1130"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703985_-1599867603","id":"20160608-202711_349494638","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1131"},{"text":"%md\n## 4.4 Non-Trivial Aggeragtion for Counting Words\n\nAnother example uses a Scala map as aggregation state and performs a word count.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>4.4 Non-Trivial Aggeragtion for Counting Words</h2>\n<p>Another example uses a Scala map as aggregation state and performs a word count.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703986_-1598713357","id":"20180619-075858_1165292118","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1132"},{"text":"val words = alice.flatMap(line => line.split(\" \"))\nwords.take(10).foreach(println)","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703987_-1599098106","id":"20180619-075912_1258205514","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1133"},{"text":"val zero = Map[String,Int]()\ndef reduce(map:Map[String,Int], word:String) : Map[String,Int] = map.updated(word, map.getOrElse(word, 0) + 1)\ndef combine(left:Map[String,Int], right:Map[String,Int]) : Map[String,Int] = {\n    val words = left.keySet ++ right.keySet\n    words.map(word => (word,left.getOrElse(word, 0) + right.getOrElse(word, 0))).toMap\n}\n\nval counts = words.aggregate(zero)(reduce, combine)","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703988_-1601021850","id":"20180619-075923_250735613","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1134"},{"text":"%md\n# 5 Making Data Distinct\n\nSpark provides also a very simple method for making records distinct. This can be useful after a `union` operation.","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>5 Making Data Distinct</h1>\n<p>Spark provides also a very simple method for making records distinct. This can be useful after a <code>union</code> operation.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703990_-1600252352","id":"20160608-202711_588606321","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1135"},{"text":"val rdd = sc.parallelize(List('b','a','c','c','a'))\nval result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:30:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703991_-1600637101","id":"20160608-202711_1396758965","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1136"},{"text":"%md\n# 6 Intermission: Calculating Pi\n\nLet us use Spark to perform a randomized approximation of the constant pi. The idea is as follows:\n\n1. Generate `n` random points in the square `[0,1]x[0,1]`\n2. For each point, check if it is inside the circle with radius 0 around the center point `(0,0)`. This is the case for all points `(x,y)` with `x*x + y*y <= 1`.\n3. Count the number of these points inside the unit circle. We denote the number with `m`\n4. Pi can be approximated via `pi = 4*m/n`","dateUpdated":"2018-12-18T13:28:23+0000","config":{"tableHide":false,"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>6 Intermission: Calculating Pi</h1>\n<p>Let us use Spark to perform a randomized approximation of the constant pi. The idea is as follows:</p>\n<ol>\n  <li>Generate <code>n</code> random points in the square <code>[0,1]x[0,1]</code></li>\n  <li>For each point, check if it is inside the circle with radius 0 around the center point <code>(0,0)</code>. This is the case for all points <code>(x,y)</code> with <code>x*x + y*y &lt;= 1</code>.</li>\n  <li>Count the number of these points inside the unit circle. We denote the number with <code>m</code></li>\n  <li>Pi can be approximated via <code>pi = 4*m/n</code></li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703992_-1602560846","id":"20170227-233340_1388824956","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1137"},{"text":"val numbers = ...\nval points = ... // use scla.util.Random.nextFloat()\nval insidePoints = ... // x*x + y*y <= 1.0\nval numberOfInsidePoints = ...\n\nval pi = 4 * numberOfInsidePoints / totalNumberOfPoints","dateUpdated":"2018-12-18T13:28:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139703993_-1602945595","id":"20180619-080537_306482722","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1138"},{"text":"%md\n# 7 Working with Key-Value Data\n\nSo far we have been looking at either simple transformations or at global aggregations. But most more powerful algorithms need grouped aggregations and/or joins. Both topics require using *key-value* pairs instead of simple values. Spark provides lots of additional functionality for *pairs* of values (2-tuples), which are interpreted as key-values pairs.\n\nSo let us first generate some key value pairs, which are represented as Scala-2-tuples (pairs):","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>7 Working with Key-Value Data</h1>\n<p>So far we have been looking at either simple transformations or at global aggregations. But most more powerful algorithms need grouped aggregations and/or joins. Both topics require using <em>key-value</em> pairs instead of simple values. Spark provides lots of additional functionality for <em>pairs</em> of values (2-tuples), which are interpreted as key-values pairs.</p>\n<p>So let us first generate some key value pairs, which are represented as Scala-2-tuples (pairs):</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139703994_-1601791348","id":"20160608-202711_1184179398","dateCreated":"2018-12-18T13:28:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1139"},{"text":"val kv = // YOUR CODE HERE\nval keys = // YOUR CODE HERE\n\nkeys.foreach(k => /* YOUR CODE HERE */)\n","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704000_-1617950802","id":"20160608-202711_1772185437","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1140"},{"text":"%md\n## 7.1 Transforming values\n\nOften keys are constant and only the values need to be transformed. This kind of operations is diectly supported in Spark via a dedicated `mapValues` method. Of course you *could* use a `map` method instead, but it may well be the case that you have to pay significantly more, because Spark has no idea how keys would be transformed. And keys may play a special role when you use grouped aggregations and joins (more on these two operations later).","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>7.1 Transforming values</h2>\n<p>Often keys are constant and only the values need to be transformed. This kind of operations is diectly supported in Spark via a dedicated <code>mapValues</code> method. Of course you <em>could</em> use a <code>map</code> method instead, but it may well be the case that you have to pay significantly more, because Spark has no idea how keys would be transformed. And keys may play a special role when you use grouped aggregations and joins (more on these two operations later).</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704002_-1617181304","id":"20181110-162842_1173163535","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1141"},{"text":"val kv = sc.parallelize(Array((\"a\" -> 1), (\"b\" -> 3), (\"c\" -> 17), (\"b\" -> 23), (\"c\" -> 12)))\nval result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:31:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704003_-1617566053","id":"20160608-202711_1669939416","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1142"},{"text":"%md\n# 8 Grouping Data\nWe have just seen how to create key-value data, now we want to investigate the first operation: grouping data. The idea is like a SQL `GROUP BY` operation, but without aggregation. A simple Spark `groupBy` or `groupByKey` simply collects all records having the same key into a list of records.\n\nThe first operation we'll use is `groupByKey` which assumes key-value data.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>8 Grouping Data</h1>\n<p>We have just seen how to create key-value data, now we want to investigate the first operation: grouping data. The idea is like a SQL <code>GROUP BY</code> operation, but without aggregation. A simple Spark <code>groupBy</code> or <code>groupByKey</code> simply collects all records having the same key into a list of records.</p>\n<p>The first operation we&rsquo;ll use is <code>groupByKey</code> which assumes key-value data.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704005_-1619874546","id":"20160608-202711_544797547","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1143"},{"text":"val result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:31:49+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704006_-1618720300","id":"20160608-202711_1689585268","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1144"},{"text":"%md\nIn cases where you do not already have key-value data, or you need to perform the grouping on a different key, you can also specify an arbitrary operation in the `groupBy` method for extracting the key.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In cases where you do not already have key-value data, or you need to perform the grouping on a different key, you can also specify an arbitrary operation in the <code>groupBy</code> method for extracting the key.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704007_-1619105048","id":"20181110-163230_109592255","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1145"},{"text":"val result = // YOUR CODE HERE\n\nresult.collect().foreach(x => println(x._1 + \" : \" + x._2.mkString(\",\")))","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704007_-1619105048","id":"20160608-202711_683334995","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1146"},{"text":"%md\n# 9 Aggregating Key-Value Data\n\nNow we have seen how groups of records sharing the same key can be created. The next commonly used operation is an aggregation. You could perform a standard Scala aggregation directly with these groups using a `mapValues` method after the `groupBy` method as follows:","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>9 Aggregating Key-Value Data</h1>\n<p>Now we have seen how groups of records sharing the same key can be created. The next commonly used operation is an aggregation. You could perform a standard Scala aggregation directly with these groups using a <code>mapValues</code> method after the <code>groupBy</code> method as follows:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704008_-1621028793","id":"20160608-202711_1207982069","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1147"},{"text":"val result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704009_-1621413542","id":"20160608-202711_68716809","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1148"},{"text":"%md\n## 9.1 Grouped aggregation via `reduceByKey`\n\nActually the `groupBy` operation is really expensive and memory intensive, because in the first step Spark has to shuffle all the data, such that all records having the same key are in the same list. But if we want to perform an aggregation on the result, it is a better idea to perform the following steps:\n\n1. Perform local partial aggregations per Spark executor\n2. Shuffle partial results\n3. Perform final aggregation of partial results\n \nThis approach reduces the amount of data that need to be shuffled between the worker nodes in the cluster. In order to use this mechanics, we need to use a `reduceByKey` method instead of a `groupBy` followed by a `mapValues`","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>9.1 Grouped aggregation via <code>reduceByKey</code></h2>\n<p>Actually the <code>groupBy</code> operation is really expensive and memory intensive, because in the first step Spark has to shuffle all the data, such that all records having the same key are in the same list. But if we want to perform an aggregation on the result, it is a better idea to perform the following steps:</p>\n<ol>\n  <li>Perform local partial aggregations per Spark executor</li>\n  <li>Shuffle partial results</li>\n  <li>Perform final aggregation of partial results</li>\n</ol>\n<p>This approach reduces the amount of data that need to be shuffled between the worker nodes in the cluster. In order to use this mechanics, we need to use a <code>reduceByKey</code> method instead of a <code>groupBy</code> followed by a <code>mapValues</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704011_-1620644044","id":"20181110-163536_1670726171","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1149"},{"text":"val result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704012_-1622567789","id":"20160608-202711_1724690424","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1150"},{"text":"%md\n## 9.2 Grouped counting\nSpark even supports a special aggregation called `countByKey`, but this does return a local Scala collection and not an RDD","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>9.2 Grouped counting</h2>\n<p>Spark even supports a special aggregation called <code>countByKey</code>, but this does return a local Scala collection and not an RDD</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704014_-1621798291","id":"20181110-163721_511931057","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1151"},{"text":"val result = // YOUR CODE HERE\n\nresult.foreach(println)","dateUpdated":"2018-12-18T13:32:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704015_-1622183040","id":"20160608-202711_244234945","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1152"},{"text":"%md\nSpark also supports a `countByValue` method, where `value` does not refer to the value of a key-value pair, but simply to the full record.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Spark also supports a <code>countByValue</code> method, where <code>value</code> does not refer to the value of a key-value pair, but simply to the full record.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704016_-1611794819","id":"20181110-164151_74670773","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1153"},{"text":"val rdd = sc.parallelize(Array(\"a\", \"b\", \"c\", \"b\", \"c\"))\nval result =  // YOUR CODE HERE\n\nresult.foreach(println)","dateUpdated":"2018-12-18T13:32:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704017_-1612179568","id":"20160608-202711_1015471198","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1154"},{"text":"%md\n## 9.3 Grouped aggregation via `aggregateByKey`\n\nSince `reduceByKey` has many limitations, Spark provides a more generic and powerful aggregation method called `aggregateByKey`. This generic method requires three parameters:\n* starting value\n* partial aggregation function\n* final aggregation function\n\nLet us have a look at an example for calculating the average value per grouping key. Conceptionally this can be achieved by counting the number of records and summing up all values. We could do that with two `reduceByKey` invocations, but these functions are expensive, so we seek some way to perform both aggregations at the same time. This can be done with the `aggregateByKey` method and appropriate aggregation state functions.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>9.3 Grouped aggregation via <code>aggregateByKey</code></h2>\n<p>Since <code>reduceByKey</code> has many limitations, Spark provides a more generic and powerful aggregation method called <code>aggregateByKey</code>. This generic method requires three parameters:<br/>* starting value<br/>* partial aggregation function<br/>* final aggregation function</p>\n<p>Let us have a look at an example for calculating the average value per grouping key. Conceptionally this can be achieved by counting the number of records and summing up all values. We could do that with two <code>reduceByKey</code> invocations, but these functions are expensive, so we seek some way to perform both aggregations at the same time. This can be done with the <code>aggregateByKey</code> method and appropriate aggregation state functions.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704019_-1611410070","id":"20160608-202711_971882077","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1155"},{"text":"val zero = (0.0, 0)\ndef reducer(acc:(Double,Int), value:Double) = (acc._1 + value, acc._2 + 1)\ndef combiner(left:(Double,Int), right:(Double,Int)) = (left._1 + right._1, left._2 + right._2)\n\nval rdd = sc.parallelize(Array((\"a\",1.0), (\"b\",3.0), (\"c\", 17.0), (\"b\", 23.0), (\"c\",12.0)))\nval result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:33:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704020_-1613333815","id":"20160608-202711_661449057","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1156"},{"text":"%md\n# 10 Sorting Data\n\nNormally the records in an RDD are not ordered, but of course in many cases you want to sort the results (so the most important records are at the beginning). This is supported by a `sortBy` method, where you specify a function for extracting the key and optionally a sorting order.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>10 Sorting Data</h1>\n<p>Normally the records in an RDD are not ordered, but of course in many cases you want to sort the results (so the most important records are at the beginning). This is supported by a <code>sortBy</code> method, where you specify a function for extracting the key and optionally a sorting order.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704022_-1612564317","id":"20160608-202711_1598944127","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1157"},{"text":"val result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:33:35+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704023_-1612949066","id":"20160608-202711_1991067578","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1158"},{"text":"%md\nFor simpe cases Spark also supports a `sortByKey` method.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>For simpe cases Spark also supports a <code>sortByKey</code> method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704024_-1614872811","id":"20181110-164448_1237092597","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1159"},{"text":"val result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:34:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704026_-1614103313","id":"20160608-202711_1782595892","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1160"},{"text":"%md\n# 11 Storing Data\n\nEventuall you also want to store results back into HDFS or S3 (or whatever filesystem you are using). Spark provides a couple of methods for different file formats:\n* `saveAsTextFile` saves this RDD as a compressed text file, using string representations of elements.\n* `saveAsObjectFile` saves this RDD as a text file, using string representations of elements.\n* `saveAsHadoopDataset`, `saveAsHadoopFile`, `saveAsNewAPIHadoopFile` saves the RDD as a Hadoop file type","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>11 Storing Data</h1>\n<p>Eventuall you also want to store results back into HDFS or S3 (or whatever filesystem you are using). Spark provides a couple of methods for different file formats:<br/>* <code>saveAsTextFile</code> saves this RDD as a compressed text file, using string representations of elements.<br/>* <code>saveAsObjectFile</code> saves this RDD as a text file, using string representations of elements.<br/>* <code>saveAsHadoopDataset</code>, <code>saveAsHadoopFile</code>, <code>saveAsNewAPIHadoopFile</code> saves the RDD as a Hadoop file type</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704027_-1614488062","id":"20160608-202711_438135470","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1161"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704028_-1616411806","id":"20160608-202711_21418727","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1162"},{"text":"%sh\nhdfs dfs -getmerge /user/zeppelin/numbers /tmp/numbers.txt\ncat /tmp/numbers.txt | head -n10","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704029_-1616796555","id":"20160608-202711_1354775640","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1163"},{"text":"%md\n# 12 Intermission: WordCount\n\nNow we have everything to implement the famous word count example with Apache Spark. The steps are as follows:\n1. Read in some text file (we use *Alice in Wonderland* at `s3://dimajix-training/data/alice`)\n2. Split each line into words\n3. Remove empty words\n4. Transform each word into a key-value pair `(word,1)`\n5. Perform a grouped aggregation for counting the occurances of each word\n6. Sort by number of occurances in descending order\n7. Save the result to HDFS (maybe to `/user/zeppelin/alice_counts`)","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>12 Intermission: WordCount</h1>\n<p>Now we have everything to implement the famous word count example with Apache Spark. The steps are as follows:<br/>1. Read in some text file (we use <em>Alice in Wonderland</em> at <code>s3://dimajix-training/data/alice</code>)<br/>2. Split each line into words<br/>3. Remove empty words<br/>4. Transform each word into a key-value pair <code>(word,1)</code><br/>5. Perform a grouped aggregation for counting the occurances of each word<br/>6. Sort by number of occurances in descending order<br/>7. Save the result to HDFS (maybe to <code>/user/zeppelin/alice_counts</code>)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704031_-1616027057","id":"20170227-233458_1147166395","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1164"},{"text":"val text = sc.textFile(\"s3://dimajix-training/data/alice\")\nval words = // YOUR CODE HERE\n    ","dateUpdated":"2018-12-18T13:28:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704032_-1630262767","id":"20181110-141209_759763098","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1165"},{"text":"%md\n### Inspect Result\nLet us inspect the result using HDFS command line utilities","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Inspect Result</h3>\n<p>Let us inspect the result using HDFS command line utilities</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704033_-1630647515","id":"20181110-165150_1769302106","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1166"},{"text":"%sh\nhdfs dfs -getmerge /user/zeppelin/alice_counts /tmp/alice_counts.txt\ncat /tmp/alice_counts.txt | head -n20","dateUpdated":"2018-12-18T13:28:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704034_-1629493269","id":"20181110-141223_1395030879","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1167"},{"text":"%md\n# 13 Joining Data\n\nThere is one important operation still missing required for a relational algebra: joining multiple RDDs. Of course this is also well supported in Apache Spark, although things get a little bit confusing because of lots of tuples we have to work with...","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>13 Joining Data</h1>\n<p>There is one important operation still missing required for a relational algebra: joining multiple RDDs. Of course this is also well supported in Apache Spark, although things get a little bit confusing because of lots of tuples we have to work with&hellip;</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704035_-1629878018","id":"20160608-202711_137688962","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1168"},{"text":"val x = sc.parallelize(Array((\"a\", 1), (\"b\", 4)))\nval y = sc.parallelize(Array((\"a\", 2), (\"c\", 8)))","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704035_-1629878018","id":"20160608-202711_1298496515","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1169"},{"text":"%md\n### Full outer join\n\nFirst let us perform a full outer join of both RDDs `x` and `y`. In the Spark RDD world, joins always require key-value data on both sides of the join (left and right).\n\nA full outer join will create a RDD with the following tuple\n```\n    (KEY_TYPE, (Option[LEFT_VALUE_TYPE], Option[RIGHT_VALUE_TYPE]))\n```\n\nSpark uses `Option`s to be able to model `NULL` values.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Full outer join</h3>\n<p>First let us perform a full outer join of both RDDs <code>x</code> and <code>y</code>. In the Spark RDD world, joins always require key-value data on both sides of the join (left and right).</p>\n<p>A full outer join will create a RDD with the following tuple</p>\n<pre><code>    (KEY_TYPE, (Option[LEFT_VALUE_TYPE], Option[RIGHT_VALUE_TYPE]))\n</code></pre>\n<p>Spark uses <code>Option</code>s to be able to model <code>NULL</code> values.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704036_-1631801762","id":"20181110-165402_1420913110","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1170"},{"text":"val result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:28:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704038_-1631032264","id":"20181110-141018_1108005333","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1171"},{"text":"%md\n### Inner join\n\nAn inner join works similar, but the result does not use options (since the inner join cannot produce `NULL` values for either side). So the resulting data type looks as follows:\n\n```\n    (KEY_TYPE, (LEFT_VALUE_TYPE, RIGHT_VALUE_TYPE))\n```","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Inner join</h3>\n<p>An inner join works similar, but the result does not use options (since the inner join cannot produce <code>NULL</code> values for either side). So the resulting data type looks as follows:</p>\n<pre><code>    (KEY_TYPE, (LEFT_VALUE_TYPE, RIGHT_VALUE_TYPE))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704040_-1633340758","id":"20181110-165423_1618351793","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1172"},{"text":"val result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704042_-1632571260","id":"20160608-202711_1102106791","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1173"},{"text":"%md\n### Left outer join\n\nNow you should have an idea how a left outer join works, and how the result looks like. Since the right side could be missing, an `Option` is only used for the second (right) value:\n\n```\n    (KEY_TYPE, (LEFT_VALUE_TYPE, Option[RIGHT_VALUE_TYPE]))\n```","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Left outer join</h3>\n<p>Now you should have an idea how a left outer join works, and how the result looks like. Since the right side could be missing, an <code>Option</code> is only used for the second (right) value:</p>\n<pre><code>    (KEY_TYPE, (LEFT_VALUE_TYPE, Option[RIGHT_VALUE_TYPE]))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704043_-1632956009","id":"20181110-165800_1941147381","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1174"},{"text":"val result = // YOUR CODE HERE\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"multiBarChart","height":294,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704044_-1634879753","id":"20160608-202711_1062380962","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1175"},{"text":"%md\n### Right outer join\n\nThe right outer join is no surprise any more, an `Option` is only used for the first (left) value:\n\n```\n    (KEY_TYPE, (Option[LEFT_VALUE_TYPE], RIGHT_VALUE_TYPE))\n```","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Right outer join</h3>\n<p>The right outer join is no surprise any more, an <code>Option</code> is only used for the first (left) value:</p>\n<pre><code>    (KEY_TYPE, (Option[LEFT_VALUE_TYPE], RIGHT_VALUE_TYPE))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704045_-1635264502","id":"20181110-165950_778548671","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1176"},{"text":"val result = x.rightOuterJoin(y)\n\nresult.collect().foreach(println)","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704046_-1634110256","id":"20160608-202711_1257787176","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1177"},{"text":"%md\n# 14 Sampling and Splitting\n\nSometimes you only want to work with a random sample of the data. This is also supported by Spark with the `sample` method:","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>14 Sampling and Splitting</h1>\n<p>Sometimes you only want to work with a random sample of the data. This is also supported by Spark with the <code>sample</code> method:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704047_-1634495004","id":"20160608-202711_1947529611","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1178"},{"text":"val result = // YOUR CODE HERE\nprintln(result.count())","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704048_-1624106784","id":"20160608-202711_184424503","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1179"},{"text":"%md\n## 14.1 Random splitting\nSpark also supports randomly splitting an RDD into multiple different RDDs. This is very important in Machine Learning, where you traditionally need to split up the data into a training, validation and testing set.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>14.1 Random splitting</h2>\n<p>Spark also supports randomly splitting an RDD into multiple different RDDs. This is very important in Machine Learning, where you traditionally need to split up the data into a training, validation and testing set.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704050_-1623337286","id":"20181110-170153_748619240","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1180"},{"text":"val Array(a,b) = // YOUR CODE HERE\n\nprintln(s\"#A: ${a.count()}\")\nprintln(s\"#B: ${b.count()}\")","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704051_-1623722035","id":"20160608-202711_803719300","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1181"},{"text":"%md\n# 15 Caching Data\n\nIn some situations, you may want to persist intermediate results. For example iterative algorithms may benefit from *caching* intermediate results, if the same RDD is transformed again and again. Spark provides some capabilities to persist intermediate results using the methods `cache()` or `persist(storageLevel)`.\n\nNote that also caching is lazy, which means that records will not be created at the time when you call `cache()` or `persist()` but at the first time when the RDD is evaluated. This could be even a simple `count()` action.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>15 Caching Data</h1>\n<p>In some situations, you may want to persist intermediate results. For example iterative algorithms may benefit from <em>caching</em> intermediate results, if the same RDD is transformed again and again. Spark provides some capabilities to persist intermediate results using the methods <code>cache()</code> or <code>persist(storageLevel)</code>.</p>\n<p>Note that also caching is lazy, which means that records will not be created at the time when you call <code>cache()</code> or <code>persist()</code> but at the first time when the RDD is evaluated. This could be even a simple <code>count()</code> action.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704052_-1625645780","id":"20160617-161239_1195207148","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1182"},{"text":"val rdd = sc.parallelize(0 to 1000)\n\n// YOUR CODE HERE\n\nprintln(rdd.count())","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704053_-1626030529","id":"20160617-161253_1903777045","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1183"},{"text":"%md\nIn order to free up some memory again, the DataFrame can also be uncached. This is done with the `unpersist` method.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In order to free up some memory again, the DataFrame can also be uncached. This is done with the <code>unpersist</code> method.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704055_-1625261031","id":"20181110-170612_982341174","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1184"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres59: rdd.type = Numbers 0 to 1000 ParallelCollectionRDD[128] at parallelize at <console>:31\n"}]},"apps":[],"jobName":"paragraph_1545139704057_-1627569524","id":"20160617-161457_1882675347","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1185"},{"text":"%md\nIn addition to the simple `cache` method, Spark also provides a `persist` method, which lets you define precisely how the data should be cached: In memory, on disk or with a mixture of both storages.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In addition to the simple <code>cache</code> method, Spark also provides a <code>persist</code> method, which lets you define precisely how the data should be cached: In memory, on disk or with a mixture of both storages.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704060_-1628723771","id":"20181110-170633_848505365","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1186"},{"text":"import org.apache.spark.storage.StorageLevel\n\nrdd.persist(StorageLevel.MEMORY_AND_DISK)","dateUpdated":"2018-12-18T13:28:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704061_-1629108520","id":"20181110-170639_1323923864","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1187"},{"text":"%md\n# 16 Working with Partitions\n\nInternally Spark splits up all records of an RDD into partitions. It thereby splits up both the data and also the work. The number of partitions has a direct impact on the parallelism of all operations, therefore it is often important to explicitly control the number of partitions.\n\nYou can retrieve the number of partitions of an RDD by accessing its `partitions` member","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>16 Working with Partitions</h1>\n<p>Internally Spark splits up all records of an RDD into partitions. It thereby splits up both the data and also the work. The number of partitions has a direct impact on the parallelism of all operations, therefore it is often important to explicitly control the number of partitions.</p>\n<p>You can retrieve the number of partitions of an RDD by accessing its <code>partitions</code> member</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704063_-1628339022","id":"20160619-093019_1230772729","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1188"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704065_-1544463762","id":"20160619-093034_316796552","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1189"},{"text":"%md\n## 16.1 Repartitioning\n\nYou can also explicitly repartition the data, either by only specifying the number of partitions or by also specifying an explicit partitioning algorithm.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>16.1 Repartitioning</h2>\n<p>You can also explicitly repartition the data, either by only specifying the number of partitions or by also specifying an explicit partitioning algorithm.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704067_-1543694264","id":"20181110-170918_1436647055","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1190"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704069_-1546002757","id":"20160619-093126_1951060033","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1191"},{"text":"%md\n## 16.2 Coalescing partitions\n\nA special form of chaning the number of partitions is to logically *coalesce* them together without performing an actual shuffle operation. This is quite useful before writing results to disk in order to keep the number of files low.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>16.2 Coalescing partitions</h2>\n<p>A special form of chaning the number of partitions is to logically <em>coalesce</em> them together without performing an actual shuffle operation. This is quite useful before writing results to disk in order to keep the number of files low.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704071_-1545233260","id":"20181110-141126_857137719","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1192"},{"text":"// YOUR CODE HERE","dateUpdated":"2018-12-18T13:28:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1545139704072_-1547157004","id":"20181110-171103_576299949","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1193"},{"text":"%sh\nhdfs dfs -ls /user/zeppelin/rdd_32p","dateUpdated":"2018-12-18T13:28:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704074_-1546387506","id":"20181110-171133_2040306891","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1194"},{"text":"val rdd_4p = // YOUR CODE HERE\nrdd_4p.saveAsTextFile(\"/user/zeppelin/rdd_4p\")","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd_4p: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[134] at coalesce at <console>:36\n"}]},"apps":[],"jobName":"paragraph_1545139704076_-1548696000","id":"20160619-093158_1646297391","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1195"},{"text":"%sh\nhdfs dfs -ls /user/zeppelin/rdd_4p","dateUpdated":"2018-12-18T13:28:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704077_-1549080749","id":"20181110-171204_1390871642","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1196"},{"text":"%md\n## 16.2 Special Use Case: mapPartitions\n\nSometimes you do not want to work with individual elements, but with all elements of a whole partition. Typical use case is when you need to acquire an expensive additional resouce (database connection, ....), which you do not want to acquire for every element but cannot be acquired in the driver program.","dateUpdated":"2018-12-18T13:28:24+0000","config":{"tableHide":false,"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>16.2 Special Use Case: mapPartitions</h2>\n<p>Sometimes you do not want to work with individual elements, but with all elements of a whole partition. Typical use case is when you need to acquire an expensive additional resouce (database connection, &hellip;.), which you do not want to acquire for every element but cannot be acquired in the driver program.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1545139704079_-1548311251","id":"20160619-093235_405645521","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1197"},{"text":"val psums = // YOUR CODE HERE\nval total_sum = // YOUR CODE HERE","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704080_-1537923031","id":"20160619-093306_865510153","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1198"},{"text":"","dateUpdated":"2018-12-18T13:28:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545139704082_-1537153533","id":"20160619-073230_1669821313","dateCreated":"2018-12-18T13:28:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1199"}],"name":"Spark RDD - Skeleton","id":"2E1MTWTV5","angularObjects":{"2D8DSN3N4:shared_process":[],"2D7W55G1J:shared_process":[],"2DA3X6UGN:shared_process":[],"2D9HTU14T:shared_process":[],"2DBA6X8JB:shared_process":[],"2DBSCZXK2:shared_process":[],"2D9M853BP:shared_process":[],"2DAXFQ4X2:shared_process":[],"2DB3TEGGU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}