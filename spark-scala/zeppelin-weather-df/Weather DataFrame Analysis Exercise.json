{"paragraphs":[{"text":"%md\n# Weather Data Analytics\nThis notebook performs some basic weather data analytics using the Spark DataFrame interface.","dateUpdated":"2018-03-10T12:08:34+0000","config":{"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Weather Data Analytics</h1>\n<p>This notebook performs some basic weather data analytics using the Spark DataFrame interface.</p>\n"}]},"apps":[],"jobName":"paragraph_1520683714440_-122773765","id":"20160612-173621_1713564499","dateCreated":"2018-03-10T12:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:28550"},{"text":"// Common data location - adjust if not correct\nval storageLocation = \"s3://dimajix-training/data/weather\"","dateUpdated":"2018-03-10T12:08:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"storageLocation: String = s3://dimajix-training/data/weather\n"}]},"apps":[],"jobName":"paragraph_1520683714440_-122773765","id":"20160612-174608_1747052107","dateCreated":"2018-03-10T12:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28551"},{"text":"%md\n# Read in Weather Data\n\nFirst we need to read in all data. Data is stored in different directories for different years. Each directory contains one file per weather station. First lets peek into the single year `2003`, which is located at `storageLocation/2003`. This gives us a basic feeling for the data.\n\nSince the data is not CSV or something else, you need to use the `read.text` method, which will simply treat each line as a single column called `value`.\n\nStore the result in a variable `rawWeatherData2003` and display the first 10 lines using the Zepplin command `z.show`.","dateUpdated":"2018-03-10T12:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{"storageLocation":""},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Read in Weather Data</h1>\n<p>First we need to read in all data. Data is stored in different directories for different years. Each directory contains one file per weather station. First lets peek into the single year <code>2003</code>, which is located at <code>storageLocation/2003</code>. This gives us a basic feeling for the data.</p>\n<p>Since the data is not CSV or something else, you need to use the <code>read.text</code> method, which will simply treat each line as a single column called <code>value</code>.</p>\n<p>Store the result in a variable <code>rawWeatherData2003</code> and display the first 10 lines using the Zepplin command <code>z.show</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520683714441_-123158513","id":"20180310-111936_205472093","dateCreated":"2018-03-10T12:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28552"},{"text":"val rawWeatherData2003 = // YOUR CODE HERE\nz.show(/* YOUR CODE HERE* /)\n","dateUpdated":"2018-03-10T12:09:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520683714441_-123158513","id":"20180310-112322_125525","dateCreated":"2018-03-10T12:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28553"},{"text":"%md\n### Read in all years\nNow we read in all years by creating a union. We also add the year as a logical partition column, this will be used later.\n\nIn order to read in all years, we use Scala functional programming, this makes things a lot more concise.","dateUpdated":"2018-03-10T12:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Read in all years</h3>\n<p>Now we read in all years by creating a union. We also add the year as a logical partition column, this will be used later.</p>\n<p>In order to read in all years, we use Scala functional programming, this makes things a lot more concise.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520683714442_-122004267","id":"20180310-113423_29861183","dateCreated":"2018-03-10T12:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28554"},{"text":"val rawWeatherData = (2003 to 2014) map {i => spark.read.text(s\"${storageLocation}/${i}\").withColumn(\"year\", lit(i)) } reduce((l,r) => l.union(r))\nz.show(rawWeatherData.limit(10))","dateUpdated":"2018-03-10T12:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520683714442_-122004267","id":"20160612-174154_349496240","dateCreated":"2018-03-10T12:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28555"},{"text":"%md\n## Extract Information\n\nThe raw data is not exactly nice to work with, so we need to extract the relevant information by using appropriate `substr` operations.","dateUpdated":"2018-03-10T12:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Extract Information</h2>\n<p>The raw data is not exactly nice to work with, so we need to extract the relevant information by using appropriate <code>substr</code> operations.</p>\n"}]},"apps":[],"jobName":"paragraph_1520683714443_-122389016","id":"20180310-113446_1901710073","dateCreated":"2018-03-10T12:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28556"},{"text":"import org.apache.spark.sql.types._\n\nval weatherData = rawWeatherData.select(\n    col(\"year\"),\n    substring(col(\"value\"),5,6) as \"usaf\",\n    substring(col(\"value\"),11,5) as \"wban\",\n    substring(col(\"value\"),16,8) as \"date\",\n    substring(col(\"value\"),24,4) as \"time\",\n    substring(col(\"value\"),42,5) as \"report_type\",\n    substring(col(\"value\"),61,3) as \"wind_direction\",\n    substring(col(\"value\"),64,1) as \"wind_direction_qual\",\n    substring(col(\"value\"),65,1) as \"wind_observation\",\n    substring(col(\"value\"),66,4).cast(FloatType) / lit(10.0) as \"wind_speed\",\n    substring(col(\"value\"),70,1) as \"wind_speed_qual\",\n    substring(col(\"value\"),88,5).cast(FloatType) / lit(10.0) as \"air_temperature\",\n    substring(col(\"value\"),93,1) as \"air_temperature_qual\"\n)\n\nz.show(weatherData.limit(10))","dateUpdated":"2018-03-10T12:08:34+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"result","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"result","index":0,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520683714443_-122389016","id":"20160612-175008_182048218","dateCreated":"2018-03-10T12:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28557"},{"text":"%md\n# Read in Station Metadata\n\nFortunately station metadata is stored as CSV, so we can directly read that using Sparks `read.csv` mechanisum. Read the data from the CSV file located at `storageLocation/isd-history`. You should also specify the DataFrameReader option `header` to be `true`, this will use the first line of the CSV for creating column names.\n\nStore the result in a variable called `stationData` and again print the first 10 lines using `z.show`.","dateUpdated":"2018-03-10T12:10:46+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Read in Station Metadata</h1>\n<p>Fortunately station metadata is stored as CSV, so we can directly read that using Sparks <code>read.csv</code> mechanisum. Read the data from the CSV file located at <code>storageLocation/isd-history</code>. You should also specify the DataFrameReader option <code>header</code> to be <code>true</code>, this will use the first line of the CSV for creating column names.</p>\n<p>Store the result in a variable called <code>stationData</code> and again print the first 10 lines using <code>z.show</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520683714443_-122389016","id":"20160612-175533_1179463035","dateCreated":"2018-03-10T12:08:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28558","user":"anonymous","dateFinished":"2018-03-10T12:10:46+0000","dateStarted":"2018-03-10T12:10:46+0000"},{"text":"val stationData = // YOUR CODE HERE\n    \nz.show(/* YOUR CODE HERE */)","dateUpdated":"2018-03-10T12:11:20+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"result","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"result","index":0,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520683714443_-122389016","id":"20160612-174633_119344646","dateCreated":"2018-03-10T12:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28559"},{"text":"%md\n# Analytics\n\nNow that we have everything in Hive, we can do the analysis easily.\n\n1. Load data as DataFrames (already done)\n2. Join both DataFrames and station code (wban and usaf)\n3. Extract year from date (or use partition column)\n4. Project on relevant columns (`year`,`wind_speed`,`wind_speed_qual`,`air_temperature`,`air_temperature_qual`,`ctry`)\n5. Rename column `ctry` to `country`\n6. Group by country and year\n7. Aggregate minimum/maximum values for wind and temperature, pay attention to quality! You have to ignore values for which the quality is not \"1\"!\n8. Print the results, such that Zeppelin can make nice graphics :)","dateUpdated":"2018-03-10T12:16:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Analytics</h1>\n<p>Now that we have everything in Hive, we can do the analysis easily.</p>\n<ol>\n  <li>Load data as DataFrames (already done)</li>\n  <li>Join both DataFrames and station code (wban and usaf)</li>\n  <li>Extract year from date (or use partition column)</li>\n  <li>Project on relevant columns (<code>year</code>,<code>wind_speed</code>,<code>wind_speed_qual</code>,<code>air_temperature</code>,<code>air_temperature_qual</code>,<code>ctry</code>)</li>\n  <li>Rename column <code>ctry</code> to <code>country</code></li>\n  <li>Group by country and year</li>\n  <li>Aggregate minimum/maximum values for wind and temperature, pay attention to quality! You have to ignore values for which the quality is not &ldquo;1&rdquo;!</li>\n  <li>Print the results, such that Zeppelin can make nice graphics :)</li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1520683714444_-124312760","id":"20160612-175151_696167562","dateCreated":"2018-03-10T12:08:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:28560","user":"anonymous","dateFinished":"2018-03-10T12:16:05+0000","dateStarted":"2018-03-10T12:16:05+0000"},{"text":"// 2. Join data\nval joined_weather = // YOUR CODE HERE\n\n// 3., 4., 5. Project on relevant columns, extract year and rename ctry to countrs\nval projectedData = // YOUR CODE HERE\n\n// 6. Group by country and year\nval groupedData = // YOUR CODE HERE\n\n// 7. Perform min/max aggregation, but ignore invalid values where quality is not 1\nval result = // YOUR CODE HERE\n\n// 8. Print results\nz.show(result)","dateUpdated":"2018-03-10T12:15:35+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"country","index":0,"aggr":"sum"}],"values":[{"name":"min_wind_speed","index":2,"aggr":"min"},{"name":"max_wind_speed","index":3,"aggr":"max"},{"name":"min_temperature","index":4,"aggr":"min"},{"name":"max_temperature","index":5,"aggr":"max"}],"groups":[{"name":"year","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"country","index":0,"aggr":"sum"},"yAxis":{"name":"year","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520683714444_-124312760","id":"20160612-180048_998679946","dateCreated":"2018-03-10T12:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28561"},{"text":"","dateUpdated":"2018-03-10T12:08:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520683714445_-124697509","id":"20160612-180513_1457527764","dateCreated":"2018-03-10T12:08:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28562"}],"name":"Weather DataFrame Analysis Exercise","id":"2D7UCYP1N","angularObjects":{"2D98FMAH1:shared_process":[],"2D7JMBHMW:shared_process":[],"2D97KQQMQ:shared_process":[],"2D75J9G8A:shared_process":[],"2D7EN88U2:shared_process":[],"2D9JW72F7:shared_process":[],"2D833H41P:shared_process":[],"2D9GNB6ZN:shared_process":[],"2D6C3QSGG:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}