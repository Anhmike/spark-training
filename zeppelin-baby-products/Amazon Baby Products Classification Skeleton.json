{"paragraphs":[{"text":"%md\n# Load Data\n\nFirst we load data from HDFS or S3. It is stored as a trivial CSV file with three columns\n\n* product name\n* review text\n* rating (1 - 5)\n\nWe will explicitly specify a Schema, because the CSV might not contain a valid header. Moreover this keeps us in detailed control over the column types and saves us from one Spark run trying to guess the types. ","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875489_-324204598","id":"20170129-041356_575741458","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Load Data</h1>\n<p>First we load data from HDFS or S3. It is stored as a trivial CSV file with three columns</p>\n<ul>\n<li>product name</li>\n<li>review text</li>\n<li>rating (1 - 5)</li>\n</ul>\n<p>We will explicitly specify a Schema, because the CSV might not contain a valid header. Moreover this keeps us in detailed control over the column types and saves us from one Spark run trying to guess the types.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23881"},{"text":"val basedir = \"file:///home/cloudera/Training/data/\"\n//val basedir = \"s3://dimajix-training/data/\"","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875489_-324204598","id":"20170129-045843_673583787","result":{"code":"SUCCESS","type":"TEXT","msg":"\nbasedir: String = file:///home/cloudera/Training/data/\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23882"},{"text":"import org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.IntegerType\nimport org.apache.spark.sql.functions.col\n\n// Explicitly build the schema as we expect it in the CSV file. Note that we also assume \"rating\" to be string type. This saves\n// us from errors due to wrong entries. Casting a String to something else later only produces SQL NULL values, but no errors.\nval schema = StructType(\n        StructField(\"name\", StringType) ::\n        StructField(\"review\", StringType) ::\n        StructField(\"rating\", StringType) ::\n        Nil\n    )\n// Read in CSV data and cast rating column to integer    \nval csvdata = sqlContext.read\n    .schema(schema)\n    .csv(basedir + \"amazon_baby\")\n    .withColumn(\"rating\", col(\"rating\").cast(IntegerType))\n    .filter(col(\"rating\").isNotNull)\ncsvdata.toZeppelin(10)","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":230.36666870117188,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"review","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"review","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875490_-323050351","id":"20170129-041420_2034427172","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23883","focus":true},{"text":"%md\n# Extract Features from raw data\n\nWe want to train a logistic regression for predicting the sentiment of a product review given its text. Since a logistic regression does not directly work with text, we need to extract some numeric features from the data.\n\nWe will perform the two following extractions, one for the label to be binary and a multi-step extraction for the features.\n\n1. Extract binary sentiment from given review value\n2. Extract bag-of-words model as features","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875490_-323050351","id":"20170129-052423_858609389","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Extract Features from raw data</h1>\n<p>We want to train a logistic regression for predicting the sentiment of a product review given its text. Since a logistic regression does not directly work with text, we need to extract some numeric features from the data.</p>\n<p>We will perform the two following extractions, one for the label to be binary and a multi-step extraction for the features.</p>\n<ol>\n<li>Extract binary sentiment from given review value</li>\n<li>Extract bag-of-words model as features</li>\n</ol>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23884"},{"text":"%md\n## 1. Extract Sentiment\n\nSince we want to perform a classification (positive review vs negative review), we need to extract a binary sentiment value. We will map the ratings as follows:\n\n* Ratings 1 and 2 count as a negative review\n* Rating 3 counts as a neutral review\n* Ratings 4 and 5 count as a positive review\n\nSince we want a binary classification, we will also remove neutral reviews altogether.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875490_-323050351","id":"20170129-042008_1516671555","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>1. Extract Sentiment</h2>\n<p>Since we want to perform a classification (positive review vs negative review), we need to extract a binary sentiment value. We will map the ratings as follows:</p>\n<ul>\n<li>Ratings 1 and 2 count as a negative review</li>\n<li>Rating 3 counts as a neutral review</li>\n<li>Ratings 4 and 5 count as a positive review</li>\n</ul>\n<p>Since we want a binary classification, we will also remove neutral reviews altogether.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23885"},{"text":"// YOUR CODE HERE\nval data = ...","dateUpdated":"2017-01-31T09:53:15-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875490_-323050351","id":"20170129-042350_2056232695","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23886","focus":true},{"text":"%md\n## 2. Extract Features from Reviews\n\nNow we want to split the review text into individual words, so we can create a \"bag of words\" model. In order to get a somewhat nice model, we also need to remove all punctuations from the reviews.\n\n### 2.1. Remove Punctuations\nSo the first step is to remove all puncuations.  This will be done as the first step using a user defined function (UDF) in Spark, which will simply replace any punctuation character by a space.","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875491_-323435100","id":"20170129-043924_1480728312","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>2. Extract Features from Reviews</h2>\n<p>Now we want to split the review text into individual words, so we can create a &ldquo;bag of words&rdquo; model. In order to get a somewhat nice model, we also need to remove all punctuations from the reviews.</p>\n<h3>2.1. Remove Punctuations</h3>\n<p>So the first step is to remove all puncuations.  This will be done as the first step using a user defined function (UDF) in Spark, which will simply replace any punctuation character by a space.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23887"},{"text":"import org.apache.spark.sql.functions.udf\n\nval removePunctuation = udf { text:String => text.replaceAll(\"\\\\p{Punct}\", \" \") }\n// YOUR CODE HERE\nval data2 = ...","dateUpdated":"2017-01-31T09:55:08-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875491_-323435100","id":"20170129-043943_507827673","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23888","focus":true},{"text":"%md\n### 2.2 Split Reviews into Words\n\nNow we want to split every review into multiple words, resulting in a new data column containing an array with all words of the review. We could do that ourselves using some Scala methods, but we use a Transformer provided by Spark instead. Saves us some time and helps to create clean code.\n\nSpark ML contains a whole bunch of Transformers for extracting features. You can find them in the `org.apache.spark.ml.feature` package. Each Transformer accepts some parameters for specifying the details of the transformation. In particular most Transformers know about an input column and and output column which can be set via `setInputCol` and `setOutputCol`.\n\nOur specific task for splitting a string column into an array column with individual words can be performed with the `Tokenizer` Transform.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875491_-323435100","id":"20170129-043958_121242280","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>2.2 Split Reviews into Words</h3>\n<p>Now we want to split every review into multiple words, resulting in a new data column containing an array with all words of the review. We could do that ourselves using some Scala methods, but we use a Transformer provided by Spark instead. Saves us some time and helps to create clean code.</p>\n<p>Spark ML contains a whole bunch of Transformers for extracting features. You can find them in the <code>org.apache.spark.ml.feature</code> package. Each Transformer accepts some parameters for specifying the details of the transformation. In particular most Transformers know about an input column and and output column which can be set via <code>setInputCol</code> and <code>setOutputCol</code>.</p>\n<p>Our specific task for splitting a string column into an array column with individual words can be performed with the <code>Tokenizer</code> Transform.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23889"},{"text":"import org.apache.spark.ml.feature.Tokenizer\n\n// YOUR CODE HERE\nval tokenizer = ...\nval words = tokenizer.transform(data2)","dateUpdated":"2017-01-31T09:55:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"review","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"review","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875491_-323435100","id":"20170129-044010_2048325285","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23890","focus":true},{"text":"%md\n### 2.3 Remove Stop words\n\nWe also want to remove so called stop words, which are all those tiny words which mainly serve as glue for building sentences. Usually they do not contain much information in a simple bag of words model. So we get rid of them.\n\nThis is so common practice that Spark already contains a `StopWordsRemover` Transformer class for just doing that. Again we need to specify some parameters like `inputCol`, `outputCol` and `stopWords`.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875492_-325358844","id":"20170129-044024_1653567754","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>2.3 Remove Stop words</h3>\n<p>We also want to remove so called stop words, which are all those tiny words which mainly serve as glue for building sentences. Usually they do not contain much information in a simple bag of words model. So we get rid of them.</p>\n<p>This is so common practice that Spark already contains a <code>StopWordsRemover</code> Transformer class for just doing that. Again we need to specify some parameters like <code>inputCol</code>, <code>outputCol</code> and <code>stopWords</code>.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23891"},{"text":"val stopWords = Array(\"the\",\"a\",\"and\",\"or\", \"it\", \"this\", \"of\", \"an\", \"as\", \"in\", \"on\", \"is\", \"are\", \"to\", \"was\", \"for\", \"then\", \"i\", \"my\", \"that\", \"with\", \"we\", \"have\", \"so\", \"you\", \"s\", \"\")\n    \n// YOUR CODE HERE\nval stopWordsRemover = ...\nval vwords = stopWordsRemover.transform(words)","dateUpdated":"2017-01-31T09:55:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875492_-325358844","id":"20170129-044037_1562436005","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23892","focus":true},{"text":"%md\nNow lets have a look at common words. Maybe we want to change the list of stop words?","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875492_-325358844","id":"20170129-103148_909017422","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now lets have a look at common words. Maybe we want to change the list of stop words?</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23893"},{"text":"// YOUR CODE HERE\nval term_freq = ...\n\nz.show(term_freq.orderBy(col(\"count\").desc).limit(10))","dateUpdated":"2017-01-31T09:56:53-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875492_-325358844","id":"20170129-103147_26603517","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23894","focus":true},{"text":"%md\n### 2.4 Create Bag of Words Features\n\nFinally we simply count the number of occurances of all words within the reviews. Again we can simply use a `CountVectorizer` Transformer from the `org.apache.spark.ml.feature` package in Spark to perform that task. Again we need to specify `inputCol` and `outputCol`. Additionally we require that each word appears in at least two documents, to ensure some basic significance and avoid fitting the sentiment of a document onto single words.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875493_-325743593","id":"20170129-044054_945211724","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>2.4 Create Bag of Words Features</h3>\n<p>Finally we simply count the number of occurances of all words within the reviews. Again we can simply use a <code>CountVectorizer</code> Transformer from the <code>org.apache.spark.ml.feature</code> package in Spark to perform that task. Again we need to specify <code>inputCol</code> and <code>outputCol</code>. Additionally we require that each word appears in at least two documents, to ensure some basic significance and avoid fitting the sentiment of a document onto single words.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23895"},{"text":"import org.apache.spark.ml.feature.CountVectorizer\n\n// YOUR CODE HERE\nval countVectorizer = ...\n\nval countVectorizerModel = countVectorizer.fit(vwords)","dateUpdated":"2017-01-31T09:57:29-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875493_-325743593","id":"20170129-044111_370443742","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23896","focus":true},{"text":"%md\n### 2.4 Bag of Words - Inspect Vocabulary\n\nThe countVectorizerModel contains an implcit vocabulary containing all words. This can be useful for mapping features back to words\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875493_-325743593","id":"20170129-044132_249596213","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>2.4 Bag of Words - Inspect Vocabulary</h3>\n<p>The countVectorizerModel contains an implcit vocabulary containing all words. This can be useful for mapping features back to words</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23897"},{"text":"// YOUR CODE HERE","dateUpdated":"2017-01-31T09:51:59-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875493_-325743593","id":"20170129-044143_1368822013","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23898","focus":true},{"text":"%md\n### 2.5 Tidy up DataFrame\n\nWe now carry so many columns inside the DataFrame, let's remove some intermediate columns to get more focus on our model. In particular we remove the following columns:\n* `words`\n* `rating`\n \nWe keep the columns `review`, `name`, `sentiment` and `features`. For the model itself, we do not need `review` and `vwords`, but we will perform some simple checks with these columns later on.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875493_-325743593","id":"20170129-044155_1317352834","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>2.5 Tidy up DataFrame</h3>\n<p>We now carry so many columns inside the DataFrame, let's remove some intermediate columns to get more focus on our model. In particular we remove the following columns:</p>\n<ul>\n<li><code>words</code></li>\n<li><code>rating</code></li>\n</ul>\n<p>We keep the columns <code>review</code>, <code>name</code>, <code>sentiment</code> and <code>features</code>. For the model itself, we do not need <code>review</code> and <code>vwords</code>, but we will perform some simple checks with these columns later on.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23899"},{"text":"// YOUR CODE HERE","dateUpdated":"2017-01-31T09:52:27-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875494_-324589347","id":"20170129-044205_190711273","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23900","focus":true},{"text":"%md\n# Split Train Data / Validation Data\n\nNow let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation. Again the Spark DataFrame class offers a built in core method to perform the split.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875494_-324589347","id":"20170129-044218_1046591560","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Split Train Data / Validation Data</h1>\n<p>Now let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation. Again the Spark DataFrame class offers a built in core method to perform the split.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23901"},{"text":"// YOUR CODE HERE\nval Array(trainingData, validationData) = ...\n\nprintln(s\"Training Data: ${trainingData.count()}\")\nprintln(s\"Validation Data: ${validationData.count()}\")","dateUpdated":"2017-01-31T09:57:59-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875494_-324589347","id":"20170129-044229_383362931","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23902","focus":true},{"text":"%md\n# Train Classifier\n\nThere are many different classification algorithms out there. We will use a LogisticRegression, of course a DecisionTreeClassifier could be another interesting option.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875494_-324589347","id":"20170129-044249_824800015","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Train Classifier</h1>\n<p>There are many different classification algorithms out there. We will use a LogisticRegression, of course a DecisionTreeClassifier could be another interesting option.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23903"},{"text":"import org.apache.spark.ml.classification.LogisticRegression\n\n// YOUR CODE HERE\nval logisticRegression = ...\nval logisticModel = ...","dateUpdated":"2017-01-31T09:58:26-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875495_-324974096","id":"20170129-044302_1834217762","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23904","focus":true},{"text":"%md\n# Inspect Model\n\nThe LogisticRegressionModel also uses coefficients mapped to individual words. Let's have a look at them.\n\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875495_-324974096","id":"20170129-044314_1157019956","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Inspect Model</h1>\n<p>The LogisticRegressionModel also uses coefficients mapped to individual words. Let's have a look at them.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23905"},{"text":"logisticModel.coefficients.toArray.take(20).foreach(println)","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875495_-324974096","id":"20170129-044329_1169250287","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23906","focus":true},{"text":"%md\nLet's have a quick look about how many words have a positive weight and how many words have a negative weight.","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875495_-324974096","id":"20170129-101914_1671147041","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let's have a quick look about how many words have a positive weight and how many words have a negative weight.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23907"},{"text":"val coefficients = logisticModel.coefficients.toArray\nval numPositiveWeights = coefficients.count(_ > 0)\nval numNegativeWeights = coefficients.count(_ < 0)\n\nprintln(s\"Number positive weights $numPositiveWeights\")\nprintln(s\"Number negative weights $numNegativeWeights\")","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875495_-324974096","id":"20170129-044553_1980650936","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23908","focus":true},{"text":"%md\n## Find Weights of some Words\n\nLet's check how coefficients look like for some clearly positive or negative words. This can be done by first looking up a specific word in the vocabulary of the CountVectorizerModel and then using that index to retrieve the weight in the LogisticRegressionModel we just trained. ","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875496_-326897840","id":"20170129-044409_841287254","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Find Weights of some Words</h2>\n<p>Let's check how coefficients look like for some clearly positive or negative words. This can be done by first looking up a specific word in the vocabulary of the CountVectorizerModel and then using that index to retrieve the weight in the LogisticRegressionModel we just trained.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23909"},{"text":"def printWordWeight(word:String) = {\n    // YOUR CODE HERE\n    val index = ...\n    val weight = ...\n    println(s\"$word : $weight\")\n}\n    \n// YOUR CODE HERE    \n","dateUpdated":"2017-01-31T09:59:29-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875496_-326897840","id":"20170129-044413_362370312","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23910","focus":true},{"text":"%md\n## Find Extreme Words\n\nLet us try to find the most positive and most negative word according to the weights. This can be achieved using a argmin function to find the index and the vocabulary to map the index to the actual word. Unfortunately Scala does not provide an argmin or argmax function out of the box, so we need to create our own.","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875496_-326897840","id":"20170129-044426_1753481605","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Find Extreme Words</h2>\n<p>Let us try to find the most positive and most negative word according to the weights. This can be achieved using a argmin function to find the index and the vocabulary to map the index to the actual word. Unfortunately Scala does not provide an argmin or argmax function out of the box, so we need to create our own.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23911"},{"text":"import scala.collection.IndexedSeq\nimport scala.reflect.ClassTag\n\ndef idxmin[X  <% Ordered[X]: ClassTag](seq:IndexedSeq[X]) = {\n    seq.zipWithIndex.minBy(_._1)._2\n}\ndef idxmax[X  <% Ordered[X]: ClassTag](seq:IndexedSeq[X]) = {\n    seq.zipWithIndex.maxBy(_._1)._2\n}","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875497_-327282589","id":"20170129-062847_749373872","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23914","focus":true},{"text":"%md\nWith argmin and argmax in place, we can now find the best and worst words","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875497_-327282589","id":"20170129-062010_1045358891","result":{"code":"SUCCESS","type":"HTML","msg":"<p>With argmin and argmax in place, we can now find the best and worst words</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23915"},{"text":"//val worstWordIndex = argmin(logisticModel.coefficients.toArray.indices, { n:Int => logisticModel.coefficients.toArray(n)} )\nval worstWordIndex = idxmin(logisticModel.coefficients.toArray)\nval worstWord = countVectorizerModel.vocabulary(worstWordIndex)\nval worstWeight = logisticModel.coefficients(worstWordIndex)\nprintln(s\"Worst word: $worstWord value $worstWeight\")\n\n//val bestWordIndex = argmax(logisticModel.coefficients.toArray.indices, { n:Int => logisticModel.coefficients.toArray(n)} )\nval bestWordIndex = idxmax(logisticModel.coefficients.toArray)\nval bestWord = countVectorizerModel.vocabulary(bestWordIndex)\nval bestWeight = logisticModel.coefficients(bestWordIndex)\nprintln(s\"Best word: $bestWord value $bestWeight\")","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875497_-327282589","id":"20170129-044442_1516043671","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23916","focus":true},{"text":"%md\n## Document Frequencies\n\nLet's have a look at document frequencies of words","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875497_-327282589","id":"20170129-064007_1643161693","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Document Frequencies</h2>\n<p>Let's have a look at document frequencies of words</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23917"},{"text":"import org.apache.spark.sql.functions.explode\n\nval doc_freq = features\n    .select(explode(features(\"vwords\")).alias(\"word\"), features(\"review\"))\n    .distinct()\n    .groupBy(col(\"word\")).count()\n\nz.show(doc_freq.orderBy(col(\"count\").desc).limit(10))","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875497_-327282589","id":"20170129-044643_486786656","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23918","focus":true},{"text":"%md\n# Making Predictions\n\nThe primary idea is of course to make predictions of the sentiment using the learned model.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875498_-326128342","id":"20170129-044654_1617813197","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Making Predictions</h1>\n<p>The primary idea is of course to make predictions of the sentiment using the learned model.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23919"},{"text":"// YOUR CODE HERE\nval pred = ...\n","dateUpdated":"2017-01-31T10:02:01-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875498_-326128342","id":"20170129-044704_1120754047","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23920","focus":true},{"text":"%md\n# Find the most Positive Review\n\nUsing the column rawPrediction, we can find the review which has the highest positive prediction.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875498_-326128342","id":"20170129-044715_63959255","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Find the most Positive Review</h1>\n<p>Using the column rawPrediction, we can find the review which has the highest positive prediction.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23921"},{"text":"import org.apache.spark.ml.linalg.Vector\n// Extract one component from a Vectors\nval extractFromVector = udf{ (v:Vector, i:Integer) => v(i) }\n\n// YOUR CODE HERE\nval positives = ...\n\nz.show()\n    positives.select(col(\"name\"),col(\"review\"),col(\"sentiment\"),col(\"rawPrediction\"),col(\"prediction\")).limit(6)\n)    ","dateUpdated":"2017-01-31T10:03:13-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"name","index":0,"aggr":"sum"}],"values":[{"name":"review","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"name","index":0,"aggr":"sum"},"yAxis":{"name":"review","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875498_-326128342","id":"20170129-044725_1084497064","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23922","focus":true},{"text":"%md\n# Evaluation of Prediction\n\nAgain we want to assess the performance of the prediction model. This can be done using the builtin class BinaryClassificationEvaluator.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875499_-326513091","id":"20170129-044740_1008352335","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Evaluation of Prediction</h1>\n<p>Again we want to assess the performance of the prediction model. This can be done using the builtin class BinaryClassificationEvaluator.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23923"},{"text":"import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n\n// YOUR CODE HERE\nval evaluator = ...\nprintln(evaluator.evaluate(pred))\n","dateUpdated":"2017-01-31T10:03:48-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875499_-326513091","id":"20170129-044751_2122449463","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23924","focus":true},{"text":"%md\n# Custom Evaluator\n\nWe want to use a different metric namely accuracy. Accuracy is defined as\n\nnumber_correct_predictions / total_number_predictions\n\nFirst let us directly calculate that metric\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875499_-326513091","id":"20170129-044803_1942847520","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Custom Evaluator</h1>\n<p>We want to use a different metric namely accuracy. Accuracy is defined as</p>\n<p>number_correct_predictions / total_number_predictions</p>\n<p>First let us directly calculate that metric</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23925"},{"text":"// YOUR CODE HERE\nval numTotal = ...\nval numCorrect = ...\n\nval modelAccuracy = numCorrect.toFloat / numTotal.toFloat\n\nprintln(s\"Model Accuracy: $modelAccuracy\")","dateUpdated":"2017-01-31T10:04:42-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875499_-326513091","id":"20170129-044811_586986077","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23926","focus":true},{"text":"%md\n## Compare with Dummy Predictor\n\nIt is always interesting to see how a trivial prediction performs. The trivial predictor simply predicts the most common class for all objects. In this case this would be a positive review.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044827_1506840292","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Compare with Dummy Predictor</h2>\n<p>It is always interesting to see how a trivial prediction performs. The trivial predictor simply predicts the most common class for all objects. In this case this would be a positive review.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23927"},{"text":"// YOUR CODE HERE\nval numTotal = ...\nval numPositive = ...\n\nval baselineAccuracy = numPositive.toFloat / numTotal\n\nprintln(s\"Baseline Accuracy: $baselineAccuracy\")","dateUpdated":"2017-01-31T10:04:42-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044840_1725986384","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23928","focus":true},{"text":"%md\n## Custom Evaluator\n\nNow let us create a new Evaluator class implementing accuracy as the relevant Metric.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044900_1965068259","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Custom Evaluator</h2>\n<p>Now let us create a new Evaluator class implementing accuracy as the relevant Metric.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23929"},{"text":"import org.apache.spark.sql.functions.col\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.ml.param.Param\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.evaluation.Evaluator\n\nclass AccuracyClassificationEvaluator(override val uid: String) extends org.apache.spark.ml.evaluation.Evaluator {\n    \n    def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(\"accuEval\"))\n\n    val labelCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"labelCol\", \"label column name\")\n    setDefault(labelCol, \"label\")\n    def getLabelCol: String = $(labelCol)\n    def setLabelCol(value:String): this.type = set(labelCol, value)\n\n    val predictionCol: org.apache.spark.ml.param.Param[String] = new org.apache.spark.ml.param.Param[String](this, \"predictionCol\", \"prediction column name\")\n    setDefault(predictionCol, \"prediction\")\n    def getPredictionCol: String = $(predictionCol)\n    def setPredictionCol(value:String): this.type = set(predictionCol, value)\n\n    override def evaluate(dataset: org.apache.spark.sql.Dataset[_]) : Double = {\n        val numTotal = dataset.count()\n        val numCorrect = dataset.filter(dataset(getLabelCol) === dataset(getPredictionCol)).count()\n        numCorrect.toDouble / numTotal\n    }\n\n    override def copy(extra: org.apache.spark.ml.param.ParamMap): AccuracyClassificationEvaluator = null // defaultCopy(extra)\n\n}","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044909_2137892823","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23930","focus":true},{"text":"// YOUR CODE HERE\nval evaluator = \n\nevaluator.evaluate(pred)","dateUpdated":"2017-01-31T10:05:07-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044928_1012472330","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23931","focus":true},{"text":"%md\n# Tweak Hyper Parameters\n\nAgain we want to improve overall performance by tweaking model parameters. So first let's see which parameters are available for tweaking\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875500_-328436836","id":"20170129-044938_1518598932","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Tweak Hyper Parameters</h1>\n<p>Again we want to improve overall performance by tweaking model parameters. So first let's see which parameters are available for tweaking</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23932"},{"text":"println(new LogisticRegression().explainParams())","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875501_-328821585","id":"20170129-044943_1471930743","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23933","focus":true},{"text":"%md\nLet us try some different parameters and check the results","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875501_-328821585","id":"20170129-044953_1947443369","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let us try some different parameters and check the results</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23934"},{"text":"val logisticRegression2 = new LogisticRegression()\n    .setFeaturesCol(\"features\")\n    .setLabelCol(\"sentiment\")\n    .setRegParam(0.01)\n    .setMaxIter(100)\nval logisticModel2 = logisticRegression2.fit(train_data)\n\nval pred = logisticModel2.transform(test_data)\n\nval roc_evaluator = new BinaryClassificationEvaluator()\n    .setLabelCol(\"sentiment\")\n    .setMetricName(\"areaUnderROC\")\nval acc_evaluator = new AccuracyClassificationEvaluator()\n    .setLabelCol(\"sentiment\")\n\nprintln(s\"areaUnderROC = ${roc_evaluator.evaluate(pred)}\")\nprintln(s\"accuracy = ${acc_evaluator.evaluate(pred)}\")","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875501_-328821585","id":"20170129-045003_1169230278","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23935","focus":true},{"text":"%md\n## Finding best Hyper Parameters\n\nSo we got an improvement, but what would be best? We need to try.\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875501_-328821585","id":"20170129-045014_296514678","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Finding best Hyper Parameters</h2>\n<p>So we got an improvement, but what would be best? We need to try.</p>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23936"},{"text":"for (reg_param <- Array(0.0, 0.0001, 0.01, 1.0, 100.0)) {\n    val logisticRegression2 = new LogisticRegression()\n        .setFeaturesCol(\"features\")\n        .setLabelCol(\"sentiment\")\n        .setRegParam(reg_param)\n        .setMaxIter(100)\n        \n    // YOUR CODE HERE        \n    val logisticModel2 = ...\n    \n    val pred = ...\n    \n    val roc_evaluator = new BinaryClassificationEvaluator()\n        .setLabelCol(\"sentiment\")\n        .setMetricName(\"areaUnderROC\")\n    val acc_evaluator = new AccuracyClassificationEvaluator()\n        .setLabelCol(\"sentiment\")\n\n    val roc = ...\n    val acc = ...\n    println(s\"reg_param = $reg_param\")\n    println(s\"    areaUnderROC = ${roc}\")\n    println(s\"    Model Accuracy = ${acc}\")\n}","dateUpdated":"2017-01-31T10:07:22-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875501_-328821585","id":"20170129-045023_455651463","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23937","focus":true},{"text":"%md\n## ParamGridBuilder & CrossValidator\n\nSince the selection of hyper parameters is a very common job and might be tedious work, there is some nice support in PySpark to simplify it. It is a two-step approach:\n\n* Use ParamGridBuilder to create a set of parameters to test, possibly for different hyper parameters\n* Use a CrossValidator for selecting the best set of parameters\n\n","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875502_-327667338","id":"20170129-045036_716307760","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>ParamGridBuilder &amp; CrossValidator</h2>\n<p>Since the selection of hyper parameters is a very common job and might be tedious work, there is some nice support in PySpark to simplify it. It is a two-step approach:</p>\n<ul>\n<li>Use ParamGridBuilder to create a set of parameters to test, possibly for different hyper parameters</li>\n<li>Use a CrossValidator for selecting the best set of parameters</li>\n</ul>\n"},"dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23938"},{"text":"import org.apache.spark.ml.tuning.ParamGridBuilder\n\nval lr = new LogisticRegression()\n    .setFeaturesCol(\"features\")\n    .setLabelCol(\"sentiment\")\nval param_grid = new ParamGridBuilder()\n    .addGrid(lr.regParam, Array(0.0, 0.0001, 0.01, 1.0, 100.0))\n    .addGrid(lr.maxIter, Array(10, 100))\n    .build()\n    \nfor (pset <- param_grid) {\n    println(pset.toSeq.map(p => s\"${p.param.name}=${p.value}\").mkString(\", \"))\n}","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875502_-327667338","id":"20170129-045049_988951344","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23939","focus":true},{"text":"import org.apache.spark.ml.tuning.CrossValidator\n\nval lr = new LogisticRegression()\n    .setFeaturesCol(\"features\")\n    .setLabelCol(\"sentiment\")\nval evaluator = new AccuracyClassificationEvaluator()\n    .setLabelCol(\"sentiment\")\nval cv = new CrossValidator()\n    .setEstimator(lr)\n    .setEstimatorParamMaps(param_grid)\n    .setEvaluator(evaluator)\n    .setNumFolds(3)\nval model = cv.fit(train_data)\n\nprintln(evaluator.evaluate(model.transform(test_data)))","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875502_-327667338","id":"20170129-045113_1186994890","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23940","focus":true},{"text":"model.bestModel.params.foreach(p => println(p.name + \"=\" + model.bestModel.get(p).getOrElse(None)))","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875502_-327667338","id":"20170129-075356_1634607010","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23941","focus":true},{"text":"","dateUpdated":"2017-01-31T09:47:55-0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485884875502_-327667338","id":"20170129-080247_746419066","dateCreated":"2017-01-31T09:47:55-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:23942"}],"name":"Amazon Baby Products Classification Skeleton","id":"2C91GE1A4","angularObjects":{"2C73P7NJN:shared_process":[],"2C7KU6EWG:shared_process":[],"2C8H7AG7Q:shared_process":[],"2CANY5QMM:shared_process":[],"2C7YM9SBT:shared_process":[],"2CAHZM5EW:shared_process":[],"2CA194TC4:shared_process":[],"2C85Z5A3J:shared_process":[],"2C77BBC6M:shared_process":[],"2C9T8R64M:shared_process":[],"2C82H3SUX:shared_process":[],"2C7W1UTSM:shared_process":[],"2C99CAHNC:shared_process":[],"2C7VKNJZ3:shared_process":[],"2C7MNAP62:shared_process":[],"2C8SJ4SC1:shared_process":[],"2C8273BS9:shared_process":[],"2CA9V89Q3:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}