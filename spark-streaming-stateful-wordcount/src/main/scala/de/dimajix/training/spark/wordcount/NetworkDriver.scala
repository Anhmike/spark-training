package de.dimajix.training.spark.wordcount

import org.apache.spark.SparkConf
import org.apache.spark.streaming.Seconds
import org.apache.spark.streaming.StreamingContext
import org.slf4j.Logger
import org.slf4j.LoggerFactory

/**
  * Created by kaya on 03.12.15.
  */
object NetworkDriver {
  def main(args: Array[String]) : Unit = {
    // First create driver, so can already process arguments
    val options = new Options(args)
    val driver = new NetworkDriver(options)

    // ... and run!
    driver.run()
  }
}


class NetworkDriver(options:Options) {
  private val logger: Logger = LoggerFactory.getLogger(classOf[NetworkDriver])

  def run() = {
    // Now create SparkContext (possibly flooding the console with logging information)
    val conf = new SparkConf()
      .setAppName("Spark Streaming Stateful Word Count")

    // Create Context with given checkpoint directory
    val ssc = new StreamingContext(conf, Seconds(1))
    ssc.checkpoint(options.checkpointDirectory)

    // Create a ReceiverInputDStream on target ip:port and count the
    // words in input stream of \n delimited test (eg. generated by 'nc')
    val input = ssc.socketTextStream(options.streamHostname, options.streamPort)
    val words = input.flatMap(_.split(" ")).filter(_ != "").map(x => (x,1))

    // Update function which updates the DStreams state
    val updateFunc = (counts: Seq[Int], runningCount: Option[Int]) => {
      val sum = counts.sum + runningCount.getOrElse(0)
      Some(sum)
    }

    words.updateStateByKey(updateFunc)
      .transform(_.sortBy(_._2, ascending = false))
      .print(20)

    ssc.start()
    ssc.awaitTermination()
  }
}
