{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "\n",
    "# Read in all years, store them in an Python array\n",
    "raw_weather_per_year = [spark.read.text(storageLocation + \"/\" + str(i)).withColumn(\"year\", lit(i)) for i in range(2003,2015)]\n",
    "\n",
    "# Union all years together\n",
    "raw_weather = reduce(lambda l,r: l.union(r), raw_weather_per_year)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a single year to keep execution plans small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather = spark.read.text(storageLocation + \"/2003\").withColumn(\"year\", lit(2003))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "    col(\"year\"),\n",
    "    substring(col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    substring(col(\"value\"),11,5).alias(\"wban\"),\n",
    "    substring(col(\"value\"),16,8).alias(\"date\"),\n",
    "    substring(col(\"value\"),24,4).alias(\"time\"),\n",
    "    substring(col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    substring(col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    substring(col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    substring(col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (substring(col(\"value\"),66,4).cast(\"float\") / lit(10.0)).alias(\"wind_speed\"),\n",
    "    substring(col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (substring(col(\"value\"),88,5).cast(\"float\") / lit(10.0)).alias(\"air_temperature\"),\n",
    "    substring(col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Station Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(storageLocation + \"/isd-history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.sql.sources.bucketing.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Normal Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [usaf#87, wban#88], [usaf#122, wban#123], Inner\n",
      ":- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      ":     +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      ":        +- *(1) Filter (isnotnull(substring(value#82, 5, 6)) && isnotnull(substring(value#82, 11, 5)))\n",
      ":           +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(4) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#122, wban#123, 200)\n",
      "      +- *(3) Project [USAF#122, WBAN#123, STATION NAME#124, CTRY#125, STATE#126, ICAO#127, LAT#128, LON#129, ELEV(M)#130, BEGIN#131, END#132]\n",
      "         +- *(3) Filter (isnotnull(wban#123) && isnotnull(usaf#122))\n",
      "            +- *(3) FileScan csv [USAF#122,WBAN#123,STATION NAME#124,CTRY#125,STATE#126,ICAO#127,LAT#128,LON#129,ELEV(M)#130,BEGIN#131,END#132] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(WBAN), IsNotNull(USAF)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n"
     ]
    }
   ],
   "source": [
    "result = weather.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Bucketed Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.write \\\n",
    "    .bucketBy(200, \"usaf\", \"wban\") \\\n",
    "    .sortBy(\"usaf\", \"wban\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"weather_buckets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect table in Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                year|                 int|   null|\n",
      "|                usaf|              string|   null|\n",
      "|                wban|              string|   null|\n",
      "|                date|              string|   null|\n",
      "|                time|              string|   null|\n",
      "|         report_type|              string|   null|\n",
      "|      wind_direction|              string|   null|\n",
      "| wind_direction_qual|              string|   null|\n",
      "|    wind_observation|              string|   null|\n",
      "|          wind_speed|              double|   null|\n",
      "|     wind_speed_qual|              string|   null|\n",
      "|     air_temperature|              double|   null|\n",
      "|air_temperature_qual|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|     weather_buckets|       |\n",
      "|               Owner|              hadoop|       |\n",
      "|        Created Time|Sun Oct 07 07:49:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.3.1|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|         Num Buckets|                 200|       |\n",
      "|      Bucket Columns|    [`usaf`, `wban`]|       |\n",
      "|        Sort Columns|    [`usaf`, `wban`]|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|       6353896 bytes|       |\n",
      "|            Location|hdfs://ip-10-200-...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED weather_buckets\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try join again, with bucketed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [usaf#694, wban#695], [usaf#122, wban#123], Inner\n",
      ":- *(1) Sort [usaf#694 ASC NULLS FIRST, wban#695 ASC NULLS FIRST], false, 0\n",
      ":  +- *(1) Project [year#693, usaf#694, wban#695, date#696, time#697, report_type#698, wind_direction#699, wind_direction_qual#700, wind_observation#701, wind_speed#702, wind_speed_qual#703, air_temperature#704, air_temperature_qual#705]\n",
      ":     +- *(1) Filter (isnotnull(wban#695) && isnotnull(usaf#694))\n",
      ":        +- *(1) FileScan parquet default.weather_buckets[year#693,usaf#694,wban#695,date#696,time#697,report_type#698,wind_direction#699,wind_direction_qual#700,wind_observation#701,wind_speed#702,wind_speed_qual#703,air_temperature#704,air_temperature_qual#705] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ip-10-200-101-220.eu-central-1.compute.internal:8020/user/hive/warehouse..., PartitionFilters: [], PushedFilters: [IsNotNull(wban), IsNotNull(usaf)], ReadSchema: struct<year:int,usaf:string,wban:string,date:string,time:string,report_type:string,wind_direction...\n",
      "+- *(3) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#122, wban#123, 200)\n",
      "      +- *(2) Project [USAF#122, WBAN#123, STATION NAME#124, CTRY#125, STATE#126, ICAO#127, LAT#128, LON#129, ELEV(M)#130, BEGIN#131, END#132]\n",
      "         +- *(2) Filter (isnotnull(wban#123) && isnotnull(usaf#122))\n",
      "            +- *(2) FileScan csv [USAF#122,WBAN#123,STATION NAME#124,CTRY#125,STATE#126,ICAO#127,LAT#128,LON#129,ELEV(M)#130,BEGIN#131,END#132] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(WBAN), IsNotNull(USAF)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n"
     ]
    }
   ],
   "source": [
    "weather_hive = spark.read.table(\"weather_buckets\")\n",
    "result = weather_hive.join(stations, (weather_hive[\"usaf\"] == stations[\"usaf\"]) & (weather_hive[\"wban\"] == stations[\"wban\"]))\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following attributes have to match\n",
    "* bucketing columns\n",
    "* number of buckets = number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.3 (Python 3)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
