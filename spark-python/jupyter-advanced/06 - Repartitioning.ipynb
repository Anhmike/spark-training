{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartitioning Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "\n",
    "# Read in all years, store them in an Python array\n",
    "raw_weather_per_year = [spark.read.text(storageLocation + \"/\" + str(i)).withColumn(\"year\", lit(i)) for i in range(2003,2015)]\n",
    "\n",
    "# Union all years together\n",
    "raw_weather = reduce(lambda l,r: l.union(r), raw_weather_per_year)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a single year to keep execution plans small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather = spark.read.text(storageLocation + \"/2003\").withColumn(\"year\", lit(2003))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "    col(\"year\"),\n",
    "    substring(col(\"value\"),5,6).alias(\"usaf\"),\n",
    "    substring(col(\"value\"),11,5).alias(\"wban\"),\n",
    "    substring(col(\"value\"),16,8).alias(\"date\"),\n",
    "    substring(col(\"value\"),24,4).alias(\"time\"),\n",
    "    substring(col(\"value\"),42,5).alias(\"report_type\"),\n",
    "    substring(col(\"value\"),61,3).alias(\"wind_direction\"),\n",
    "    substring(col(\"value\"),64,1).alias(\"wind_direction_qual\"),\n",
    "    substring(col(\"value\"),65,1).alias(\"wind_observation\"),\n",
    "    (substring(col(\"value\"),66,4).cast(\"float\") / lit(10.0)).alias(\"wind_speed\"),\n",
    "    substring(col(\"value\"),70,1).alias(\"wind_speed_qual\"),\n",
    "    (substring(col(\"value\"),88,5).cast(\"float\") / lit(10.0)).alias(\"air_temperature\"),\n",
    "    substring(col(\"value\"),93,1).alias(\"air_temperature_qual\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Station Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(storageLocation + \"/isd-history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(10, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Repartition & Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect execution plan of normal JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [usaf#87, wban#88], [usaf#122, wban#123], Inner\n",
      ":- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      ":     +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      ":        +- *(1) Filter (isnotnull(substring(value#82, 11, 5)) && isnotnull(substring(value#82, 5, 6)))\n",
      ":           +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(4) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#122, wban#123, 200)\n",
      "      +- *(3) Project [USAF#122, WBAN#123, STATION NAME#124, CTRY#125, STATE#126, ICAO#127, LAT#128, LON#129, ELEV(M)#130, BEGIN#131, END#132]\n",
      "         +- *(3) Filter (isnotnull(usaf#122) && isnotnull(wban#123))\n",
      "            +- *(3) FileScan csv [USAF#122,WBAN#123,STATION NAME#124,CTRY#125,STATE#126,ICAO#127,LAT#128,LON#129,ELEV(M)#130,BEGIN#131,END#132] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n"
     ]
    }
   ],
   "source": [
    "result = weather.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-partition data (first try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [usaf#87, wban#88], [usaf#122, wban#123], Inner\n",
      ":- *(1) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      ":  +- *(1) Filter (isnotnull(wban#88) && isnotnull(usaf#87))\n",
      ":     +- InMemoryTableScan [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], [isnotnull(wban#88), isnotnull(usaf#87)]\n",
      ":           +- InMemoryRelation [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      ":                 +- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      ":                    +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      ":                       +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(3) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#122, wban#123, 200)\n",
      "      +- *(2) Project [USAF#122, WBAN#123, STATION NAME#124, CTRY#125, STATE#126, ICAO#127, LAT#128, LON#129, ELEV(M)#130, BEGIN#131, END#132]\n",
      "         +- *(2) Filter (isnotnull(usaf#122) && isnotnull(wban#123))\n",
      "            +- *(2) FileScan csv [USAF#122,WBAN#123,STATION NAME#124,CTRY#125,STATE#126,ICAO#127,LAT#128,LON#129,ELEV(M)#130,BEGIN#131,END#132] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the uncached sort and filter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-partition data (second try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"]) \\\n",
    "    .orderBy(weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [usaf#87, wban#88], [usaf#122, wban#123], Inner\n",
      ":- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      ":     +- *(1) Filter (isnotnull(wban#88) && isnotnull(usaf#87))\n",
      ":        +- InMemoryTableScan [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], [isnotnull(wban#88), isnotnull(usaf#87)]\n",
      ":              +- InMemoryRelation [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      ":                    +- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], true, 0\n",
      ":                       +- Exchange rangepartitioning(usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST, 200)\n",
      ":                          +- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      ":                             +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      ":                                +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(4) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#122, wban#123, 200)\n",
      "      +- *(3) Project [USAF#122, WBAN#123, STATION NAME#124, CTRY#125, STATE#126, ICAO#127, LAT#128, LON#129, ELEV(M)#130, BEGIN#131, END#132]\n",
      "         +- *(3) Filter (isnotnull(usaf#122) && isnotnull(wban#123))\n",
      "            +- *(3) FileScan csv [USAF#122,WBAN#123,STATION NAME#124,CTRY#125,STATE#126,ICAO#127,LAT#128,LON#129,ELEV(M)#130,BEGIN#131,END#132] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the two sorts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-partition data (final try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, usaf: string, wban: string, date: string, time: string, report_type: string, wind_direction: string, wind_direction_qual: string, wind_observation: string, wind_speed: double, wind_speed_qual: string, air_temperature: double, air_temperature_qual: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep = weather.repartition(200, weather[\"usaf\"], weather[\"wban\"]) \\\n",
    "    .sortWithinPartitions(weather[\"usaf\"], weather[\"wban\"])\n",
    "weather_rep.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [usaf#87, wban#88], [usaf#122, wban#123], Inner\n",
      ":- *(1) Filter (isnotnull(wban#88) && isnotnull(usaf#87))\n",
      ":  +- InMemoryTableScan [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], [isnotnull(wban#88), isnotnull(usaf#87)]\n",
      ":        +- InMemoryRelation [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      ":              +- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      ":                 +- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      ":                    +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      ":                       +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(3) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#122, wban#123, 200)\n",
      "      +- *(2) Project [USAF#122, WBAN#123, STATION NAME#124, CTRY#125, STATE#126, ICAO#127, LAT#128, LON#129, ELEV(M)#130, BEGIN#131, END#132]\n",
      "         +- *(2) Filter (isnotnull(usaf#122) && isnotnull(wban#123))\n",
      "            +- *(2) FileScan csv [USAF#122,WBAN#123,STATION NAME#124,CTRY#125,STATE#126,ICAO#127,LAT#128,LON#129,ELEV(M)#130,BEGIN#131,END#132] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect WebUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 1: Build cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1798753"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 2: Use cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1798753"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Repartition & Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Simple Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[usaf#87, wban#88], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      "   +- *(1) HashAggregate(keys=[usaf#87, wban#88], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "      +- *(1) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "         +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n"
     ]
    }
   ],
   "source": [
    "result = weather.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        min(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        max(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Aggregation after repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) HashAggregate(keys=[usaf#87, wban#88], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- *(1) HashAggregate(keys=[usaf#87, wban#88], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "   +- InMemoryTableScan [usaf#87, wban#88, air_temperature#97, air_temperature_qual#98]\n",
      "         +- InMemoryRelation [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               +- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      "                     +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "                        +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n"
     ]
    }
   ],
   "source": [
    "result = weather_rep.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        min(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        max(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Interaction between Join, Aggregate & Repartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Aggregation after Join on same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) HashAggregate(keys=[usaf#87, wban#88], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- *(5) HashAggregate(keys=[usaf#87, wban#88], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "   +- *(5) Project [usaf#87, wban#88, air_temperature#97, air_temperature_qual#98]\n",
      "      +- *(5) SortMergeJoin [usaf#87, wban#88], [usaf#122, wban#123], Inner\n",
      "         :- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      "         :  +- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      "         :     +- *(1) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "         :        +- *(1) Filter (isnotnull(substring(value#82, 11, 5)) && isnotnull(substring(value#82, 5, 6)))\n",
      "         :           +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "         +- *(4) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(usaf#122, wban#123, 200)\n",
      "               +- *(3) Project [USAF#122, WBAN#123]\n",
      "                  +- *(3) Filter (isnotnull(usaf#122) && isnotnull(wban#123))\n",
      "                     +- *(3) FileScan csv [USAF#122,WBAN#123] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        min(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        max(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Aggregation after Join using repartitioned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[usaf#87, wban#88], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- *(2) HashAggregate(keys=[usaf#87, wban#88], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "   +- *(2) Project [usaf#87, wban#88, air_temperature#97, air_temperature_qual#98]\n",
      "      +- *(2) BroadcastHashJoin [usaf#87, wban#88], [usaf#122, wban#123], Inner, BuildRight\n",
      "         :- *(2) Filter (isnotnull(wban#88) && isnotnull(usaf#87))\n",
      "         :  +- InMemoryTableScan [usaf#87, wban#88, air_temperature#97, air_temperature_qual#98], [isnotnull(wban#88), isnotnull(usaf#87)]\n",
      "         :        +- InMemoryRelation [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         :              +- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      "         :                 +- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      "         :                    +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "         :                       +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "         +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true], input[1, string, true]))\n",
      "            +- *(1) Project [USAF#122, WBAN#123]\n",
      "               +- *(1) Filter (isnotnull(usaf#122) && isnotnull(wban#123))\n",
      "                  +- *(1) FileScan csv [USAF#122,WBAN#123] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n"
     ]
    }
   ],
   "source": [
    "joined = weather_rep.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        min(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        max(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Aggregation after Join with different key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[ctry#125], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- Exchange hashpartitioning(ctry#125, 200)\n",
      "   +- *(2) HashAggregate(keys=[ctry#125], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "      +- *(2) Project [air_temperature#97, air_temperature_qual#98, CTRY#125]\n",
      "         +- *(2) BroadcastHashJoin [usaf#87, wban#88], [usaf#122, wban#123], Inner, BuildRight\n",
      "            :- *(2) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "            :  +- *(2) Filter (isnotnull(substring(value#82, 11, 5)) && isnotnull(substring(value#82, 5, 6)))\n",
      "            :     +- *(2) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true]))\n",
      "               +- *(1) Project [USAF#122, WBAN#123, CTRY#125]\n",
      "                  +- *(1) Filter (isnotnull(usaf#122) && isnotnull(wban#123))\n",
      "                     +- *(1) FileScan csv [USAF#122,WBAN#123,CTRY#125] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,CTRY:string>\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result = joined.groupBy(stations[\"ctry\"]).agg(\n",
    "        min(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        max(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Aggregation after Broadcast-Join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[usaf#87, wban#88], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      "   +- *(2) HashAggregate(keys=[usaf#87, wban#88], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "      +- *(2) Project [usaf#87, wban#88, air_temperature#97, air_temperature_qual#98]\n",
      "         +- *(2) BroadcastHashJoin [usaf#87, wban#88], [usaf#122, wban#123], Inner, BuildRight\n",
      "            :- *(2) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "            :  +- *(2) Filter (isnotnull(substring(value#82, 11, 5)) && isnotnull(substring(value#82, 5, 6)))\n",
      "            :     +- *(2) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true]))\n",
      "               +- *(1) Project [USAF#122, WBAN#123]\n",
      "                  +- *(1) Filter (isnotnull(usaf#122) && isnotnull(wban#123))\n",
      "                     +- *(1) FileScan csv [USAF#122,WBAN#123] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string>\n"
     ]
    }
   ],
   "source": [
    "joined = weather.join(stations, (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]))\n",
    "result = joined.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "        min(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('min_temp'),\n",
    "        max(when(joined.air_temperature_qual == lit(1), joined.air_temperature)).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Merge Partitions using coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Coalesce 16\n",
      "+- InMemoryTableScan [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98]\n",
      "      +- InMemoryRelation [year#84, usaf#87, wban#88, date#89, time#90, report_type#91, wind_direction#92, wind_direction_qual#93, wind_observation#94, wind_speed#95, wind_speed_qual#96, air_temperature#97, air_temperature_qual#98], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      "                  +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "                     +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n"
     ]
    }
   ],
   "source": [
    "weather_small = weather_rep.coalesce(16)\n",
    "weather_small.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect WebUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1798753"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_rep.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Saving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_rep.write.mode(\"overwrite\").parquet(\"/tmp/weather_rep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 91 items\n",
      "-rw-r--r--   1 hadoop hadoop          0 2018-10-07 07:17 /tmp/weather_rep/_SUCCESS\n",
      "-rw-r--r--   1 hadoop hadoop       1337 2018-10-07 07:16 /tmp/weather_rep/part-00000-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      24241 2018-10-07 07:16 /tmp/weather_rep/part-00003-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      63340 2018-10-07 07:17 /tmp/weather_rep/part-00005-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      32695 2018-10-07 07:17 /tmp/weather_rep/part-00006-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     126661 2018-10-07 07:17 /tmp/weather_rep/part-00011-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      89610 2018-10-07 07:17 /tmp/weather_rep/part-00013-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      73063 2018-10-07 07:17 /tmp/weather_rep/part-00014-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      70655 2018-10-07 07:17 /tmp/weather_rep/part-00016-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      61512 2018-10-07 07:17 /tmp/weather_rep/part-00017-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     181909 2018-10-07 07:17 /tmp/weather_rep/part-00025-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      67545 2018-10-07 07:17 /tmp/weather_rep/part-00026-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      87515 2018-10-07 07:17 /tmp/weather_rep/part-00028-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      76725 2018-10-07 07:17 /tmp/weather_rep/part-00031-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      16246 2018-10-07 07:17 /tmp/weather_rep/part-00032-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      68058 2018-10-07 07:17 /tmp/weather_rep/part-00033-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      84538 2018-10-07 07:17 /tmp/weather_rep/part-00034-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      73316 2018-10-07 07:17 /tmp/weather_rep/part-00035-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     123655 2018-10-07 07:17 /tmp/weather_rep/part-00036-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      37920 2018-10-07 07:17 /tmp/weather_rep/part-00038-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      57775 2018-10-07 07:17 /tmp/weather_rep/part-00039-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      67351 2018-10-07 07:17 /tmp/weather_rep/part-00040-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      55996 2018-10-07 07:17 /tmp/weather_rep/part-00041-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      59784 2018-10-07 07:17 /tmp/weather_rep/part-00043-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      80773 2018-10-07 07:17 /tmp/weather_rep/part-00046-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      84986 2018-10-07 07:17 /tmp/weather_rep/part-00048-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     133418 2018-10-07 07:17 /tmp/weather_rep/part-00049-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      75265 2018-10-07 07:17 /tmp/weather_rep/part-00050-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      60268 2018-10-07 07:17 /tmp/weather_rep/part-00053-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      76993 2018-10-07 07:17 /tmp/weather_rep/part-00058-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     199806 2018-10-07 07:17 /tmp/weather_rep/part-00059-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      40241 2018-10-07 07:17 /tmp/weather_rep/part-00066-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      97540 2018-10-07 07:17 /tmp/weather_rep/part-00068-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      29008 2018-10-07 07:17 /tmp/weather_rep/part-00071-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      73180 2018-10-07 07:17 /tmp/weather_rep/part-00078-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop       3393 2018-10-07 07:17 /tmp/weather_rep/part-00081-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      62817 2018-10-07 07:17 /tmp/weather_rep/part-00084-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop       3359 2018-10-07 07:17 /tmp/weather_rep/part-00088-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      34895 2018-10-07 07:17 /tmp/weather_rep/part-00092-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      21333 2018-10-07 07:17 /tmp/weather_rep/part-00096-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      76141 2018-10-07 07:17 /tmp/weather_rep/part-00098-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      48870 2018-10-07 07:17 /tmp/weather_rep/part-00099-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      31191 2018-10-07 07:17 /tmp/weather_rep/part-00100-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      61306 2018-10-07 07:17 /tmp/weather_rep/part-00102-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     145618 2018-10-07 07:17 /tmp/weather_rep/part-00104-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      60617 2018-10-07 07:17 /tmp/weather_rep/part-00108-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      78265 2018-10-07 07:17 /tmp/weather_rep/part-00111-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      31085 2018-10-07 07:17 /tmp/weather_rep/part-00112-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      90587 2018-10-07 07:17 /tmp/weather_rep/part-00113-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      59706 2018-10-07 07:17 /tmp/weather_rep/part-00114-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      22701 2018-10-07 07:17 /tmp/weather_rep/part-00118-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      66911 2018-10-07 07:17 /tmp/weather_rep/part-00119-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     161560 2018-10-07 07:17 /tmp/weather_rep/part-00122-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      79337 2018-10-07 07:17 /tmp/weather_rep/part-00124-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      73118 2018-10-07 07:17 /tmp/weather_rep/part-00127-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     123673 2018-10-07 07:17 /tmp/weather_rep/part-00129-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      75963 2018-10-07 07:17 /tmp/weather_rep/part-00130-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      86810 2018-10-07 07:17 /tmp/weather_rep/part-00132-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      57741 2018-10-07 07:17 /tmp/weather_rep/part-00133-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop       3160 2018-10-07 07:17 /tmp/weather_rep/part-00134-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     124276 2018-10-07 07:17 /tmp/weather_rep/part-00137-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      68907 2018-10-07 07:17 /tmp/weather_rep/part-00141-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      37198 2018-10-07 07:17 /tmp/weather_rep/part-00143-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      80649 2018-10-07 07:17 /tmp/weather_rep/part-00145-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      12477 2018-10-07 07:17 /tmp/weather_rep/part-00150-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      52018 2018-10-07 07:17 /tmp/weather_rep/part-00151-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      79631 2018-10-07 07:17 /tmp/weather_rep/part-00152-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      90223 2018-10-07 07:17 /tmp/weather_rep/part-00154-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     135687 2018-10-07 07:17 /tmp/weather_rep/part-00156-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     142939 2018-10-07 07:17 /tmp/weather_rep/part-00157-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      63448 2018-10-07 07:17 /tmp/weather_rep/part-00158-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     144695 2018-10-07 07:17 /tmp/weather_rep/part-00163-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      56188 2018-10-07 07:17 /tmp/weather_rep/part-00164-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     163375 2018-10-07 07:17 /tmp/weather_rep/part-00165-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      61759 2018-10-07 07:17 /tmp/weather_rep/part-00166-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      18942 2018-10-07 07:17 /tmp/weather_rep/part-00171-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop       8239 2018-10-07 07:17 /tmp/weather_rep/part-00172-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      78075 2018-10-07 07:17 /tmp/weather_rep/part-00173-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      69343 2018-10-07 07:17 /tmp/weather_rep/part-00174-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      86969 2018-10-07 07:17 /tmp/weather_rep/part-00178-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      30513 2018-10-07 07:17 /tmp/weather_rep/part-00179-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      78521 2018-10-07 07:17 /tmp/weather_rep/part-00181-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      69376 2018-10-07 07:17 /tmp/weather_rep/part-00182-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      15683 2018-10-07 07:17 /tmp/weather_rep/part-00186-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      70658 2018-10-07 07:17 /tmp/weather_rep/part-00187-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      33030 2018-10-07 07:17 /tmp/weather_rep/part-00189-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      56766 2018-10-07 07:17 /tmp/weather_rep/part-00191-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      78657 2018-10-07 07:17 /tmp/weather_rep/part-00192-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      50076 2018-10-07 07:17 /tmp/weather_rep/part-00195-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      78921 2018-10-07 07:17 /tmp/weather_rep/part-00198-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop      60186 2018-10-07 07:17 /tmp/weather_rep/part-00199-19014412-a1e6-4348-a41d-49986590b40b-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /tmp/weather_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_small.write.mode(\"overwrite\").parquet(\"/tmp/weather_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 items\n",
      "-rw-r--r--   1 hadoop hadoop          0 2018-10-07 07:17 /tmp/weather_small/_SUCCESS\n",
      "-rw-r--r--   1 hadoop hadoop     290888 2018-10-07 07:17 /tmp/weather_small/part-00000-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     539188 2018-10-07 07:17 /tmp/weather_small/part-00001-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     490533 2018-10-07 07:17 /tmp/weather_small/part-00002-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     338415 2018-10-07 07:17 /tmp/weather_small/part-00003-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     460959 2018-10-07 07:17 /tmp/weather_small/part-00004-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     358779 2018-10-07 07:17 /tmp/weather_small/part-00005-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     394439 2018-10-07 07:17 /tmp/weather_small/part-00006-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     295745 2018-10-07 07:17 /tmp/weather_small/part-00007-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     274293 2018-10-07 07:17 /tmp/weather_small/part-00008-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     352943 2018-10-07 07:17 /tmp/weather_small/part-00009-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     405437 2018-10-07 07:17 /tmp/weather_small/part-00010-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     337051 2018-10-07 07:17 /tmp/weather_small/part-00011-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     521293 2018-10-07 07:17 /tmp/weather_small/part-00012-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     330085 2018-10-07 07:17 /tmp/weather_small/part-00013-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     365699 2018-10-07 07:17 /tmp/weather_small/part-00014-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n",
      "-rw-r--r--   1 hadoop hadoop     398450 2018-10-07 07:17 /tmp/weather_small/part-00015-31d2cdf1-532c-4549-b706-d040d0a0921b-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /tmp/weather_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.3 (Python 3)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
