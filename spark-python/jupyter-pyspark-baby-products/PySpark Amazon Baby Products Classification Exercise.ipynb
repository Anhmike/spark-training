{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "First we load data from HDFS. It is stored as a trivial CSV file with three columns\n",
    "1. product name\n",
    "2. review text\n",
    "3. rating (1 - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = spark.read.text(\"s3://dimajix-training/data/amazon_baby\")\n",
    "data = data.select(\n",
    "        split('value',',')[0].alias('name'),\n",
    "        split('value',',')[1].alias('review'),\n",
    "        split('value',',')[2].alias('rating').cast('int')\n",
    ").filter(col('rating').isNotNull())\n",
    "\n",
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentiment\n",
    "\n",
    "Since we want to perform a classification (positive review vs negative review), we need to extract a binary sentiment value. We will map the ratings as follows:\n",
    "\n",
    "1. Ratings 1 and 2 count as a negative review\n",
    "2. Rating 3 counts as a neutral review\n",
    "3. Ratings 4 and 5 count as a positive review\n",
    "\n",
    "Since we want a binary classification, we will also remove neutral reviews altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove all reviews with a rating of 3 from data\n",
    "data = ...\n",
    "\n",
    "# Add new column sentiment according to rules above\n",
    "data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features from Reviews\n",
    "\n",
    "Now we want to split the review text into individual words, so we can create a \"bag of words\" model. In order to get a somewhat nice model, we also need to remove all punctuations from the reviews. This will be done as the first step using a user defined function (UDF) in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def cleanup_text(text):\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c, ' ')\n",
    "    return text\n",
    "\n",
    "# Register cleanup_text as a UDF\n",
    "remove_punctuation = ...\n",
    "\n",
    "# Apply udf to data and store result in data2. The cleaned column should be called 'review' again\n",
    "data2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Reviews into Words\n",
    "We could do that ourselves using the Python split method, but we use a Transformer provided by PySpark instead. Saves us some time and helps to create clean code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "\n",
    "# Create appropriate instance of PySpark Tokenizer, such that \n",
    "# reviews will be split up and stored in a new column 'words'\n",
    "tokenizer = ...\n",
    "\n",
    "# Create new DataFrame words by applying the Tokenizer to data2\n",
    "words = ...\n",
    "\n",
    "# Fetch first 3 rows and display them as a Pandas DataFrame\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop words\n",
    "\n",
    "We also want to remove so called stop words, which are all those tiny words which mainly serve as glue for building sentences. Usually they do not contain much information in a simple bag of words model. So we get rid of them.\n",
    "\n",
    "This is so common practice that PySpark already contains a Transformer for just doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopWords=['the','a','and','or', 'it', 'this', 'of', 'an', 'as', 'in', 'on', 'is', 'are', 'to', 'was', 'for', 'then', 'i']\n",
    "\n",
    "# Create an instance of StopWordRemover. Store the result in new column 'vwords'.\n",
    "stopWordsRemover = ...\n",
    "\n",
    "# Create new DataFrame words by applying the StopWordRemover to data2\n",
    "vwords = ...\n",
    "\n",
    "# Fetch first 3 rows and display them as a Pandas DataFrame\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bag of Words Features\n",
    "\n",
    "Finally we simply count the number of occurances of all words within the reviews. Again we can simply use a Transformer from PySpark to perform that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create instance of CountVectorizer, store results in column 'features'\n",
    "# Set additional parameter minDF to 2.0, such that each word needs to appear in at least two documents.\n",
    "countVectorizer = ...\n",
    "\n",
    "# Create a model from the CountVectorizer by fitting vwords\n",
    "countVectorizerModel = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Vocabulary\n",
    "\n",
    "The countVectorizerModel contains an implcit vocabulary containing all words. This can be useful for mapping features back to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print countVectorizerModel.vocabulary[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy up DataFrame\n",
    "\n",
    "We now carry so many columns inside the DataFrame, let's remove some intermediate columns to get more focus on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract features by using the model to transform vwords\n",
    "features = ...\n",
    "\n",
    "# Display first three rows of result as Pandas DataFrame\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train Data / Test Data\n",
    "\n",
    "Now let's do the usual split of our data into a training data set and a validation data set. Let's use 80% of all reviews for training and 20% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data = features.randomSplit([0.8,0.2], seed=0)\n",
    "\n",
    "print \"train_data: %d\" % train_data.count()\n",
    "print \"test_data: %d\" % test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier\n",
    "\n",
    "There are many different classification algorithms out there. We will use a LogisticRegression, of course a DecisionTreeClassifier could be another interesting option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "\n",
    "logisticRegression = LogisticRegression(featuresCol='features',labelCol='sentiment')\n",
    "logisticModel = logisticRegression.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Model\n",
    "The LogisticRegressionModel also uses coefficients mapped to individual words. Let's have a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print logisticModel.coefficients.toArray()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numPositiveWeights = len(filter(lambda x: x > 0, logisticModel.coefficients.toArray()))\n",
    "numNegativeWeights = len(filter(lambda x: x < 0, logisticModel.coefficients.toArray()))\n",
    "\n",
    "print \"Number positive weights %d\" % numPositiveWeights\n",
    "print \"Number negative weights %d\" % numNegativeWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Weights of some Words\n",
    "\n",
    "Let's check how coefficients look like for some clearly positive or negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function which prints the weight of a given word\n",
    "def print_word_weight(word):\n",
    "    # First you need to find the word in the vocabulary of the countVectorizerModel. \n",
    "    # You need the index within the vocubalary. Note: Python arrays have a nice method called 'index'\n",
    "    index = ...\n",
    "    # Now lookup the weight in the model's coefficients using the index\n",
    "    weight = ...\n",
    "    print '%s : %f' % (word, weight)\n",
    "    \n",
    "print_word_weight('good')\n",
    "print_word_weight('great')    \n",
    "print_word_weight('bad')\n",
    "print_word_weight('ugly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Extreme Words\n",
    "\n",
    "Let us try to find the most positive and most negative word according to the weights. This can be achieved using numpy argmin function to find the index and the vocabulary to map the index to the actual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Find the index of the coefficient with the lowest value. Note np.argmin could be your friend\n",
    "worstWordIndex = ...\n",
    "# Find the word belonging to the index in the models vocabulary\n",
    "worstWord = ...\n",
    "# Find the weight belonging to the index\n",
    "worstWeight = ...\n",
    "print \"Worst word: %s  value %f\" % (worstWord, worstWeight)\n",
    "\n",
    "# Repeat exercise with most positive word\n",
    "bestWordIndex = ...\n",
    "bestWord = ...\n",
    "bestWeight = ...\n",
    "print \"Best word: %s value %f\" % (bestWord, bestWeight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_freq = features.select(explode('vwords').alias('word'),'review') \\\n",
    "     .distinct() \\\n",
    "     .groupBy('word').count()\n",
    "\n",
    "doc_freq.orderBy(col('count').desc()).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions\n",
    "\n",
    "The primary idea is of course to make predictions of the sentiment using the learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = logisticModel.transform(test_data)\n",
    "\n",
    "pred.drop('features').limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most Positive Review\n",
    "\n",
    "Using the column rawPrediction, we can find the review which has the highest positive prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract one component from a Vectors\n",
    "extract_from_vector = udf(lambda v,i : float(v[i]), FloatType())\n",
    "\n",
    "positives = pred.orderBy(extract_from_vector(pred.rawPrediction,lit(1)).desc())\n",
    "\n",
    "positives.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Prediction\n",
    "\n",
    "Again we want to assess the performance of the prediction model. This can be done using the builtin class BinaryClassificationEvaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import *\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='sentiment')\n",
    "result = evaluator.evaluate(pred)\n",
    "\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Evaluator\n",
    "\n",
    "We want to use a different metric namely accuracy. Accuracy is defined as\n",
    "\n",
    "    number_correct_predictions / total_number_predictions\n",
    "    \n",
    "First let us directly calculate that metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the total number of predictions\n",
    "num_total = ...\n",
    "# Get the number of correct predictions according to learned model\n",
    "num_correct = ...\n",
    "\n",
    "model_accuracy = float(num_correct) / num_total\n",
    "\n",
    "print \"Model Accuracy: %f\" % (float(num_correct) / num_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Dummy Predictor\n",
    "\n",
    "It is always interesting to see how a trivial prediction performs. The trivial predictor simply predicts the most common class for all objects. In this case this would be a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get total number of predictions\n",
    "num_total = ...\n",
    "# Get the number of correct predictions according to baseline classifier, which always predicts \"positive\"\n",
    "num_correct = ...\n",
    "\n",
    "baseline_accuracy = float(num_correct) / num_total\n",
    "\n",
    "print \"Baseline Accuracy: %f\" % (float(num_correct) / num_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Evaluator\n",
    "\n",
    "Now let us create a new Evaluator class implementing accuracy as the relevant Metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AccuracyClassificationEvaluator(Evaluator):\n",
    "    def __init__(self, predictionCol='prediction', labelCol='label'):\n",
    "        super(Evaluator,self).__init__()\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "    \n",
    "    def _evaluate(self, dataset):\n",
    "        # Get total number of records in dataset\n",
    "        num_total = ...\n",
    "        # Get number of records where prediction matches real label. Note that you should\n",
    "        # use col(self.predictionCol) and col(self.labelCol) to access the columns\n",
    "        num_correct = ...\n",
    "        # Calculate accuracy\n",
    "        accuracy = ...\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluator = AccuracyClassificationEvaluator(labelCol='sentiment')\n",
    "\n",
    "print evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweak Hyper Parameters\n",
    "\n",
    "Again we want to improve overall performance by tweaking model parameters. So first let's see which parameters are available for tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print LogisticRegression().explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try some different parameters and check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logisticRegression2 = LogisticRegression(featuresCol='features',labelCol='sentiment')\n",
    "logisticRegression2.setRegParam(0.01).setMaxIter(100)\n",
    "logisticModel2 = logisticRegression2.fit(train_data)\n",
    "\n",
    "pred = logisticModel2.transform(test_data)\n",
    "\n",
    "roc_evaluator = BinaryClassificationEvaluator(labelCol='sentiment', metricName=\"areaUnderROC\")\n",
    "acc_evaluator = AccuracyClassificationEvaluator(labelCol='sentiment')\n",
    "\n",
    "print \"areaUnderROC = %f\" % roc_evaluator.evaluate(pred)\n",
    "print \"Model Accuracy = %f\" % acc_evaluator.evaluate(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding best Hyper Parameters\n",
    "\n",
    "So we got an improvement, but what would be best? We need to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for reg_param in [0.0, 0.0001, 0.01, 1.0, 100.0]:\n",
    "    logisticRegression2 = LogisticRegression(featuresCol='features',labelCol='sentiment')\n",
    "    logisticRegression2.setRegParam(reg_param).setMaxIter(100)\n",
    "    logisticModel2 = logisticRegression2.fit(train_data)\n",
    "    \n",
    "    pred = logisticModel2.transform(test_data)\n",
    "    \n",
    "    roc_evaluator = BinaryClassificationEvaluator(labelCol='sentiment', metricName=\"areaUnderROC\")\n",
    "    acc_evaluator = AccuracyClassificationEvaluator(labelCol='sentiment')\n",
    "\n",
    "    print \"reg_param = %f\" % reg_param\n",
    "    print \"    areaUnderROC = %f\" % roc_evaluator.evaluate(pred)\n",
    "    print \"    Model Accuracy = %f\" % acc_evaluator.evaluate(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParamGridBuilder & CrossValidator\n",
    "\n",
    "Since the selection of hyper parameters is a very common job and might be tedious work, there is some nice support in PySpark to simplify it. It is a two-step approach:\n",
    "1. Use ParamGridBuilder to create a set of parameters to test, possibly for different hyper parameters\n",
    "2. Use a CrossValidator for selecting the best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import *\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='sentiment')\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.0, 0.0001, 0.01, 1.0, 100.0]) \\\n",
    "    .addGrid(lr.maxIter, [10, 100]) \\\n",
    "    .build()\n",
    "    \n",
    "for pset in param_grid:\n",
    "    params = [\"%s=%s\" % (key.name, str(value)) for (key,value) in pset.items()]\n",
    "    print ' '.join(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features',labelCol='sentiment')\n",
    "evaluator = AccuracyClassificationEvaluator(labelCol='sentiment')\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "model = cv.fit(train_data)\n",
    "\n",
    "print evaluator.evaluate(model.transform(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark 2.1 (Python 3.5)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
